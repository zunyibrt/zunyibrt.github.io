{"pageProps":{"posts":[{"slug":"05_mech_interp","content":"\nThis is a pretty long paper, and is divided into 12 sections (although the last 3 are discussion, related work and comments).\n\n## Section 1: Background & Motivation\n\nThe first section provides an introduction and summary of the main results in this paper. They begin by mentioning that it would be nice if each neuron had a one to one correspondence with a specific feature such as dog’s snout. This has been shown empirically to sometimes happen, but this is not always the case especially in LLMs. So the motivating question is when and why this happens (or doesn’t happen). By using toy models which are small ReLU networks trained to learn five features of varying importance and varying sparsity, they show that models can store more features than they have dimensions (**superposition**), and they can even perform computations in this superposed state. They hypothesize that the small networks simulate large sparse networks.\n\nThis seems like an interesting way to view superposition —- as a way of compressing information through continuous representation, but also as a computation mechanism (cue the name superposition and some possible relation to QM). When they say `no interference', this is what I think of as a circuit, with each neuron activating on, and only on, specific features.\n\nThey go on to say the main contribution is that they directly demonstrate superposition occurs relatively natural and offer a theory (phase diagram!) of when this happens. They make lots of parallels to physics phenomena, which I am excited to read given my physics background. There is an interesting note for example, that superposition organizes features into geometric structures, which immediately signals to be some kind of energy (loss function) minimizing geometries. They have not defined **sparsity** yet, although they discuss it, but I’m sure that is coming next. \n\n## Section 1b: Definitions and Motivation: Features, Directions, and Superposition\n\nThe next part of this section kicks off by discussing the Linear Representation Hypothesis ([see here](https://arxiv.org/abs/2311.03658)), which is the idea that certain directions in a representation space correspond to specific high level concepts, in other words that they carry semantic meaning. The claim that these are linear also seems important since that makes them easy to compose and decompose, and even to do vector arithmetic. This is more an empirical observation, e,g, see Mikolov et al for a famous example in word embeddings.\n\nIn their words, ‘features of the input’ are captured as ‘directions in activation space’. If we can figure out these vector representations, we can decompose activations into something we can more easily understand — as a basis of concepts each activated with a certain strength. \n\nOne natural basis in a neural network by virtue of its structure is having each neuron correspond to one feature. Naturally this would be limited to the number of neurons you have (This is a privileged basis, and there are incentives to used this basis such as activation functions, but this doesn’t mean that the basis will be used)! However, if you have more features than neutrons, a different strategy is required. This is where the idea of superposition comes in, where the features are captured by a certain direction in the high dimensional input space. I assume this is akin to creating a non-orthogonal basis. \n\nThere is some interesting discussion of what actually constitutes a feature, since `concept' is a pretty vague word. They settle on defining it as a property which will be an individual neuron in a sufficiently large network for the same problem. Does this mean that any neural network that exhibits superposition has an equivalent much larger neural network that works in the basis where each neuron is one feature? What implications does that have for discrete valued/low bit neural networks? What are the trade offs for being able to do this network compression?\n\nNext, a strong case is made for linear representations, of which I like the argument that linear representations make features linearly accessible, meaning that since neural network layers are linear functions followed by a non-linearity, a feature must be linear in order for a neuron to consistently excite or inhibit on that feature in a single step. \n\nLastly, there is more discussion on **the superposition hypothesis**, which I think is the core of this paper.  Basically, if a neural network needs to represent more features than there are neurons, features can’t align with the basis and hence polysemanticity (single neuron represents to multiple geatures) is unavoidable. While you can only have n orthogonal vectors in n dimensions, you can have an exponential exp(n) almost orthogonal vectors. So for large-n, this would be very efficient at cramming more features in than neurons! The cost is that each feature activating creates some small amount of noise by making it looks as some other features slightly activating. \n\nThe other big idea here is that sparsity is important. From compressed sensing, it is generally difficult to reconstruct a high dimensional vector projected into a low dimensional space since information is lost, but it is possible if you can assume the original vector is sparse. So sparsity here means that activations don’t happen together, and that means this `noise' is minimized. If the activations are not sparse, multiple present features will interfere.\n\nAgain the hypothesis is that small networks with polysemanticity are low dimensional projections of larger networks. The requirements for this to be effective seem pretty strict, so it would nice to be able to better quantify them and to understand how important this is in most networks.","title":"Mech Interp: Toy Models Part I","date":"2025-07-11","excerpt":"","tags":["mech interp"]},{"slug":"04_mech_interp","content":"\nWith so much to look at, it is difficult to decide where to start, but it is probably best to just start somewhere. To start off, I think I will read [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) (Elhage et al 2022). I remember really liking the findings and approach of the paper, since trying to understand simple toy models seems like a very intuitive approach to me. I have found this very useful even in my own research, where instead of directly trying to understand the results of large simulations with lots of moving pieces, we try to understand much simple toy model simulations to build a scaffold for understanding the complex behavior we observe. Since the work deals with toy models, I should also be able to play around and run some of these experiments fairly easily. This should also be a natural start to understanding sparse autoencoders (SAEs) since they seem to be the current main approach towards untangling this problem of superposition.\n\nBefore that I would like to read some resource that gives a higher level view of the history of the field and where it is going, to get a better sense of the bird’s eye view landscape. At the same time I am keeping a list of concepts I want to learn more about as I come across them, and hopefully this list does not get too long.\n\nI decided to begin by reading [this blog post](https://www.alignmentforum.org/posts/beREnXhBnzxbJtr8k/mech-interp-is-not-pre-paradigmatic) on the alignment forum by Lee Sharkey titled “Mech interp is not pre-paradigmatic”. In particular, since I am looking for some initial paradigm in which to frame mech interp research, the title of this post indicated it would present some form of such a framework. So as the title suggests, this post basically begins by arguing that the field of mech interp is **not** in what might be called a pre-paradigmatic phase, and instead is actually at the mature stage of a second mini-paradigmic wave. Pre-paradigmatic here refers to a stage in the development of a scientific field before it has established a dominant theoretical framework or \"paradigm.\" This concept comes from philosopher Thomas Kuhn's influential work \"The Structure of Scientific Revolutions”, where he outlines how a field progresses through these stages. While some argue mech interp, being a relatively new field, is in this pre-paradigmatic phase, this post pushes the idea that mech interp actually begun as an offshoot of computational neuroscience and hence inherits many of the ideas and concepts established there. In fact, the argument is made that it has been rediscovering many of the same ideas explored in neuroscience! I certainly lack sufficient knowledge in either area to judge this statement, but I don’t find this claim surprising given the parallels. Further, given the greater tractability of working with neural networks, perhaps progress might actually eventually flow in the other direction! I think the worry lies in the fact that neuroscience seems to be struggling to make recent progress and that mech interp might reach a similar wall. The article suggests the following **Three Waves of Mech Interp**:\n\n1. **First Wave** (2010s): Focused on demonstrating that interpretable structure exists in deep neural networks. 'Features are the fundamental unit of neural networks' and 'features are connected by weights, forming circuits' . This wave ended when researchers discovered polysemantic neurons (neurons that respond to multiple unrelated concepts).\n2. **Second Wave** (2022-present): Emerged after the \"Toy Models of Superposition\" paper, introducing sparse dictionary learning (SDLs) to address polysemanticity. However, this wave now faces its own anomalies.\n3. **Potential Third Wave?** The post suggests \"Parameter Decomposition\" as a promising approach that could resolve Second-Wave anomalies by decomposing neural network parameters into interpretable components representing computational mechanisms. Worth nothing that this is what the author is working on at present.\n\nAt the same time, I think the transition from first to second wave lines up with the rise of LLMs, since much early work was done with CNNs. Much larger LLMs clearly showed much richer representation and required understanding polysemanticity and superposition. As for parameter decomposition, I will have to come back to this again since I don’t understand enough to appreciate its arguments. This outline also makes me feel like the Toy Models paper is indeed the best place to start if it marks this big transition in the thinking in the field. I think it would be good for me to try and define mech interp, superposition and polysemancity, perhaps after going through the paper, and for example think about this particular quote from the post:\n\n> The idea that 'networks represent more features than they have neurons'. It is a natural corollary of the superposition hypothesis that neurons would exhibit polysemanticity, since there cannot be a one-to-one relationship between neurons and 'features'.\n\nThe article also presents a list of anomalies in this Second Wave that I would like to return to once I have built more understanding, since these should be the open problems the field is looking at now. While this post is just one perspective, I think it is a sufficiently good mental framework to start with, with lots of references and ideas to think about. Alright, onwards to Toy Models!","title":"Mech Interp: Paradigms","date":"2025-07-10","excerpt":"","tags":["mech interp"]},{"slug":"03_mech_interp","content":"\n## Starting my Mech Interp Journey\n\nChange is never easy, and stepping away from astronomy after many years feels like giving up part of my identity. I don't regret my decision to pursue graduate school, those years have been nothing but fulfilling. It's a great privilege to dedicate time and effort to a pursuit of such intellectual purity. However, at the end of graduate school, one must once again cast thoughts toward the future.\nRemaining on the academic path brings uncertainty in many areas of life and often demands personal sacrifices: constant relocation, distance from loved ones, evolving responsibilities as teaching loads and funding pressures mount. Balancing these demands with other personal priorities is far from easy, and I found myself unable to envision navigating this path happily. I've also always been acutely aware that while astronomy is fascinating, there are infinite equally interesting areas of study and work—many of more immediate relevance to humanity.\nWhile changing trajectory has been difficult, deciding what to do next has been even more challenging. One area I'm strongly interested in pursuing is mechanistic interpretability.\n\n## The Rise of Neural Networks\n\nOver the years, I've observed the explosion of machine learning, or more accurately, the rise of neural networks, deep learning, and large language models as they evolved from esoteric topics to universal adoption, taking root in all aspects of society. The social implications are tremendous, and it really does appear to be a watershed moment in how humans interact with technology.\nYet so much remains mysterious about how these networks actually work. We train these systems on vast amounts of data, but their resultant capabilities have repeatedly exceeded expectations while theoretical understanding struggles to catch up. This reminds me of complex systems that display emergent behavior even with simple, local rule-based evolution. We know the full state of a trained neural network (every weight, bias, and computation that flows through it), but its overall capabilities still baffle us.\nInformation is being processed in ways that seem opaque to us, so a field has crystallized around making sense of this opacity in ways we can understand and interpret. This is mechanistic interpretability (or \"mech interp\"), the science of understanding how machine learning networks learn to process information, much like neuroscience tries to understand how the biological brain does something similar.\nWhile young, this field is moving at lightning speed. I strongly believe that progress in mech interp is among the most important research being done today, given the reach and rate of growth of this technology. Preventing these systems from pursuing unintended goals (known as AI alignment) surely requires developing an understanding of how these networks do what they do.\n\n## A New Experience\n\nWhile I've maintained interest in machine learning and neural networks over the years, my direct experience has been limited. The applications to my research were never convincing or promising enough (a lack of interpretability makes neural networks problematic for theoretical applications).\nRecently, however, I experimented with using Neural ODEs as a natural way of extending our usual process of modeling physical systems with differential equations through deep learning, while exploring symbolic regression to improve interpretability and generalizability. I also attended NeurIPS 2024 and got a feel for what the field was excited about, including in the context of scientific applications.\nIt's been over half a year since then, and many of the big ideas such as MCP, multimodal inputs, and agentic AI have dominated advances in that time. Along the way, I've repeatedly encountered work being done on mechanistic interpretability, including papers from the Anthropic team. Beyond skimming these papers, I haven't devoted time to thinking more deeply about these ideas.\nGiven that this is a field I'm interested in pursuing, I've decided to invest time in diving deeper into the ideas and research in the literature, and exploring where I might be able to contribute.\n\n## Why This Blog\n\nStarting this research blog serves three purposes.\n1. Mainly, I hope it will document my thoughts and ideas as they change and evolve while I learn and explore this new field. As I progress, it will be useful to return to earlier thoughts.\n2. Second, I want to improve the clarity of my writing, since I often find it difficult to express thoughts without extensive refinement.\n3. Finally, I'm hoping this imposes some level of self-accountability to keep at it regularly, since this will be on the side and doesn't overlap with my current work.","title":"Mech Interp Day 0: Motivations","date":"2025-07-09","excerpt":"Starting my mech interp journey","tags":["mech interp"]},{"slug":"02_animation","content":"\nThe previous iteration of this website featured an interactive 3D pulsating sphere built using Three.js. For the sake of a cleaner feel, I decided to remove it for this iteration, but found the experience rewarding. The animation I originally had can be found at the botto of this post! The capability for 3D graphics that Three.js provide is rich in potential, especially since modern web development has evolved far beyond static pages. Today's users expect rich, interactive experiences that feel more like native applications than traditional websites.\n\n## The Technology Stack\n\nCombining several powerful technologies can create truly engaging web experiences. The combination I initially went with included:\n\n- **Three.js** for 3D graphics and WebGL rendering\n- **Framer Motion** for smooth animations and transitions\n- **React** for component-based architecture\n- **Next.js** for performance optimization\n\n## Why 3D on the Web?\n\nThree.js has revolutionized how we think about web interfaces. Here's a simple example of creating a rotating cube:\n\n```javascript\nimport * as THREE from 'three';\n\nconst scene = new THREE.Scene();\nconst camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\nconst renderer = new THREE.WebGLRenderer();\n\nconst geometry = new THREE.BoxGeometry();\nconst material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });\nconst cube = new THREE.Mesh(geometry, material);\n\nscene.add(cube);\ncamera.position.z = 5;\n\nfunction animate() {\n    requestAnimationFrame(animate);\n    cube.rotation.x += 0.01;\n    cube.rotation.y += 0.01;\n    renderer.render(scene, camera);\n}\n\nanimate();\n```\n\n## Animation with Framer Motion\n\nFramer Motion makes it incredibly easy to add smooth animations to React components:\n\n```jsx\nimport { motion } from 'framer-motion';\n\nconst AnimatedCard = () => {\n  return (\n    \n      Interactive Card\n      This card animates on load and hover!\n    \n  );\n};\n```\n\n## My Experience\n\nThe combination of Three.js and Framer Motion in a React/Next.js environment provides an incredibly powerful toolkit for creating memorable web experiences. The key is finding the right balance between visual impact and performance. The rest of this post will go through how I set up my initial animation, with the final product at the end!\n\n## Uniform Sampling of Points on A Sphere\n\nThe key to creating a convincing sphere lies in proper point distribution. Unlike naive approaches that create clustering at poles, we use **spherical coordinates** with uniform random sampling:\n\n```javascript\nconst points = [];\nfor (var i = 0; i < 1500; i++) {\n  var vertex = new THREE.Vector3();\n\n  // Uniform random sampling on sphere surface\n  var u = THREE.MathUtils.randFloat(0, 1);\n  var v = THREE.MathUtils.randFloat(0, 1);\n  var theta = 2 * Math.PI * u;           // Azimuthal angle\n  var phi = Math.acos(2 * v - 1);        // Polar angle\n\n  // Convert to Cartesian coordinates\n  vertex.x = 3.5 * Math.sin(phi) * Math.cos(theta);\n  vertex.y = 3.5 * Math.sin(phi) * Math.sin(theta);\n  vertex.z = 3.5 * Math.cos(phi);\n\n  points.push(vertex);\n}\n```\n\nThis approach ensures **uniform distribution** across the sphere surface, avoiding the visual artifacts of simpler random placement methods.\n\n## GLSL Shader\n\nThe pulsating light effect is achieved through custom **vertex and fragment shaders** that run directly on the GPU:\n\n### Shader Uniforms Setup\n\n```javascript\nconst shaderPoint = THREE.ShaderLib.points;\nconst uniforms = THREE.UniformsUtils.clone(shaderPoint.uniforms);\nuniforms.time = { value: 0 };\nuniforms.color = { type: \"v3\", value: colorValue };\n\nconst pMaterial = new THREE.ShaderMaterial({\n  uniforms,\n  transparent: true,\n  depthWrite: false,\n  blending: THREE.AdditiveBlending,  // Creates light emission effect\n  vertexShader,\n  fragmentShader,\n});\n```\n\n### The Shader Pipeline\n\nThis particle system consists of two interconnected shaders:\n\n- **Vertex Shader** (\"vert.glsl\") - Handles particle positioning, sizing, and movement\n- **Fragment Shader** (\"frag.glsl\") - Controls particle appearance, color, and glow effects\n\n### 3D Simplex Noise Implementation\n\nThe vertex shader begins with a complete **Simplex noise** implementation - a sophisticated algorithm for generating natural-looking randomness:\n\n```glsl\nfloat snoise(vec3 v) {\n    // 85 lines of math\n    // Creates smooth, continuous 3D noise\n    return 42.0 * dot(m*m, vec4(dot(p0,x0), dot(p1,x1), dot(p2,x2), dot(p3,x3)));\n}\n```\n\n**Why Simplex Noise?** Unlike basic random functions, Simplex noise provides:\n- **Continuous gradients** - no sudden jumps or artifacts\n- **3D coherence** - neighboring points have similar values\n- **Performance optimization** - faster than Perlin noise\n- **Natural patterns** - mimics organic movement and growth\n\n### Organic Particle Movement\n\nThe main vertex shader transforms each particle's position using time-based trigonometric functions:\n\n```glsl\nvec3 newPos = position;\n\nnewPos.x += sin(time + position.x * position.y) * 0.08;\nnewPos.y += cos(time + position.x * position.y * 1.1) * 0.08;\nnewPos.z += cos(time + position.x * position.y * 1.3) * 0.08;\n```\n\n**Mathematical Breakdown:**\n- **Base oscillation**: \"sin(time)\" and \"cos(time)\" create rhythmic movement\n- **Position coupling**: \"position.x * position.y\" makes each particle's movement unique\n- **Frequency variation**: Multipliers prevent synchronized motion\n- **Amplitude control**: \"0.08\" keeps movement subtle and elegant\n\nThis creates a **Lissajous-like pattern** where each particle follows its own complex orbital path, determined by its starting position.\n\n### Dynamic Particle Sizing\n\nThe most sophisticated aspect is the noise-driven size variation:\n\n```glsl\ngl_PointSize = 50. + snoise(position * 0.05 + vec3(0, 0, vtime * 0.1)) * 50.;\ngl_PointSize *= 0.5;\n```\n\n**Size Calculation Explained:**\n- **Base size**: \"50.\" pixels provides consistent minimum visibility\n- **Noise sampling**: \"snoise(position * 0.05 + ...)\" creates spatial variation\n- **Temporal drift**: \"vec3(0, 0, vtime * 0.1)\" makes noise evolve over time\n- **Amplitude**: \"* 50.\" allows sizes to range from 0 to 100 pixels\n- **Final scaling**: \"* 0.5\" reduces overall scale to 0-50 pixels\n\nThe result is particles that **breathe** - growing and shrinking organically as the noise field evolves through time.\n\n### Time-Based Animation\n\nThe pulsating effect is driven by a time uniform that updates every frame:\n\n```javascript\nconst animate = (time) => {\n  // Update shader time for pulsating effect\n  pMaterial.uniforms.time.value = time * 0.004;\n  \n  // Continue animation loop\n  requestAnimationFrame(animate);\n  renderer.render(scene, camera);\n}\n```\n\nThe **AdditiveBlending** mode creates the characteristic light emission, making particles appear to glow and blend naturally when they overlap.\n\n## Responsive Color System\n\nThe particle system adapts to the user's color mode preference through dynamic uniform updates:\n\n```javascript\n// Real-time color mode detection and adaptation\nif (localStorage.getItem(\"chakra-ui-color-mode\") === \"dark\") {\n  pMaterial.uniforms.color.value = new THREE.Color(0xffffff);  // White particles\n} else {\n  pMaterial.uniforms.color.value = new THREE.Color(0x000000);  // Black particles\n}\n```\n\nThis creates a seamless experience where the 3D scene automatically adapts to the user's interface preferences without requiring page refreshes.\n\n## Smooth Camera Animations\n\nThe initial camera movement uses an **easing function** to create natural motion:\n\n```javascript\nfunction easeOutCirc(x) {\n  return Math.sqrt(1 - Math.pow(x - 1, 4));\n}\n\n// Camera animation during first 100 frames\nif (frame <= 100) {\n  const rotSpeed = -easeOutCirc(frame / 120) * Math.PI * 20;\n  \n  camera.position.x = p.x * Math.cos(rotSpeed) + p.z * Math.sin(rotSpeed);\n  camera.position.z = p.z * Math.cos(rotSpeed) - p.x * Math.sin(rotSpeed);\n  camera.lookAt(target);\n} else {\n  // Switch to user-controlled orbit after animation\n  controls.update();\n}\n```\n\nAfter the initial animation completes, control transitions to **OrbitControls** for user interaction, with automatic rotation enabled.\n\n## Performance Optimizations\n\n### GPU-Accelerated Rendering\n- **ShaderMaterial** for GPU-based calculations\n\n### Efficient Animation Loop\n```javascript\nlet req = null;\nconst animate = (time) => {\n  req = requestAnimationFrame(animate);\n  \n  // Minimal CPU calculations\n  // GPU handles particle transformations\n  \n  renderer.render(scene, camera);\n}\n\n// Proper cleanup\nreturn () => {\n  cancelAnimationFrame(req);\n  renderer.domElement.remove();\n  renderer.dispose();\n}\n```\n\n## Final Product\nHere is the final animation, best viewed in dark mode!\n<hr />\n<voxel-art />","title":"Building Interactive Web Experiences","date":"2025-07-08","excerpt":"Exploring the combination of Three.js, Framer Motion, and React for creating engaging user interfaces","tags":["threejs","framer-motion","webgl","animation"]},{"slug":"01_implementation","content":"\nThis document summarizes the complete implementation of a markdown-based blog system using Next.js, Chakra UI, react-markdown, and KaTeX for mathematical equations.\n\n## Tech Stack\n\n- **Next.js** - React framework with static site generation\n- **Chakra UI** - Component library for styling\n- **react-markdown** - Markdown to React component converter\n- **KaTeX** - LaTeX equation rendering\n- **gray-matter** - Frontmatter parsing\n- **Framer Motion** - Animations\n\n## Required Dependencies\n\n```bash\nnpm install react-markdown remark-gfm rehype-highlight rehype-raw gray-matter katex rehype-katex remark-math\n```\n\n## File Structure\n\n```\nproject/\n├── components/\n│   └── BlogPost.jsx\n├── lib/\n│   └── posts.js\n├── pages/\n│   ├── writing.js (blog index)\n│   └── blog/\n│       └── [slug].js (individual posts)\n├── posts/\n│   ├── my-first-post.md\n│   └── second-blog-post.md\n└── pages/_app.js\n```\n\n## Core Components\n\n### lib/posts.js - File System Functions\n```javascript\nimport fs from 'fs';\nimport path from 'path';\nimport matter from 'gray-matter';\n\nconst postsDirectory = path.join(process.cwd(), 'posts');\n\nexport function getAllPosts() {\n  const fileNames = fs.readdirSync(postsDirectory);\n  const allPostsData = fileNames.map((fileName) => {\n    const slug = fileName.replace(/\\.md$/, '');\n    const fullPath = path.join(postsDirectory, fileName);\n    const fileContents = fs.readFileSync(fullPath, 'utf8');\n    const { data, content } = matter(fileContents);\n\n    return {\n      slug,\n      content,\n      ...data,\n    };\n  });\n\n  return allPostsData.sort((a, b) => (a.date < b.date ? 1 : -1));\n}\n\nexport function getPostBySlug(slug) {\n  const fullPath = path.join(postsDirectory, `${slug}.md`);\n  const fileContents = fs.readFileSync(fullPath, 'utf8');\n  const { data, content } = matter(fileContents);\n\n  return {\n    slug,\n    content,\n    ...data,\n  };\n}\n```\n\n### components/BlogPost.jsx - Markdown Renderer\n```javascript\nimport ReactMarkdown from 'react-markdown';\nimport remarkGfm from 'remark-gfm';\nimport remarkMath from 'remark-math';\nimport rehypeHighlight from 'rehype-highlight';\nimport rehypeKatex from 'rehype-katex';\nimport { \n  Box, \n  Heading, \n  Text, \n  Code, \n  Divider, \n  Link,\n  UnorderedList,\n  OrderedList,\n  ListItem,\n  useColorModeValue\n} from '@chakra-ui/react';\n\nconst BlogPost = ({ content }) => {\n  const blockquoteBg = useColorModeValue('gray.50', 'gray.700');\n  const inlineCodeBg = useColorModeValue('gray.100', 'gray.600');\n  const inlineCodeColor = useColorModeValue('gray.800', 'gray.100');\n\n  return (\n    <Box maxW=\"800px\" mx=\"auto\" p={6}>\n      <ReactMarkdown\n        remarkPlugins={[remarkGfm, remarkMath]}\n        rehypePlugins={[rehypeHighlight, rehypeKatex]}\n        components={{\n          h1: ({ children }) => (\n            <Heading as=\"h1\" size=\"2xl\" mb={6} mt={8}>\n              {children}\n            </Heading>\n          ),\n          h2: ({ children }) => (\n            <Heading as=\"h2\" size=\"xl\" mb={4} mt={6}>\n              {children}\n            </Heading>\n          ),\n          h3: ({ children }) => (\n            <Heading as=\"h3\" size=\"lg\" mb={3} mt={5}>\n              {children}\n            </Heading>\n          ),\n          p: ({ children }) => (\n            <Text mb={4} lineHeight=\"1.7\">\n              {children}\n            </Text>\n          ),\n          ul: ({ children }) => (\n            <UnorderedList mb={4} spacing={2}>\n              {children}\n            </UnorderedList>\n          ),\n          ol: ({ children }) => (\n            <OrderedList mb={4} spacing={2}>\n              {children}\n            </OrderedList>\n          ),\n          li: ({ children }) => (\n            <ListItem>{children}</ListItem>\n          ),\n          code: ({ inline, children, className }) => {\n            return inline ? (\n              <Code \n                px={2} \n                py={1} \n                bg={inlineCodeBg}\n                color={inlineCodeColor}\n                borderRadius=\"md\"\n                suppressHydrationWarning={true}\n              >\n                {children}\n              </Code>\n            ) : (\n              <Box mb={4}>\n                <Code\n                  as=\"pre\"\n                  display=\"block\"\n                  p={4}\n                  bg=\"gray.900\"\n                  color=\"white\"\n                  _dark={{ bg: \"gray.800\" }}\n                  borderRadius=\"md\"\n                  overflow=\"auto\"\n                  suppressHydrationWarning={true}\n                >\n                  {children}\n                </Code>\n              </Box>\n            );\n          },\n          a: ({ href, children }) => (\n            <Link href={href} color=\"blue.500\" isExternal>\n              {children}\n            </Link>\n          ),\n          blockquote: ({ children }) => (\n            <Box\n              as=\"blockquote\"\n              borderLeft=\"4px solid\"\n              borderColor=\"blue.500\"\n              pl={4}\n              py={2}\n              mb={4}\n              fontStyle=\"italic\"\n              bg={blockquoteBg}\n              _dark={{ bg: \"gray.700\", borderColor: \"blue.300\" }}\n              borderRadius=\"md\"\n              suppressHydrationWarning={true}\n            >\n              {children}\n            </Box>\n          ),\n          hr: () => <Divider my={6} />,\n        }}\n      >\n        {content}\n      </ReactMarkdown>\n    </Box>\n  );\n};\n\nexport default BlogPost;\n```\n\n### pages/writing.js - Blog Index\n```javascript\nimport NextLink from 'next/link'\nimport { Box, Container, Heading, Link, Text } from '@chakra-ui/react'\nimport Layout from '../components/layouts/article'\nimport Section from '../components/section'\nimport { getAllPosts } from '../lib/posts'\n\nconst Writing = ({ posts = [] }) => {\n  return (\n    <Layout title=\"Posts\">\n      <Container>\n        <Heading as=\"h3\" fontSize={20} mb={4} mt={5}>\n          Posts\n        </Heading>\n\n        {posts && posts.map((post, index) => (\n          <Section key={post.slug} delay={0.1 + (index + 1) * 0.1}>\n            <Box my={4}>\n              <Text fontSize=\"sm\" color=\"gray.500\" display=\"inline\" mr={3}>\n                {new Date(post.date + 'T00:00:00').toLocaleDateString('en-US', {\n                  year: 'numeric',\n                  month: 'short',\n                  day: 'numeric',\n                  timeZone: 'UTC'\n                })}\n              </Text>\n              <Link as={NextLink} href={`/blog/${post.slug}`}>\n                {post.title}\n              </Link>\n            </Box>\n          </Section>\n        ))}\n\n      </Container>\n    </Layout>\n  )\n}\n\nexport async function getStaticProps() {\n  try {\n    const posts = getAllPosts();\n    return {\n      props: {\n        posts,\n      },\n    };\n  } catch (error) {\n    console.error('Error in getStaticProps:', error);\n    return {\n      props: {\n        posts: [],\n      },\n    };\n  }\n}\n\nexport default Writing\n```\n\n### pages/blog/[slug].js - Individual Post Pages\n```javascript\nimport { getAllPosts, getPostBySlug } from '../../lib/posts';\nimport BlogPost from '../../components/BlogPost';\nimport { Box, Heading, Text } from '@chakra-ui/react';\n\nexport default function Post({ post }) {\n  return (\n    <Box>\n      <Box textAlign=\"center\" mb={8}>\n        <Heading as=\"h1\" size=\"3xl\" mb={4}>\n          {post.title}\n        </Heading>\n        <Text color=\"gray.600\">{post.date}</Text>\n      </Box>\n      <BlogPost content={post.content} />\n    </Box>\n  );\n}\n\nexport async function getStaticProps({ params }) {\n  const post = getPostBySlug(params.slug);\n  return {\n    props: {\n      post,\n    },\n  };\n}\n\nexport async function getStaticPaths() {\n  const posts = getAllPosts();\n  const paths = posts.map((post) => ({\n    params: { slug: post.slug },\n  }));\n\n  return {\n    paths,\n    fallback: false,\n  };\n}\n```\n\n## Markdown Features Supported\n\n### Basic Formatting\n- **Bold text** and *italic text*\n- Headers (H1, H2, H3)\n- Paragraphs with proper spacing\n- Horizontal rules\n- Links (internal and external)\n\n### Lists\n- Unordered lists with bullet points\n- Ordered lists with numbers\n- Proper spacing between items\n\n### Code\n- Multi-line code blocks with language-specific highlighting\n- Dark/light mode responsive styling\n\n### Blockquotes\n> Styled blockquotes with left border. Dark/light mode responsive\n\n### Mathematical Equations\n- Inline math: $E = mc^2$\n- Block equations: $$\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$$\n- Complex expressions with proper LaTeX rendering\n\n## Frontmatter Structure\n```yaml\n---\ntitle: \"Post Title\"\ndate: \"2025-01-15\"\nexcerpt: \"Brief description of the post\"\ntags: [\"tag1\", \"tag2\", \"tag3\"]\n---\n```\n\n## Development Workflow\n\n1. Create markdown files in posts/ directory\n2. Posts automatically appear on /writing page\n\n## Troubleshooting Notes\n\n- **Date Issues**: Use `new Date(post.date + 'T00:00:00')` to prevent timezone shifts\n- Inline equations not working","title":"Blog Implementation Summary","date":"2025-07-07","excerpt":"Complete guide to implementing a markdown blog with Next.js, Chakra UI, and mathematical equations","tags":["nextjs","react-markdown","chakra-ui","katex","blog"]},{"slug":"00_hello_world","content":"\n## Welcome to My Blog\n\nThis is my first blog post! I will just test functionality in this post, and write up how I implemented post functionality in a future post.\n\n## Code Example\n\n```python\ndef hello_world():\n    print(\"Hello, World!\")\n    return True\n``` \n\n## More Examples\n\n> This is a blockquote with some important information.\n\nCheck out [this link](https://example.com) for more info.\n\nHere's some **bold text** and *italic text*.\n\n### Lists\n\nUnordered list:\n- Item 1\n- Item 2\n- Item 3\n\nOrdered list:\n1. First item\n2. Second item\n3. Third item \n\n### Equations\nThis is an inline equation: $E = mc^2$.\n\nIn web animations, we often use trigonometric functions. The general form of a sine wave is:\n\n$$\ny(x,t) = A \\sin(kx - \\omega t + \\phi)\n$$\n\nWhere:\n- $A$ is the amplitude\n- $k$ is the wave number  \n- $\\omega$ is the angular frequency\n- $\\phi$ is the phase shift\n\nThis equation is perfect for creating smooth, natural animations in Three.js!\n\nThat's all for now!\n","title":"Hello World","date":"2025-07-06","excerpt":"This is my first blog post using react-markdown","tags":["react","nextjs","markdown"]}]},"__N_SSG":true}