{"pageProps":{"posts":[{"slug":"11_transformers_6","content":"*This is Part 6 of our GPT from scratch series. Having upgraded to modern tokenization in Part 5, we now tackle the data limitation by training on a much larger corpus and exploring pretraining + finetuning.*\n\n## Training on a Larger Corpus\n\nSince we have a larger model, we now want to train on a larger dataset. I'm still interested in generating poems, so let's use this [poetry dataset from Kaggle](https://www.kaggle.com/datasets/tgdivy/poetry-foundation-poems) which gives us a CSV file containing a list of poems from [Poetry Foundation](https://www.poetryfoundation.org/). \n\nWe first have to do some data cleaning with pandas to get the newline Unicode characters consistent, and concatenate all the poems into a single text file.\n\n```\nLine terminators found in the poems:\n----------------------------------------\nMac Classic (\\r only): 417494 occurrences\nDouble CR + LF (\\r\\r\\n): 379146 occurrences\nWindows (\\r\\n): 379146 occurrences\nReversed (\\n\\r): 19556 occurrences\nDouble CR (\\r\\r): 10527 occurrences\nLine Separator (U+2028): 419 occurrences\nParagraph Separator (U+2029): 1 occurrences\n----------------------------------------\n\nSuccessfully concatenated 13854 poems into 'all_poems_cleaned.txt'\n\nSample cleaning (poem at index 101):\nOriginal: '\\r\\r\\nThe sound of a guitar drifts through the air.\\r\\r\\nCupped in my hand, a snowflake quivers lightly.\\r\\r'...\nCleaned: 'The sound of a guitar drifts through the air.\\nCupped in my hand, a snowflake quivers lightly.\\nThick '...\n```\n\nThis file is around 20MB and 400,000 lines long, which is 20 times the size and 10 times the number of lines of the Shakespeare dataset.\n\n## Training Results on Poetry Dataset\n\n```\nstep 0: train loss 10.8247, val loss 10.8255\nstep 100: train loss 6.2405, val loss 6.5146\nstep 200: train loss 5.7141, val loss 6.2657\nstep 300: train loss 5.5298, val loss 6.1561\nstep 400: train loss 5.4678, val loss 5.8279\nstep 500: train loss 5.5526, val loss 5.7174\nstep 600: train loss 5.2497, val loss 5.6578\nstep 700: train loss 5.1549, val loss 5.6028\nstep 800: train loss 5.0009, val loss 5.7194\nstep 900: train loss 5.2021, val loss 5.5778\nstep 1000: train loss 4.8988, val loss 5.5454\nstep 1100: train loss 4.9935, val loss 5.4814\nstep 1200: train loss 4.9520, val loss 5.3551\nstep 1300: train loss 5.0129, val loss 5.3685\nstep 1400: train loss 4.9534, val loss 5.3807\nstep 1500: train loss 4.7947, val loss 5.3912\nstep 1600: train loss 5.0557, val loss 5.2993\nstep 1700: train loss 4.8419, val loss 5.2611\nstep 1800: train loss 4.7657, val loss 5.0324\nstep 1900: train loss 4.4551, val loss 5.0815\nstep 2000: train loss 4.6674, val loss 5.1306\nstep 2100: train loss 4.6318, val loss 5.0484\nstep 2200: train loss 4.6357, val loss 5.2188\nstep 2300: train loss 4.5006, val loss 4.9910\nstep 2400: train loss 4.3960, val loss 5.0353\nstep 2500: train loss 4.4227, val loss 5.0051\nstep 2600: train loss 4.4193, val loss 4.9974\nstep 2700: train loss 4.4097, val loss 4.9683\nstep 2800: train loss 4.2323, val loss 4.9965\nstep 2900: train loss 4.3671, val loss 4.8900\nstep 3000: train loss 4.3548, val loss 4.8972\nstep 3100: train loss 4.3923, val loss 4.7957\nstep 3200: train loss 4.4397, val loss 4.7694\nstep 3300: train loss 4.3573, val loss 4.9981\nstep 3400: train loss 4.3772, val loss 4.8391\nstep 3500: train loss 4.3908, val loss 4.8069\nstep 3600: train loss 4.2172, val loss 4.7400\nstep 3700: train loss 4.1613, val loss 4.8643\nstep 3800: train loss 4.2559, val loss 4.9014\nstep 3900: train loss 4.0760, val loss 4.7784\nstep 4000: train loss 3.9681, val loss 4.7814\nstep 4100: train loss 4.0662, val loss 4.8351\nstep 4200: train loss 4.1156, val loss 4.9005\nstep 4300: train loss 4.1036, val loss 4.7409\nstep 4400: train loss 4.0790, val loss 4.6941\nstep 4500: train loss 4.1168, val loss 4.8207\nstep 4600: train loss 3.8932, val loss 4.7466\nstep 4700: train loss 4.0971, val loss 4.9108\nstep 4800: train loss 4.0825, val loss 4.8164\nstep 4900: train loss 4.0249, val loss 4.7728\nstep 4999: train loss 4.0157, val loss 4.7919\n```\n\nThe model generates much more diverse and poetic text:\n\n```\n! but ...  \nbeautise on a head, everything's\nserved, seemed—\nLewash on the back of the garbled pllesh,\nare not to be vigilant\nby food, tears, eyes burnt,\ndust and screaming. Just say —I've brought\nGrandter aesthetics—\nmaybe God like John Fanny or that.7.\nEverything's that alonghed in.\nButoried to me\nwhen he yelled.\n\nJust urboilherer appeared. Then he were falling, sits crying and prayed to his brother, being missing. They started on time they went up, though\nGod said his father had to stop a good reporters\nstand, locked, hooves, feet up tighter, \"Is smooth?\"\nThe Morning\nSeason explores the world of the walls\nGazels through the light. Later each day,\nA reasonably thing is beauties\nAt evening a while Frains unself from a walk.Zusted in light old\nThe way used to the land. On the snow?\nThere said, \"Not offensive.\" \"but that's the trietry, \"Nor does\nsay, but it\" with footage,\n Maoaching the crowd.\"You like the power actual\nor on in their apartmentrelices, and the vinyl themselves lie together by that the Jewish sound.\" So, in one hand,\nyou could folding it backdroaches and wall you made\nhad keep what'﻿s done.\" and I stared, picking a grump offpour, and stared on the smoke, tapped to a wheel and the twos winkboat.\nThe house was a cigar on my stopes of dilelict and\nthose threatened hunger from how to be at moving steadily, could never believe\nas white but a man with pens would not take  to. I could remember that night.\nThe moon was thick, the wind rivening the waves of the sea, the swift dark whiteerly recedes\nfrom are it and its silkency of    paths comprising sleep, a rail of o'clock. Outspread it somewhere,\nand if it ever repeated; soon, just angularling forward, the magager and schimology.The rifle wasn't wobbling paths and blinked,\n```\n\n## Further Training Attempts\n\nTo try and further improve, we try training longer and with a learning rate that is ten times smaller. However, both training and validation seem to have saturated. We also try a cosine learning rate with multiple restarts, but the results are not very good:\n\n```python\nstep_lr = config.learning_rate*np.cos(0.5*np.pi*iter/(config.max_iters-1))\nfor group in optimizer.param_groups:\n  group['lr'] = step_lr\n```\n\n```\nstep 0: train loss 10.8202, val loss 10.8287\n...\nstep 3000: train loss 4.5538, val loss 5.0448\n...\nstep 1000: train loss 4.5272, val loss 4.9344\n...\nstep 999: train loss 4.2557, val loss 4.8072\n...\nstep 999: train loss 4.0713, val loss 4.7289\n...\nstep 999: train loss 4.1581, val loss 4.8483\n```\n\nNext, we try reshuffling the training and validation split of the input data by using the first 10% for validation instead. We find that training loss is now higher, another sign we were overfitting. After further training, we end up with training loss and validation loss being about 4. The final result is decent:\n\n```\nSummer dazed\nwith its theatrical designs, tubes, guts.\nI clump the bripscrewed black.\nI share the precise whatever\nnot actually needed.\nI say when it grazed the time,\nthings myself between them\nand there's little wings forth\nEnglish stains, the slick cold force.\nRunning the basement that moved\nnever. I wonder whether it meant\nthat this sad adventure within and\nunlemishes the dead.\nI was writing that Old Friend was\nalways working on I have\nseen many plans seeing unsatisfied,\nshe was always ashamed\nof this region.\nI think of the Second Coming\nasleep clear as I thought\nwaswitch recognized yet because of himself\nwere not afraid of me\nbut this sequence after the war\nthat had been doing to me\ndon't hurt me I meant so many years ago.\nI wasn't my brother.\n```\n\n## Finetuning on Shakespeare\n\nLastly, we try finetuning on the tiny Shakespeare dataset we started with. We have essentially pretrained our larger model on the larger poetry dataset and now use a much lower learning rate (3e-6) and 10,000 training iterations to refine on the smaller tiny Shakespeare dataset.\n\n```\nstep 10000: train loss 3.1057, val loss 4.1152\n```\n\nThis is much better than when we just tried to train our larger model with the GPT-2 tokenizer on the Shakespeare data alone. This is the advantage of pretraining! Here are the results:\n\n```\nIs he contented himself with his exercise,\nEven he that fall'd upon his tent with him\nThe ground o'er his bed, his dole from the north,\nAnd cries 'He well.'\nAnd Tranio, which consummate honour, in turn\nProclaims him on his way; and he cried,\nLay him that comments mortals play out.\n\nLADY CAPULET:\nMy lords, I will not hear my traitor.\n\nDUKE VINCENTIO:\nWrut, away! thou shalt never weep;\nAnd yet I have received thy royal presence,\nHaving done the corse. But thou'll hear be\nMy father's son, blessing in the cruel gown,\nthough mine own buried heir\nOur father's poor son, my poor daughter, my child,\nWhere I am resign'd me; and I will be king,\nAnd pain'd the house of this churchyard call.\nThy father, and I seek thee here;\nSo, newly wench'd with that great aspect,\nAnd thy servile' mortal presence made me,\nAnd thou wouldst board thy mortal bones\nIn readiness, he may hear it bear.\nAh, art thou law to be made my nurse.\n\nLADY CAPULET:\nAy, I'll win thee would not woe awhile.\n\nDUKE OF YORK:\nO gentle Plantagenet queen,\n\nISABELLA:\nAh, my lord, thou art safe and younger!\n\nDUKE OF YORK:\nFarewell with us, bleated those that have said,\nThat we should plague thee for many days.\n\nKING RICHARD II:\nGive me my father to your brother's house,\nOr else send him by your fortunes to the street.\nWhat art thou slain?\n\nGLOUCESTER:\nHarry of York, I tell thee that kill thy life:\nAnd thou, contracted hither to my farm,\nThou art so happy to be repelling'd at thy mind!\nAs on a day that pageant thou sufficiency,\nNow let thy father speak before thy function.\n\nKING RICHARD II:\nO coward to this day; there comes Hereford with thee!\n```\n\nWow! While it's still not perfectly coherent, it seems much better than our output from the character-level tokenizer. Some characters even have multiple lines!\n\n## Key Insights\n\nThis part demonstrates several important concepts in modern language model training:\n\n1. **Dataset Scale Matters**: The larger poetry dataset (20x bigger) significantly improved text quality and diversity.\n\n2. **Data Cleaning is Critical**: Proper handling of line terminators and text formatting is essential for good training data.\n\n3. **Pretraining + Finetuning Works**: The two-stage approach of pretraining on a large diverse corpus, then finetuning on a specific domain, produces better results than training on the target domain alone.\n\n4. **Overfitting Remains a Challenge**: Even with more data, the model still shows signs of overfitting, indicating we need even more data or better regularization.\n\n5. **Modern Tokenization Benefits**: Subword tokens produce more natural-looking text even when the model isn't perfectly trained.\n\nThe progression from character-level Shakespeare generation to subword-level poetry and then Shakespeare finetuning shows the power of scaling both model size and training data in the right sequence.","title":"Building GPT from Scratch - Part 6: Pretraining and Finetuning","date":"2025-08-11","excerpt":"","tags":["transformers","tutorial"]},{"slug":"10_transformers_5","content":"*This is Part 5 of our GPT from scratch series. Having built and optimized our transformer in Parts 1-4, we now upgrade from character-level tokenization to modern subword tokenization using tiktoken.*\n\n## What is Tokenization?\n\n**Tokenization** is the process of breaking text into smaller units called *tokens*. A *tokenizer* handles this task and assigns each token a unique integer ID from its vocabulary. Andrej Karpathy gives a great overview of tokenization in [this lecture](https://www.youtube.com/watch?v=zduSFxRajkE).\n\n## Tokenizer Vocabulary\n\nThe tokenizer's vocabulary is the full set of tokens it knows how to process. Tokenizers are *trained* on large text corpora—though this training differs from training a neural network. You can even train your own tokenizer and control its vocabulary size and tokenization rules.\n\n## How Strings Are Tokenized\n\nIn English, tokens usually range from single characters to whole words—like `\"t\"` or `\" great\"`. Different languages and models may result in longer or shorter tokens. Most modern LLMs use *subword* tokenization methods, which balance vocabulary size with coverage of possible inputs. Some common tokenization algorithms include **Byte Pair Encoding (BPE)**, **WordPiece** and **Unigram**.\n\n## Why Tokenization Matters\n\nIf your input text includes tokens *not* in the tokenizer's vocabulary, things can go wrong—often in frustrating or subtle ways. Many quirks and failures in LLM behavior can be traced back to poor or unexpected tokenization.\n\n> Tokenization issues are a major source of bugs in LLM applications.\n\n## Using Tiktoken\n\nWe'll use [`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md), OpenAI's fast, open-source tokenizer. Given a text (e.g., `\"tiktoken is great!\"`) and an encoding (e.g., `\"cl100k_base\"`), it produces a tokenized output like:\n\n```python\n[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]\n```\n\nYou can test tokenizations interactively using:\n\n- [**OpenAI Tokenizer Tool**](https://beta.openai.com/tokenizer)\n- [**Tiktokenizer Web App**](https://tiktokenizer.vercel.app/)\n\nThis is as simple as letting\n\n```python\nenc = tiktoken.encoding_for_model(\"gpt-2\")\n```\n\nand then calling `enc.encode` and `enc.decode`.\n\n## Scaling Challenges with Larger Vocabularies\n\nLet's try `o200k_base`, which is used in `gpt-4o`. The vocab size is now 200,000 and our number of parameters in the model jumps to 164M! This is because the embedding and unembedding layers scale with the vocab size. Since `o200k_base` is too big, let's try the GPT-2 model which has a vocab size of around 50,000. Our model is now reduced to 50M parameters, still much bigger than what we had previously, with most of the weights in the embedding and unembedding layers. \n\nCurrently, our dataset that we train on is small, so this is overkill. Most tokens probably don't even appear! We can see this disparity when we try to train the model:\n\n```\nstep 0: train loss 10.8323, val loss 10.8315\nstep 500: train loss 4.8271, val loss 5.3269\nstep 1000: train loss 3.9008, val loss 4.8286\nstep 1500: train loss 3.3493, val loss 4.7968\nstep 2000: train loss 2.7602, val loss 4.9501\nstep 2500: train loss 2.1606, val loss 5.2748\nstep 3000: train loss 1.4815, val loss 5.7240\n```\n\nThe validation loss is terrible, i.e. next token prediction is bad. Note that our tokens no longer correspond to individual characters. The training loss keeps going down since we now have an even more extreme example of overfitting to the data with increased parameters. \n\n## Improved Text Generation\n\nInterestingly, the output is still quite readable since tokens are sub-word and not character level, showing the advantage of an increased vocabulary:\n\n```\nwhat stay to-morrow! I must desire her:\nHard shrowa thousand pound a book\nOur friend. The one cockues!\n\nFLORDS:\nAy, good testimony, come.\n\nServant:\nMy lord!\n\nMIONwell, my lord, the heavens to cry't: look down,\nAnd occupations speak again, I do come on,\nIs't speak to see you. All my lord--\nAlack, your lords! my lords, who, I have\nUpon my heart:--your convey him, my lord,\nMowius; from my blood against your hand,\nOut on your stables there! O injurious sin!\n\nISABELLHORIZELO:\nStay, good heaven;\nMy soul is some good's liver for her true self,\nWhose settled good edencing trade here.\n```\n\nDespite the overfitting issues, the subword tokenization produces more natural-sounding text compared to our character-level approach. Words are more complete and the overall flow is more coherent.\n\n## Key Takeaways\n\nWe have improved the tokenization part of our model, but we need to train on a larger dataset to properly utilize the expanded vocabulary. The main insights from this upgrade:\n\n1. **Modern tokenization** uses subword algorithms that balance vocabulary efficiency with coverage\n2. **Vocabulary size directly impacts model parameters** - larger vocabularies mean more embedding parameters\n3. **Dataset size must match vocabulary size** - small datasets can't effectively train large vocabularies\n4. **Subword tokens produce more natural text** even when the model is overfitting\n\nIn the next section, we'll address the dataset limitation by training on a much larger corpus to properly leverage our improved tokenization.","title":"Building GPT from Scratch - Part 5: Tokenization","date":"2025-08-08","excerpt":"","tags":["transformers","tutorial"]},{"slug":"09_transformers_4","content":"*This is Part 4 of our GPT from scratch series. Having built a complete transformer in Parts 1-3, we now look at further optimizations for more efficient training.*\n\n## Restructuring and Configuration\n\nNow that we have successfully implemented a transformer network, we can make a number of adjustments to optimize for more efficient training.\n\nFirst, we restructure our script and put all hyperparameters into a config class:\n\n```python\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'using device: {DEVICE}')\n\nconfig = Config(\n  batch_size = 64,\n  block_size = 64,\n  max_iters = 5000,\n  eval_interval = 500,\n  learning_rate = 3e-4,\n  eval_iters = 10,\n  n_embd = 128,\n  n_head = 4,\n  n_layer = 6,\n  dropout = 0.2,\n  vocab_size = vocab_size,\n)\n\nmodel = GPTLanguageModel(config, device=DEVICE)\nmodel.to(DEVICE)\nmodel = torch.compile(model) # requires PyTorch 2.0\n```\n\nAs you can see, we also compile the model before training to improve training speed.\n\n## Efficient Multi-Head Attention\n\nThe next big change we make is to compute the keys, queries, and values for the multiple heads all in a single matrix multiplication. This means we combine the 'Head' and 'Multi-Head' classes into one single class, which processes the heads as a new batch dimension.\n\n```python\nclass MultiHeadAttention(nn.Module):\n  \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n  def __init__(self, config):\n    super().__init__()\n    self.n_head = config.n_head\n    self.n_embd = config.n_embd\n    self.dropout = config.dropout\n\n    # key, query, value projections for all heads, but in a batch\n    self.attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n    self.proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n    self.attn_dropout = nn.Dropout(config.dropout)\n    self.resid_dropout = nn.Dropout(config.dropout)\n    self.register_buffer(\"bias\",\n      torch.tril(torch.ones(1, 1, config.block_size, config.block_size)))\n\n  def forward(self, x):\n    B, T, C = x.size()\n    q, k, v  = self.attn(x).split(self.n_embd, dim=2)\n    # reshape and move head dimension forward using einops\n    k = rearrange(k, 'b t (nh hs) -> b nh t hs', nh=self.n_head)\n    q = rearrange(q, 'b t (nh hs) -> b nh t hs', nh=self.n_head)\n    v = rearrange(v, 'b t (nh hs) -> b nh t hs', nh=self.n_head)\n\n    att = (q @ k.transpose(-2, -1)) * k.size(-1)**-0.5\n    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n    att = F.softmax(att, dim=-1)\n    att = self.attn_dropout(att)\n    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n\n    # re-assemble all head outputs side by side\n    y = rearrange(y, 'b nh t hs -> b t (nh hs)')\n    y = self.resid_dropout(self.proj(y))\n    return y\n```\n\n## Flash Attention\n\nFinally, we implement flash attention, which reorders the attention computation such that computations can be tiled which greatly speeds up the attention step.\n\n```python\n# In MultiHeadAttention class\nself.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n# ... in forward pass ...\ny = torch.nn.functional.scaled_dot_product_attention(q, k, v, \n                                                     attn_mask=None, \n                                                     dropout_p=dropout_p,\n                                                     is_causal=True)\n```\n\n## Training Performance and Scaling\n\nTraining on a T4 GPU, a 1.2M parameters model now trains in 2 minutes. We are ready to scale up to the size shown in the tutorial video:\n\n```python\nconfig = Config(\n  batch_size = 64,\n  block_size = 256,\n  max_iters = 5000,\n  eval_interval = 500,\n  learning_rate = 3e-4,\n  eval_iters = 10,\n  n_embd = 384,\n  n_head = 6,\n  n_layer = 6,\n  dropout = 0.2,\n  vocab_size = vocab_size,\n)\n```\n\nThis has 10 million parameters and trains in 30 minutes! Let's take a look at the output:\n\n```\nBRAKENBURY:\nDespite of this seven sit in the noble\nOf Lord Hastings, and my grave is my charged mine;\nFor George shall not speak not pass'd it:\nThe valour upon it. Is deliver'd it with me?\n\nBRUTUS:\nYea, beggar, by your voices and hearts,\nBut since she changed in your packs and bloody,\nYour joy your might in him writ\nBe punk'd between pains. I am struckcomment\nTo son I writ you that yet you did love:\nIf it were example to your knees to express\nBy your fleships of state? Exeter, me\nAnd wring, my son, come on, my sorrow ladys,\nWhose profession joy wings and me down,\n```\n\nPretty good! Most words are recognizable and the sentences have more structure. The overall form is also consistent with the training data with readable names (Brutus is a familiar one!)\n\n## Bias-Variance Trade-off\nIf we plot the losses against training steps for our small 0.3M model and our final 10M model:\n![Validation loss comparison](/images/training_validation_comparison.png)\nThe larger model generalizes better (has a lower validation loss) but overfits more (training loss is much lower than validation loss). This represents a classic **bias-variance trade-off** in neural scaling. The larger model's increased parameter capacity enables better feature representation learning and pattern recognition, resulting in lower validation error. However, this same capacity creates **memorization potential** for training data.\n\n## Conclusion\n\nThese optimizations demonstrate how production transformer implementations differ from educational versions. The key improvements - configuration management, batched attention computation, model compilation, and flash attention - provide significant training speedups while maintaining the same underlying architecture.\n\nThe scaling results show that our simple transformer can achieve reasonable quality when given sufficient parameters and training time, following the same principles that power modern large language models.","title":"Building GPT from Scratch - Part 4: Further Optimizations","date":"2025-08-07","excerpt":"","tags":["transformers","tutorial"]},{"slug":"08_transformers_3","content":"*This is Part 3 of a 3-part series on building a Generative Pretrained Transformer from scratch. In Parts 1 and 2, we built the data pipeline and attention mechanism. Now we'll complete our transformer with the remaining key components.*\n\n## Feed-Forward Networks\n\nAfter our attention mechanism learns to communicate between tokens, we need to give the model computation time to process what it has learned. This is where feed-forward networks come in - they're simply MLPs with ReLU activation that allow the model to \"think\" about the information it just gathered.\n\n```python\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n\nWe add this to our model after the attention:\n\n```python\n# In our model:\nself.sa_head = MultiHeadAttention(4, n_embd//4)\nself.ffwd = FeedFoward(n_embd) # MLP with RELU\n\n# In forward pass:\nx = self.sa_head(x)\nx = self.ffwd(x)\nlogits = self.lm_head(x)\n```\n\n## Transformer Blocks: Communication + Computation\n\nThe pattern of attention (communication) followed by feed-forward (computation) is so fundamental that we package it into reusable **Transformer Blocks**. These blocks can be stacked to create deeper networks that alternate between communication and computation phases.\n\n```python\nn_head = 4\nn_layer = 2\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n```\n\nWe can then stack multiple blocks:\n\n```python\n# In our model:\nself.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n\n# In forward pass:\nx = tok_emb + pos_emb # (B,T,C)\nx = self.blocks(x)\nlogits = self.lm_head(x)\n```\n\nAt this point, our model starts generating recognizable English words!\n\n```\nRILELE\nMAssell use you.\nNt I RYan lake off.\n\nFicink'd\nhote carewtledeche quie to whanl Gatt Mejesery ely:\nThe if, bet leveses it theave be ry skit you file.\n\nKay wred tome dake stance, suks,\nAdech JORo!\n\nALOUSBET:\nWis brake grourst and ald creapsss,\nAndite\nnoat,\nAmothery are doreast is\n```\n\n## Training Optimizations\n\nAs our network gets deeper with multiple transformer blocks, we need optimizations to improve training stability and performance.\n\n### Residual Connections\n\nThe first optimization is **residual/skip connections**, introduced in the [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) paper. These connections allow gradients to flow directly through the network, avoiding training bottlenecks in deep networks.\n\nInstead of 'x = self.sa(x)', we do 'x = x + self.sa(x)'. The computation now returns a residual that gets added to the original input:\n\n```python\nclass Block(nn.Module):\n    def forward(self, x):\n        x = x + self.sa(x)  # residual connection\n        x = x + self.ffwd(x)  # residual connection\n        return x\n```\n\n### Projection Layers\n\nWe also add projection layers to map back to the embedding space after our multi-head attention and expand our feed-forward networks as in the original paper:\n\n```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),  # expand by 4x as in the paper\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),  # project back down\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n\n### Layer Normalization\n\nLayer normalization helps stabilize training by normalizing inputs to have zero mean and unit variance. We apply it before both the self-attention and feed-forward computations in each block, and once more after all blocks:\n\n```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n```\n\n### Dropout\n\nFinally, we add **dropout** for regularization, which randomly sets some activations to zero during training to prevent overfitting:\n\n```python\nclass FeedFoward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),  # add dropout\n        )\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n```\n\n## Final Model and Hyperparameters\n\nWe now have a complete decoder-only transformer ready to scale and train! Here are the hyperparameters for our final model:\n\n```python\n# hyperparameters\nbatch_size = 32\nblock_size = 32\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\neval_iters = 200\nn_embd = 64\nn_head = 6\nn_layer = 6\ndropout = 0.2\n```\n\n## Final Results\n\nAfter training our complete transformer, we get much more structured output:\n\n```\nDOF GlLN:\nThey good, then then, bladgy bone not thindnes\nI way Jeain, fainly!\n\nISABELLA:\nI way, thou fourd to havary flown dews'm-sine.\n\nNowZALLET:\nWhich here old thy's warring hiod\nOn dearys hory be wive to more; greseli!\n\nBut nighd Wart, prance:\nBarch,\nAnd prayem not welld she you, coldinger:\nO I the oldst God somed:\nSirry is let never To be whith, I new'n be thy limpiny\nWord: where deblitiss for give upon the conqueennifult,\nAnd so pobeterl. by they she thy truge,\nIf you his let a brotess.\n```\n\nWhile still not perfect, the improvement is dramatic! The model now:\n- Uses proper character names (ISABELLA, etc.)\n- Maintains somewhat consistent dramatic structure\n- Shows more coherent word formation\n- Demonstrates longer-range dependencies\n\n## What We've Accomplished\n\nIn this three-part series, we've built a complete transformer from scratch:\n\n1. **Part 1**: Data loading, tokenization, and a simple bigram baseline\n2. **Part 2**: The attention mechanism - the core innovation of transformers\n3. **Part 3**: Complete transformer architecture with all the essential components\n\n### Key Components We've Implemented:\n\n- **Self-attention mechanisms** for learning relationships between tokens\n- **Multi-head attention** for capturing different types of relationships\n- **Feed-forward networks** for computation after communication\n- **Transformer blocks** that stack attention and computation\n- **Residual connections** for stable training of deep networks\n- **Layer normalization** for training stability\n- **Positional embeddings** so the model understands sequence order\n- **Dropout** for regularization and preventing overfitting\n\n## The Path Forward\n\nOur small transformer demonstrates the core principles, but to achieve GPT-level performance, you'd need to scale up significantly:\n\n- **More parameters**: Modern language models have billions of parameters\n- **More data**: Training on much larger text corpora\n- **More compute**: Training for weeks or months on powerful hardware\n- **Better tokenization**: Using subword tokenizers like BPE or SentencePiece\n- **Longer context windows**: Supporting much longer sequences\n\n## Key Takeaways\n\n1. **Attention is the core innovation**: The self-attention mechanism allows tokens to communicate and share information based on content rather than just position.\n\n2. **Transformers are surprisingly simple**: The architecture is just attention + feed-forward blocks stacked together with some normalization and residual connections.\n\n3. **Scale matters**: The same architecture that gives us semi-coherent Shakespeare can generate human-level text when scaled up with more parameters, data, and compute.\n\n4. **Training stability is crucial**: Residual connections, layer normalization, and proper weight initialization are essential for training deep networks.\n\n## Conclusion\n\nThis tutorial has shown how to build a transformer model from scratch using PyTorch. We've implemented all the key components of the decoder transformer architecture and seen how they work together to create a language model capable of generating structured text.\n\nThe progression from random gibberish (bigram model) to semi-coherent Shakespeare-like text (full transformer) demonstrates the power of the attention mechanism and proper architectural choices. While our small model doesn't generate fully coherent text yet, scaling up the parameters, training data, and computation would lead to increasingly capable language models.\n\nThe complete nanoGPT implementation, with additional optimizations and the ability to train larger models, can be found in [Andrej Karpathy's repository](https://github.com/karpathy/nanoGPT). This serves as an excellent foundation for understanding and experimenting with transformer architectures.","title":"Building GPT from Scratch - Part 3: Building the Complete Transformer","date":"2025-08-06","excerpt":"","tags":["transformers","tutorial"]},{"slug":"07_transformers_2","content":"*This is Part 2 of a 3-part series on building a Generative Pretrained Transformer from scratch. In Part 1, we built a simple bigram model. Now we'll introduce the core innovation that makes transformers work: Attention.*\n\n## The Mathematical Foundation: Weighted Aggregation\n\nBefore diving into attention, let's understand a neat mathematical trick that forms its foundation. By multiplying a row-normalized T×T lower triangular matrix with a T×C matrix, we can get a T×C matrix which calculates a running average of the preceding rows.\n\nWhen we combine this with softmax (setting -inf instead of 0), we get a more general weighted aggregation of past elements:\n\n```python\n# Weighted aggregation with softmax\ntorch.manual_seed(1337)\nB,T,C = 4,8,2\nx = torch.randn(B,T,C)\n\ntril = torch.tril(torch.ones(T,T)) # lower triangular\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril==0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nres = wei @ x\nres.shape\n```\n\nThis is the mathematical foundation of attention - we're learning to compute weighted averages of past tokens, where the weights are determined by the content of the tokens themselves.\n\n## Single Head Self-Attention\n\nNow for the meat of transformers: the attention mechanism. We create an attention space of dimension 'head_size'. We project each token into this space using three matrices: **Key (K)**, **Query (Q)**, and **Value (V)**.\n\nHere's the intuition:\n- **Query (Q)**: What the token is looking for\n- **Key (K)**: What the token contains/offers\n- **Value (V)**: What the token communicates if paid attention to\n\nWhen K and Q are similar (measured using dot product), they have high affinity and receive higher attention weights. We don't aggregate tokens directly - we aggregate V(X), which represents what each token communicates when attended to.\n\nThe normalization by 1/√(head_size) is crucial. When Q and K have unit variance, this ensures the attention weights also have unit variance. Without this normalization, softmax would saturate towards one-hot vectors, meaning tokens would only aggregate information from a single previous token.\n\n```python\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ rearrange(k, 'b t h -> b h t') * C**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n```\n\nNote that we register 'tril' as a buffer since it's not a learnable parameter of the model.\n\n## Important Notes About Attention\n\nHere are some crucial points about how attention works:\n\n- **Note 1**: Attention is a communication mechanism. You have a directed graph and you want every node to take weighted sums of every node pointing to it.\n\n- **Note 2**: Positional encoding must be embedded in the nodes, since the structure of the attention mechanism doesn't inherently understand position.\n\n- **Note 3**: Batches are independent - there's no cross-communication between different sequences in a batch.\n\n- **Note 4**: Here we ignore future tokens using triangular masking. If you were doing sentiment analysis, you might want to allow this communication. Encoder blocks would allow bidirectional attention, decoder blocks would not. The difference is just the triangular masking.\n\n- **Note 5**: This is called \"self-attention\" because K, Q, V all come from the same input X. If the keys and values came from somewhere else, you'd have \"cross-attention\", which is used when there's a separate source of information to pull from.\n\n## Building the Complete Single-Head Model\n\nLet's put everything together into a complete model with single-head attention. We'll make several improvements to our training setup:\n\n```python\n# hyperparameters\nbatch_size = 32\nblock_size = 8 \nmax_iters = 5000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32\ntorch.manual_seed(1337)\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n```\n\nKey improvements in this version:\n\n1. **GPU Support**: We move data and model to GPU using 'device = 'cuda''\n2. **Better Loss Estimation**: 'estimate_loss()' averages losses over multiple batches for more accurate estimates\n3. **No Gradient Context**: '@torch.no_grad()' tells PyTorch this function won't be backpropagated through\n4. **Embedding Dimension**: Our logits are now embedded in an embedding dimension, so we need an \"unembedding\" linear layer\n5. **Positional Embeddings**: We add position information so tokens know where they are in the sequence\n\n```python\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_head = Head(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.sa_head(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```\n\n## Results with Single-Head Attention\n\nAfter training, we get improved results that show more structure, though still not quite Shakespeare:\n\n```\nArrg hor; ho, ho aak.\n\nLUCKUCARE Fqhelree ndel te:\nThece waplt ko.\nFOCI\nMHABRIEKY tur waverer orid betievis dyof b.\n\nBo,\nABIUCE:\nN-, evesune athid nt cobasr!, Go hern, alsemin rsin varit ther I;\nANher en:ouingaroua lis py Bh mithe ast prird band bad youun theis pioat hed man ile ere sty hanonoue avillars d ty hon I:\nSo Ilar chy w'guma my tono yat d,\nI more hathive, ha utithe baag tee\nIile athadog at y hoke hay hate whinsintoourtarenol usdon co whe sou; mer wif yoh hlele coud illesen?o I choteban\n```\n\nThe attention mechanism has given our model the ability to look back at previous characters and make more informed predictions. We can see more English-like structure emerging.\n\n## Multi-Head Attention\n\nInstead of using a single attention head, we can use multiple smaller heads in parallel. This allows the model to attend to different types of relationships simultaneously - some heads might focus on syntax, others on semantics, etc.\n\n```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n```\n\nIn our model, we replace the single head with:\n```python\nself.sa_head = MultiHeadAttention(4, n_embd//4)\n```\n\nThis gives us slightly better loss and more diverse attention patterns.\n\n## What's Next?\n\nWe now have a working attention mechanism! However, our model is still missing several key components that make modern transformers work well:\n\n- **Feed-forward networks** for computation after attention\n- **Transformer blocks** that repeat the attention + computation pattern\n- **Residual connections** and **layer normalization** for training stability\n- **Dropout** for regularization\n\nIn Part 3, we'll add these final components and see our model start generating much more coherent text. The attention mechanism we've built here is the foundation - everything else builds on top of it to create the full transformer architecture.","title":"Building GPT from Scratch - Part 2: The Attention Mechanism","date":"2025-08-05","excerpt":"","tags":["transformers","tutorial"]},{"slug":"06_transformers_1","content":"\n*This is Part 1 of a 3-part series on building a Generative Pretrained Transformer from scratch, based on Andrej Karpathy's excellent tutorial.*\n\n## Introduction\n\nThe goal of this series is to build a 'Generative Pretrained Transformer' from scratch, following the attention architecture from the original 2017 [**Attention is all you need**](https://arxiv.org/abs/1706.03762) paper and OpenAI's GPT-2. We'll be building a character-level generative model - essentially a next character predictor that can generate Shakespeare-like text.\n\nThis tutorial is based on Andrej Karpathy's video [**Let's build GPT: from scratch, in code, spelled out**](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=10s), which has gained 6 million views in 2 years! Transformers get their name from their original use case of machine translation.\n\n## Data Loading\n\nThe first step in building any language model is preparing our data. We'll use the tiny Shakespeare dataset for this tutorial.\n\n```python\n# Download tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# read and inspect the file\nwith open('input.txt', 'r') as f:\n  text = f.read()\nprint(\"Length of dataset in characters: \", len(text))\nprint(text[:1000])\n```\n\n### Tokenization\n\nThe first interesting concept is **tokenization**. We want to convert our input text into integers that our model can work with. The model will predict the next integer, which we can then decode back into a character.\n\nOur simple approach creates a dictionary of all available characters (including newline) and assigns each an integer. For more sophisticated treatments, you'd want to look at SentencePiece by Google or tiktoken by OpenAI. There's always a tradeoff between dictionary size and encoding lengths.\n\n```python\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(f\"{vocab_size} characters in dictionary : {''.join(chars)}\")\n# First one is a new line!\n\n# Create a mapping from characters to integers (A Tokenizer)\n# This is called tokenizing. Google uses SentencePiece. OpenAI and GPT uses tiktoken\nitos = { i:ch for i, ch in enumerate(chars)}\nstoi = { ch:i for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s] # encoder: convert string to list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: convert list of integers to string\n\n# Now encode the whole text dataset and store it as a pytorch Tensor\nimport torch\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:100])\nprint(decode([int(v) for v in (data[:100].data)]))\n\n# Split the data into training and validation sets\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n```\n\n### Block Size and Context Windows\n\nThe second important concept is **block size**, which sets the context window length. This is the size of the 'memory' of the model. A larger block size would give more long-range associations, but is probably more expensive to train and would require more data.\n\nWith block size defined, we can input batch_size rows of block_size integers for each data batch. Training in batches is faster since multiple independent blocks can be processed simultaneously.\n\n```python\nblock_size = 8 # or alternatively context lengths\ntrain_data[:block_size+1]\n# in a sample of 9 characters, we have 8 examples with different context lengths.\n\ntorch.manual_seed(1337)\nbatch_size = 4\nblock_size = 8\n\ndef get_batch(split):\n  # generate a small batch of data of inputs x and targets y\n  data = train_data if split == 'train' else val_data\n  ix = torch.randint(len(data) - block_size , (batch_size,)) # should have a -1\n  x = torch.stack([data[i:i+block_size] for i in ix])\n  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n  return x, y\n```\n\n## The Bigram Model\n\nNow let's implement our first model - a simple bigram model. This model predicts the next character based only on the current character (context length of 1) and stores probabilities for the next character.\n\nWe use a log-likelihood/cross-entropy loss. Initially, we expect the loss to be around ln(1/65) ≈ 4.17, since we have 65 characters and initially each character should be equally likely.\n\n```python\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\nfrom einops import rearrange\n\nclass BigramLanguageModel(nn.Module):\n\n  def __init__(self, vocab_size):\n    super().__init__() # initialize the parent class Module\n    # each token directly reads off the logits for the next token from a lookup table\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n  def forward(self, idx, targets=None):\n    # idx and targets and both (B,T) tensors of integers\n    logits = self.token_embedding_table(idx) # (B,T,C) batch time channel\n\n    if targets is None:\n      loss = None\n    else:\n      logits = rearrange(logits,'b t c -> (b t) c')\n      targets = rearrange(targets, 'b t -> (b t)')\n      loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\n  def generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indicies in the current context\n    for _ in range(max_new_tokens):\n      # get the predictions\n      logits, loss = self(idx)\n      # focus only on last time step\n      logits = logits[:, -1 , :] # becomes B x C\n      # apply softmax to get probabilties\n      probs = F.softmax(logits, dim=1) # becomes B x C\n      # sample from distribution\n      idx_next = torch.multinomial(probs, num_samples=1) # B x 1\n      # append sampled index to running sequence\n      idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1,1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```\n\nNote that for PyTorch, when we subclass 'nn.Module', calling the model (including 'self') actually calls the 'forward' method!\n\n## Training the Bigram Model\n\nWe can train our model by choosing an **optimizer** and a **learning rate**. A good learning rate is typically 1e-3, but for smaller networks you can get away with higher rates.\n\n```python\n# training\nbatch_size = 32\n\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nfor steps in range(10000):\n  # sample a batch of data\n  xb, yb = get_batch('train')\n\n  logits, loss = m(xb, yb)\n  optimizer.zero_grad(set_to_none=True)\n  loss.backward()\n  optimizer.step()\n\nprint(loss.item())\n```\n\n## Results\n\nUnfortunately, the results using the bigram model aren't great:\n\n```\nIyoteng h hasbe pave pirance\nRie hicomyonthar's\nPlinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\nKIN d pe wither vouprrouthercc.\nhathe; d!\nMy hind tt hinig t ouchos tes; st yo\n# Not quite Shakespeare\n```\n\nThe bigram model is too simple - it can only look at one character back to predict the next. We need something more sophisticated that can consider longer context windows and learn more complex patterns.\n\n## What's Next?\n\nIn the next part of this series, we'll introduce the key mathematical trick that makes transformers possible: the attention mechanism. We'll start with a mathematical foundation for weighted aggregation and build up to single-head self-attention.\n\nThe bigram model gives us a solid foundation to build upon, and more importantly, it establishes our training pipeline and data handling. Now we're ready to dive into the real magic of transformers!","title":"Building GPT from Scratch - Part 1: Data Loading and the Bigram Model","date":"2025-08-04","excerpt":"","tags":["transformers","tutorial"]},{"slug":"05_mech_interp","content":"\nThis is a pretty long paper, and is divided into 12 sections (although the last 3 are discussion, related work and comments).\n\n## Section 1: Background & Motivation\n\nThe first section provides an introduction and summary of the main results in this paper. They begin by mentioning that it would be nice if each neuron had a one to one correspondence with a specific feature such as dog’s snout. This has been shown empirically to sometimes happen, but this is not always the case especially in LLMs. So the motivating question is when and why this happens (or doesn’t happen). By using toy models which are small ReLU networks trained to learn five features of varying importance and varying sparsity, they show that models can store more features than they have dimensions (**superposition**), and they can even perform computations in this superposed state. They hypothesize that the small networks simulate large sparse networks.\n\nThis seems like an interesting way to view superposition —- as a way of compressing information through continuous representation, but also as a computation mechanism (cue the name superposition and some possible relation to QM). When they say `no interference', this is what I think of as a circuit, with each neuron activating on, and only on, specific features.\n\nThey go on to say the main contribution is that they directly demonstrate superposition occurs relatively natural and offer a theory (phase diagram!) of when this happens. They make lots of parallels to physics phenomena, which I am excited to read given my physics background. There is an interesting note for example, that superposition organizes features into geometric structures, which immediately signals to be some kind of energy (loss function) minimizing geometries. They have not defined **sparsity** yet, although they discuss it, but I’m sure that is coming next. \n\n## Section 1b: Definitions and Motivation: Features, Directions, and Superposition\n\nThe next part of this section kicks off by discussing the Linear Representation Hypothesis ([see here](https://arxiv.org/abs/2311.03658)), which is the idea that certain directions in a representation space correspond to specific high level concepts, in other words that they carry semantic meaning. The claim that these are linear also seems important since that makes them easy to compose and decompose, and even to do vector arithmetic. This is more an empirical observation, e,g, see Mikolov et al for a famous example in word embeddings.\n\nIn their words, ‘features of the input’ are captured as ‘directions in activation space’. If we can figure out these vector representations, we can decompose activations into something we can more easily understand — as a basis of concepts each activated with a certain strength. \n\nOne natural basis in a neural network by virtue of its structure is having each neuron correspond to one feature. Naturally this would be limited to the number of neurons you have (This is a privileged basis, and there are incentives to used this basis such as activation functions, but this doesn’t mean that the basis will be used)! However, if you have more features than neutrons, a different strategy is required. This is where the idea of superposition comes in, where the features are captured by a certain direction in the high dimensional input space. I assume this is akin to creating a non-orthogonal basis. \n\nThere is some interesting discussion of what actually constitutes a feature, since `concept' is a pretty vague word. They settle on defining it as a property which will be an individual neuron in a sufficiently large network for the same problem. Does this mean that any neural network that exhibits superposition has an equivalent much larger neural network that works in the basis where each neuron is one feature? What implications does that have for discrete valued/low bit neural networks? What are the trade offs for being able to do this network compression?\n\nNext, a strong case is made for linear representations, of which I like the argument that linear representations make features linearly accessible, meaning that since neural network layers are linear functions followed by a non-linearity, a feature must be linear in order for a neuron to consistently excite or inhibit on that feature in a single step. \n\nLastly, there is more discussion on **the superposition hypothesis**, which I think is the core of this paper.  Basically, if a neural network needs to represent more features than there are neurons, features can’t align with the basis and hence polysemanticity (single neuron represents to multiple geatures) is unavoidable. While you can only have n orthogonal vectors in n dimensions, you can have an exponential exp(n) almost orthogonal vectors. So for large-n, this would be very efficient at cramming more features in than neurons! The cost is that each feature activating creates some small amount of noise by making it looks as some other features slightly activating. \n\nThe other big idea here is that sparsity is important. From compressed sensing, it is generally difficult to reconstruct a high dimensional vector projected into a low dimensional space since information is lost, but it is possible if you can assume the original vector is sparse. So sparsity here means that activations don’t happen together, and that means this `noise' is minimized. If the activations are not sparse, multiple present features will interfere.\n\nAgain the hypothesis is that small networks with polysemanticity are low dimensional projections of larger networks. The requirements for this to be effective seem pretty strict, so it would nice to be able to better quantify them and to understand how important this is in most networks.","title":"Mech Interp: Toy Models Part I","date":"2025-07-11","excerpt":"","tags":["mech interp"]},{"slug":"04_mech_interp","content":"\nWith so much to look at, it is difficult to decide where to start, but it is probably best to just start somewhere. To start off, I think I will read [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) (Elhage et al 2022). I remember really liking the findings and approach of the paper, since trying to understand simple toy models seems like a very intuitive approach to me. I have found this very useful even in my own research, where instead of directly trying to understand the results of large simulations with lots of moving pieces, we try to understand much simple toy model simulations to build a scaffold for understanding the complex behavior we observe. Since the work deals with toy models, I should also be able to play around and run some of these experiments fairly easily. This should also be a natural start to understanding sparse autoencoders (SAEs) since they seem to be the current main approach towards untangling this problem of superposition.\n\nBefore that I would like to read some resource that gives a higher level view of the history of the field and where it is going, to get a better sense of the bird’s eye view landscape. At the same time I am keeping a list of concepts I want to learn more about as I come across them, and hopefully this list does not get too long.\n\nI decided to begin by reading [this blog post](https://www.alignmentforum.org/posts/beREnXhBnzxbJtr8k/mech-interp-is-not-pre-paradigmatic) on the alignment forum by Lee Sharkey titled “Mech interp is not pre-paradigmatic”. In particular, since I am looking for some initial paradigm in which to frame mech interp research, the title of this post indicated it would present some form of such a framework. So as the title suggests, this post basically begins by arguing that the field of mech interp is **not** in what might be called a pre-paradigmatic phase, and instead is actually at the mature stage of a second mini-paradigmic wave. Pre-paradigmatic here refers to a stage in the development of a scientific field before it has established a dominant theoretical framework or \"paradigm.\" This concept comes from philosopher Thomas Kuhn's influential work \"The Structure of Scientific Revolutions”, where he outlines how a field progresses through these stages. While some argue mech interp, being a relatively new field, is in this pre-paradigmatic phase, this post pushes the idea that mech interp actually begun as an offshoot of computational neuroscience and hence inherits many of the ideas and concepts established there. In fact, the argument is made that it has been rediscovering many of the same ideas explored in neuroscience! I certainly lack sufficient knowledge in either area to judge this statement, but I don’t find this claim surprising given the parallels. Further, given the greater tractability of working with neural networks, perhaps progress might actually eventually flow in the other direction! I think the worry lies in the fact that neuroscience seems to be struggling to make recent progress and that mech interp might reach a similar wall. The article suggests the following **Three Waves of Mech Interp**:\n\n1. **First Wave** (2010s): Focused on demonstrating that interpretable structure exists in deep neural networks. 'Features are the fundamental unit of neural networks' and 'features are connected by weights, forming circuits' . This wave ended when researchers discovered polysemantic neurons (neurons that respond to multiple unrelated concepts).\n2. **Second Wave** (2022-present): Emerged after the \"Toy Models of Superposition\" paper, introducing sparse dictionary learning (SDLs) to address polysemanticity. However, this wave now faces its own anomalies.\n3. **Potential Third Wave?** The post suggests \"Parameter Decomposition\" as a promising approach that could resolve Second-Wave anomalies by decomposing neural network parameters into interpretable components representing computational mechanisms. Worth nothing that this is what the author is working on at present.\n\nAt the same time, I think the transition from first to second wave lines up with the rise of LLMs, since much early work was done with CNNs. Much larger LLMs clearly showed much richer representation and required understanding polysemanticity and superposition. As for parameter decomposition, I will have to come back to this again since I don’t understand enough to appreciate its arguments. This outline also makes me feel like the Toy Models paper is indeed the best place to start if it marks this big transition in the thinking in the field. I think it would be good for me to try and define mech interp, superposition and polysemancity, perhaps after going through the paper, and for example think about this particular quote from the post:\n\n> The idea that 'networks represent more features than they have neurons'. It is a natural corollary of the superposition hypothesis that neurons would exhibit polysemanticity, since there cannot be a one-to-one relationship between neurons and 'features'.\n\nThe article also presents a list of anomalies in this Second Wave that I would like to return to once I have built more understanding, since these should be the open problems the field is looking at now. While this post is just one perspective, I think it is a sufficiently good mental framework to start with, with lots of references and ideas to think about. Alright, onwards to Toy Models!","title":"Mech Interp: Paradigms","date":"2025-07-10","excerpt":"","tags":["mech interp"]},{"slug":"03_mech_interp","content":"\n## Starting my Mech Interp Journey\n\nChange is never easy, and stepping away from astronomy after many years feels like giving up part of my identity. I don't regret my decision to pursue graduate school, those years have been nothing but fulfilling. It's a great privilege to dedicate time and effort to a pursuit of such intellectual purity. However, at the end of graduate school, one must once again cast thoughts toward the future.\nRemaining on the academic path brings uncertainty in many areas of life and often demands personal sacrifices: constant relocation, distance from loved ones, evolving responsibilities as teaching loads and funding pressures mount. Balancing these demands with other personal priorities is far from easy, and I found myself unable to envision navigating this path happily. I've also always been acutely aware that while astronomy is fascinating, there are infinite equally interesting areas of study and work—many of more immediate relevance to humanity.\nWhile changing trajectory has been difficult, deciding what to do next has been even more challenging. One area I'm strongly interested in pursuing is mechanistic interpretability.\n\n## The Rise of Neural Networks\n\nOver the years, I've observed the explosion of machine learning, or more accurately, the rise of neural networks, deep learning, and large language models as they evolved from esoteric topics to universal adoption, taking root in all aspects of society. The social implications are tremendous, and it really does appear to be a watershed moment in how humans interact with technology.\nYet so much remains mysterious about how these networks actually work. We train these systems on vast amounts of data, but their resultant capabilities have repeatedly exceeded expectations while theoretical understanding struggles to catch up. This reminds me of complex systems that display emergent behavior even with simple, local rule-based evolution. We know the full state of a trained neural network (every weight, bias, and computation that flows through it), but its overall capabilities still baffle us.\nInformation is being processed in ways that seem opaque to us, so a field has crystallized around making sense of this opacity in ways we can understand and interpret. This is mechanistic interpretability (or \"mech interp\"), the science of understanding how machine learning networks learn to process information, much like neuroscience tries to understand how the biological brain does something similar.\nWhile young, this field is moving at lightning speed. I strongly believe that progress in mech interp is among the most important research being done today, given the reach and rate of growth of this technology. Preventing these systems from pursuing unintended goals (known as AI alignment) surely requires developing an understanding of how these networks do what they do.\n\n## A New Experience\n\nWhile I've maintained interest in machine learning and neural networks over the years, my direct experience has been limited. The applications to my research were never convincing or promising enough (a lack of interpretability makes neural networks problematic for theoretical applications).\nRecently, however, I experimented with using Neural ODEs as a natural way of extending our usual process of modeling physical systems with differential equations through deep learning, while exploring symbolic regression to improve interpretability and generalizability. I also attended NeurIPS 2024 and got a feel for what the field was excited about, including in the context of scientific applications.\nIt's been over half a year since then, and many of the big ideas such as MCP, multimodal inputs, and agentic AI have dominated advances in that time. Along the way, I've repeatedly encountered work being done on mechanistic interpretability, including papers from the Anthropic team. Beyond skimming these papers, I haven't devoted time to thinking more deeply about these ideas.\nGiven that this is a field I'm interested in pursuing, I've decided to invest time in diving deeper into the ideas and research in the literature, and exploring where I might be able to contribute.\n\n## Why This Blog\n\nStarting this research blog serves three purposes.\n1. Mainly, I hope it will document my thoughts and ideas as they change and evolve while I learn and explore this new field. As I progress, it will be useful to return to earlier thoughts.\n2. Second, I want to improve the clarity of my writing, since I often find it difficult to express thoughts without extensive refinement.\n3. Finally, I'm hoping this imposes some level of self-accountability to keep at it regularly, since this will be on the side and doesn't overlap with my current work.","title":"Mech Interp Day 0: Motivations","date":"2025-07-09","excerpt":"Starting my mech interp journey","tags":["mech interp"]},{"slug":"02_animation","content":"\nThe previous iteration of this website featured an interactive 3D pulsating sphere built using Three.js. For the sake of a cleaner feel, I decided to remove it for this iteration, but found the experience rewarding. The animation I originally had can be found at the botto of this post! The capability for 3D graphics that Three.js provide is rich in potential, especially since modern web development has evolved far beyond static pages. Today's users expect rich, interactive experiences that feel more like native applications than traditional websites.\n\n## The Technology Stack\n\nCombining several powerful technologies can create truly engaging web experiences. The combination I initially went with included:\n\n- **Three.js** for 3D graphics and WebGL rendering\n- **Framer Motion** for smooth animations and transitions\n- **React** for component-based architecture\n- **Next.js** for performance optimization\n\n## Why 3D on the Web?\n\nThree.js has revolutionized how we think about web interfaces. Here's a simple example of creating a rotating cube:\n\n```javascript\nimport * as THREE from 'three';\n\nconst scene = new THREE.Scene();\nconst camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\nconst renderer = new THREE.WebGLRenderer();\n\nconst geometry = new THREE.BoxGeometry();\nconst material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });\nconst cube = new THREE.Mesh(geometry, material);\n\nscene.add(cube);\ncamera.position.z = 5;\n\nfunction animate() {\n    requestAnimationFrame(animate);\n    cube.rotation.x += 0.01;\n    cube.rotation.y += 0.01;\n    renderer.render(scene, camera);\n}\n\nanimate();\n```\n\n## Animation with Framer Motion\n\nFramer Motion makes it incredibly easy to add smooth animations to React components:\n\n```jsx\nimport { motion } from 'framer-motion';\n\nconst AnimatedCard = () => {\n  return (\n    \n      Interactive Card\n      This card animates on load and hover!\n    \n  );\n};\n```\n\n## My Experience\n\nThe combination of Three.js and Framer Motion in a React/Next.js environment provides an incredibly powerful toolkit for creating memorable web experiences. The key is finding the right balance between visual impact and performance. The rest of this post will go through how I set up my initial animation, with the final product at the end!\n\n## Uniform Sampling of Points on A Sphere\n\nThe key to creating a convincing sphere lies in proper point distribution. Unlike naive approaches that create clustering at poles, we use **spherical coordinates** with uniform random sampling:\n\n```javascript\nconst points = [];\nfor (var i = 0; i < 1500; i++) {\n  var vertex = new THREE.Vector3();\n\n  // Uniform random sampling on sphere surface\n  var u = THREE.MathUtils.randFloat(0, 1);\n  var v = THREE.MathUtils.randFloat(0, 1);\n  var theta = 2 * Math.PI * u;           // Azimuthal angle\n  var phi = Math.acos(2 * v - 1);        // Polar angle\n\n  // Convert to Cartesian coordinates\n  vertex.x = 3.5 * Math.sin(phi) * Math.cos(theta);\n  vertex.y = 3.5 * Math.sin(phi) * Math.sin(theta);\n  vertex.z = 3.5 * Math.cos(phi);\n\n  points.push(vertex);\n}\n```\n\nThis approach ensures **uniform distribution** across the sphere surface, avoiding the visual artifacts of simpler random placement methods.\n\n## GLSL Shader\n\nThe pulsating light effect is achieved through custom **vertex and fragment shaders** that run directly on the GPU:\n\n### Shader Uniforms Setup\n\n```javascript\nconst shaderPoint = THREE.ShaderLib.points;\nconst uniforms = THREE.UniformsUtils.clone(shaderPoint.uniforms);\nuniforms.time = { value: 0 };\nuniforms.color = { type: \"v3\", value: colorValue };\n\nconst pMaterial = new THREE.ShaderMaterial({\n  uniforms,\n  transparent: true,\n  depthWrite: false,\n  blending: THREE.AdditiveBlending,  // Creates light emission effect\n  vertexShader,\n  fragmentShader,\n});\n```\n\n### The Shader Pipeline\n\nThis particle system consists of two interconnected shaders:\n\n- **Vertex Shader** (\"vert.glsl\") - Handles particle positioning, sizing, and movement\n- **Fragment Shader** (\"frag.glsl\") - Controls particle appearance, color, and glow effects\n\n### 3D Simplex Noise Implementation\n\nThe vertex shader begins with a complete **Simplex noise** implementation - a sophisticated algorithm for generating natural-looking randomness:\n\n```glsl\nfloat snoise(vec3 v) {\n    // 85 lines of math\n    // Creates smooth, continuous 3D noise\n    return 42.0 * dot(m*m, vec4(dot(p0,x0), dot(p1,x1), dot(p2,x2), dot(p3,x3)));\n}\n```\n\n**Why Simplex Noise?** Unlike basic random functions, Simplex noise provides:\n- **Continuous gradients** - no sudden jumps or artifacts\n- **3D coherence** - neighboring points have similar values\n- **Performance optimization** - faster than Perlin noise\n- **Natural patterns** - mimics organic movement and growth\n\n### Organic Particle Movement\n\nThe main vertex shader transforms each particle's position using time-based trigonometric functions:\n\n```glsl\nvec3 newPos = position;\n\nnewPos.x += sin(time + position.x * position.y) * 0.08;\nnewPos.y += cos(time + position.x * position.y * 1.1) * 0.08;\nnewPos.z += cos(time + position.x * position.y * 1.3) * 0.08;\n```\n\n**Mathematical Breakdown:**\n- **Base oscillation**: \"sin(time)\" and \"cos(time)\" create rhythmic movement\n- **Position coupling**: \"position.x * position.y\" makes each particle's movement unique\n- **Frequency variation**: Multipliers prevent synchronized motion\n- **Amplitude control**: \"0.08\" keeps movement subtle and elegant\n\nThis creates a **Lissajous-like pattern** where each particle follows its own complex orbital path, determined by its starting position.\n\n### Dynamic Particle Sizing\n\nThe most sophisticated aspect is the noise-driven size variation:\n\n```glsl\ngl_PointSize = 50. + snoise(position * 0.05 + vec3(0, 0, vtime * 0.1)) * 50.;\ngl_PointSize *= 0.5;\n```\n\n**Size Calculation Explained:**\n- **Base size**: \"50.\" pixels provides consistent minimum visibility\n- **Noise sampling**: \"snoise(position * 0.05 + ...)\" creates spatial variation\n- **Temporal drift**: \"vec3(0, 0, vtime * 0.1)\" makes noise evolve over time\n- **Amplitude**: \"* 50.\" allows sizes to range from 0 to 100 pixels\n- **Final scaling**: \"* 0.5\" reduces overall scale to 0-50 pixels\n\nThe result is particles that **breathe** - growing and shrinking organically as the noise field evolves through time.\n\n### Time-Based Animation\n\nThe pulsating effect is driven by a time uniform that updates every frame:\n\n```javascript\nconst animate = (time) => {\n  // Update shader time for pulsating effect\n  pMaterial.uniforms.time.value = time * 0.004;\n  \n  // Continue animation loop\n  requestAnimationFrame(animate);\n  renderer.render(scene, camera);\n}\n```\n\nThe **AdditiveBlending** mode creates the characteristic light emission, making particles appear to glow and blend naturally when they overlap.\n\n## Responsive Color System\n\nThe particle system adapts to the user's color mode preference through dynamic uniform updates:\n\n```javascript\n// Real-time color mode detection and adaptation\nif (localStorage.getItem(\"chakra-ui-color-mode\") === \"dark\") {\n  pMaterial.uniforms.color.value = new THREE.Color(0xffffff);  // White particles\n} else {\n  pMaterial.uniforms.color.value = new THREE.Color(0x000000);  // Black particles\n}\n```\n\nThis creates a seamless experience where the 3D scene automatically adapts to the user's interface preferences without requiring page refreshes.\n\n## Smooth Camera Animations\n\nThe initial camera movement uses an **easing function** to create natural motion:\n\n```javascript\nfunction easeOutCirc(x) {\n  return Math.sqrt(1 - Math.pow(x - 1, 4));\n}\n\n// Camera animation during first 100 frames\nif (frame <= 100) {\n  const rotSpeed = -easeOutCirc(frame / 120) * Math.PI * 20;\n  \n  camera.position.x = p.x * Math.cos(rotSpeed) + p.z * Math.sin(rotSpeed);\n  camera.position.z = p.z * Math.cos(rotSpeed) - p.x * Math.sin(rotSpeed);\n  camera.lookAt(target);\n} else {\n  // Switch to user-controlled orbit after animation\n  controls.update();\n}\n```\n\nAfter the initial animation completes, control transitions to **OrbitControls** for user interaction, with automatic rotation enabled.\n\n## Performance Optimizations\n\n### GPU-Accelerated Rendering\n- **ShaderMaterial** for GPU-based calculations\n\n### Efficient Animation Loop\n```javascript\nlet req = null;\nconst animate = (time) => {\n  req = requestAnimationFrame(animate);\n  \n  // Minimal CPU calculations\n  // GPU handles particle transformations\n  \n  renderer.render(scene, camera);\n}\n\n// Proper cleanup\nreturn () => {\n  cancelAnimationFrame(req);\n  renderer.domElement.remove();\n  renderer.dispose();\n}\n```\n\n## Final Product\nHere is the final animation, best viewed in dark mode!\n<hr />\n<voxel-art />","title":"Building Interactive Web Experiences","date":"2025-07-08","excerpt":"Exploring the combination of Three.js, Framer Motion, and React for creating engaging user interfaces","tags":["threejs","framer-motion","webgl","animation"]},{"slug":"01_implementation","content":"\nThis document summarizes the complete implementation of a markdown-based blog system using Next.js, Chakra UI, react-markdown, and KaTeX for mathematical equations.\n\n## Tech Stack\n\n- **Next.js** - React framework with static site generation\n- **Chakra UI** - Component library for styling\n- **react-markdown** - Markdown to React component converter\n- **KaTeX** - LaTeX equation rendering\n- **gray-matter** - Frontmatter parsing\n- **Framer Motion** - Animations\n\n## Required Dependencies\n\n```bash\nnpm install react-markdown remark-gfm rehype-highlight rehype-raw gray-matter katex rehype-katex remark-math\n```\n\n## File Structure\n\n```\nproject/\n├── components/\n│   └── BlogPost.jsx\n├── lib/\n│   └── posts.js\n├── pages/\n│   ├── writing.js (blog index)\n│   └── blog/\n│       └── [slug].js (individual posts)\n├── posts/\n│   ├── my-first-post.md\n│   └── second-blog-post.md\n└── pages/_app.js\n```\n\n## Core Components\n\n### lib/posts.js - File System Functions\n```javascript\nimport fs from 'fs';\nimport path from 'path';\nimport matter from 'gray-matter';\n\nconst postsDirectory = path.join(process.cwd(), 'posts');\n\nexport function getAllPosts() {\n  const fileNames = fs.readdirSync(postsDirectory);\n  const allPostsData = fileNames.map((fileName) => {\n    const slug = fileName.replace(/\\.md$/, '');\n    const fullPath = path.join(postsDirectory, fileName);\n    const fileContents = fs.readFileSync(fullPath, 'utf8');\n    const { data, content } = matter(fileContents);\n\n    return {\n      slug,\n      content,\n      ...data,\n    };\n  });\n\n  return allPostsData.sort((a, b) => (a.date < b.date ? 1 : -1));\n}\n\nexport function getPostBySlug(slug) {\n  const fullPath = path.join(postsDirectory, `${slug}.md`);\n  const fileContents = fs.readFileSync(fullPath, 'utf8');\n  const { data, content } = matter(fileContents);\n\n  return {\n    slug,\n    content,\n    ...data,\n  };\n}\n```\n\n### components/BlogPost.jsx - Markdown Renderer\n```javascript\nimport ReactMarkdown from 'react-markdown';\nimport remarkGfm from 'remark-gfm';\nimport remarkMath from 'remark-math';\nimport rehypeHighlight from 'rehype-highlight';\nimport rehypeKatex from 'rehype-katex';\nimport { \n  Box, \n  Heading, \n  Text, \n  Code, \n  Divider, \n  Link,\n  UnorderedList,\n  OrderedList,\n  ListItem,\n  useColorModeValue\n} from '@chakra-ui/react';\n\nconst BlogPost = ({ content }) => {\n  const blockquoteBg = useColorModeValue('gray.50', 'gray.700');\n  const inlineCodeBg = useColorModeValue('gray.100', 'gray.600');\n  const inlineCodeColor = useColorModeValue('gray.800', 'gray.100');\n\n  return (\n    <Box maxW=\"800px\" mx=\"auto\" p={6}>\n      <ReactMarkdown\n        remarkPlugins={[remarkGfm, remarkMath]}\n        rehypePlugins={[rehypeHighlight, rehypeKatex]}\n        components={{\n          h1: ({ children }) => (\n            <Heading as=\"h1\" size=\"2xl\" mb={6} mt={8}>\n              {children}\n            </Heading>\n          ),\n          h2: ({ children }) => (\n            <Heading as=\"h2\" size=\"xl\" mb={4} mt={6}>\n              {children}\n            </Heading>\n          ),\n          h3: ({ children }) => (\n            <Heading as=\"h3\" size=\"lg\" mb={3} mt={5}>\n              {children}\n            </Heading>\n          ),\n          p: ({ children }) => (\n            <Text mb={4} lineHeight=\"1.7\">\n              {children}\n            </Text>\n          ),\n          ul: ({ children }) => (\n            <UnorderedList mb={4} spacing={2}>\n              {children}\n            </UnorderedList>\n          ),\n          ol: ({ children }) => (\n            <OrderedList mb={4} spacing={2}>\n              {children}\n            </OrderedList>\n          ),\n          li: ({ children }) => (\n            <ListItem>{children}</ListItem>\n          ),\n          code: ({ inline, children, className }) => {\n            return inline ? (\n              <Code \n                px={2} \n                py={1} \n                bg={inlineCodeBg}\n                color={inlineCodeColor}\n                borderRadius=\"md\"\n                suppressHydrationWarning={true}\n              >\n                {children}\n              </Code>\n            ) : (\n              <Box mb={4}>\n                <Code\n                  as=\"pre\"\n                  display=\"block\"\n                  p={4}\n                  bg=\"gray.900\"\n                  color=\"white\"\n                  _dark={{ bg: \"gray.800\" }}\n                  borderRadius=\"md\"\n                  overflow=\"auto\"\n                  suppressHydrationWarning={true}\n                >\n                  {children}\n                </Code>\n              </Box>\n            );\n          },\n          a: ({ href, children }) => (\n            <Link href={href} color=\"blue.500\" isExternal>\n              {children}\n            </Link>\n          ),\n          blockquote: ({ children }) => (\n            <Box\n              as=\"blockquote\"\n              borderLeft=\"4px solid\"\n              borderColor=\"blue.500\"\n              pl={4}\n              py={2}\n              mb={4}\n              fontStyle=\"italic\"\n              bg={blockquoteBg}\n              _dark={{ bg: \"gray.700\", borderColor: \"blue.300\" }}\n              borderRadius=\"md\"\n              suppressHydrationWarning={true}\n            >\n              {children}\n            </Box>\n          ),\n          hr: () => <Divider my={6} />,\n        }}\n      >\n        {content}\n      </ReactMarkdown>\n    </Box>\n  );\n};\n\nexport default BlogPost;\n```\n\n### pages/writing.js - Blog Index\n```javascript\nimport NextLink from 'next/link'\nimport { Box, Container, Heading, Link, Text } from '@chakra-ui/react'\nimport Layout from '../components/layouts/article'\nimport Section from '../components/section'\nimport { getAllPosts } from '../lib/posts'\n\nconst Writing = ({ posts = [] }) => {\n  return (\n    <Layout title=\"Posts\">\n      <Container>\n        <Heading as=\"h3\" fontSize={20} mb={4} mt={5}>\n          Posts\n        </Heading>\n\n        {posts && posts.map((post, index) => (\n          <Section key={post.slug} delay={0.1 + (index + 1) * 0.1}>\n            <Box my={4}>\n              <Text fontSize=\"sm\" color=\"gray.500\" display=\"inline\" mr={3}>\n                {new Date(post.date + 'T00:00:00').toLocaleDateString('en-US', {\n                  year: 'numeric',\n                  month: 'short',\n                  day: 'numeric',\n                  timeZone: 'UTC'\n                })}\n              </Text>\n              <Link as={NextLink} href={`/blog/${post.slug}`}>\n                {post.title}\n              </Link>\n            </Box>\n          </Section>\n        ))}\n\n      </Container>\n    </Layout>\n  )\n}\n\nexport async function getStaticProps() {\n  try {\n    const posts = getAllPosts();\n    return {\n      props: {\n        posts,\n      },\n    };\n  } catch (error) {\n    console.error('Error in getStaticProps:', error);\n    return {\n      props: {\n        posts: [],\n      },\n    };\n  }\n}\n\nexport default Writing\n```\n\n### pages/blog/[slug].js - Individual Post Pages\n```javascript\nimport { getAllPosts, getPostBySlug } from '../../lib/posts';\nimport BlogPost from '../../components/BlogPost';\nimport { Box, Heading, Text } from '@chakra-ui/react';\n\nexport default function Post({ post }) {\n  return (\n    <Box>\n      <Box textAlign=\"center\" mb={8}>\n        <Heading as=\"h1\" size=\"3xl\" mb={4}>\n          {post.title}\n        </Heading>\n        <Text color=\"gray.600\">{post.date}</Text>\n      </Box>\n      <BlogPost content={post.content} />\n    </Box>\n  );\n}\n\nexport async function getStaticProps({ params }) {\n  const post = getPostBySlug(params.slug);\n  return {\n    props: {\n      post,\n    },\n  };\n}\n\nexport async function getStaticPaths() {\n  const posts = getAllPosts();\n  const paths = posts.map((post) => ({\n    params: { slug: post.slug },\n  }));\n\n  return {\n    paths,\n    fallback: false,\n  };\n}\n```\n\n## Markdown Features Supported\n\n### Basic Formatting\n- **Bold text** and *italic text*\n- Headers (H1, H2, H3)\n- Paragraphs with proper spacing\n- Horizontal rules\n- Links (internal and external)\n\n### Lists\n- Unordered lists with bullet points\n- Ordered lists with numbers\n- Proper spacing between items\n\n### Code\n- Multi-line code blocks with language-specific highlighting\n- Dark/light mode responsive styling\n\n### Blockquotes\n> Styled blockquotes with left border. Dark/light mode responsive\n\n### Mathematical Equations\n- Inline math: $E = mc^2$\n- Block equations: $$\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$$\n- Complex expressions with proper LaTeX rendering\n\n## Frontmatter Structure\n```yaml\n---\ntitle: \"Post Title\"\ndate: \"2025-01-15\"\nexcerpt: \"Brief description of the post\"\ntags: [\"tag1\", \"tag2\", \"tag3\"]\n---\n```\n\n## Development Workflow\n\n1. Create markdown files in posts/ directory\n2. Posts automatically appear on /writing page\n\n## Troubleshooting Notes\n\n- **Date Issues**: Use `new Date(post.date + 'T00:00:00')` to prevent timezone shifts\n- Inline equations not working","title":"Blog Implementation Summary","date":"2025-07-07","excerpt":"Complete guide to implementing a markdown blog with Next.js, Chakra UI, and mathematical equations","tags":["nextjs","react-markdown","chakra-ui","katex","blog"]},{"slug":"00_hello_world","content":"\n## Welcome to My Blog\n\nThis is my first blog post! I will just test functionality in this post, and write up how I implemented post functionality in a future post.\n\n## Code Example\n\n```python\ndef hello_world():\n    print(\"Hello, World!\")\n    return True\n``` \n\n## More Examples\n\n> This is a blockquote with some important information.\n\nCheck out [this link](https://example.com) for more info.\n\nHere's some **bold text** and *italic text*.\n\n### Lists\n\nUnordered list:\n- Item 1\n- Item 2\n- Item 3\n\nOrdered list:\n1. First item\n2. Second item\n3. Third item \n\n### Equations\nThis is an inline equation: $E = mc^2$.\n\nIn web animations, we often use trigonometric functions. The general form of a sine wave is:\n\n$$\ny(x,t) = A \\sin(kx - \\omega t + \\phi)\n$$\n\nWhere:\n- $A$ is the amplitude\n- $k$ is the wave number  \n- $\\omega$ is the angular frequency\n- $\\phi$ is the phase shift\n\nThis equation is perfect for creating smooth, natural animations in Three.js!\n\nThat's all for now!\n","title":"Hello World","date":"2025-07-06","excerpt":"This is my first blog post using react-markdown","tags":["react","nextjs","markdown"]}]},"__N_SSG":true}