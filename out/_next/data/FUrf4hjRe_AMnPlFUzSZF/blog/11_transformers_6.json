{"pageProps":{"post":{"slug":"11_transformers_6","content":"*This is Part 6 of our GPT from scratch series. Having upgraded to modern tokenization in Part 5, we now tackle the data limitation by training on a much larger corpus and exploring pretraining + finetuning.*\n\n## Training on a Larger Corpus\n\nSince we have a larger model, we now want to train on a larger dataset. I'm still interested in generating poems, so let's use this [poetry dataset from Kaggle](https://www.kaggle.com/datasets/tgdivy/poetry-foundation-poems) which gives us a CSV file containing a list of poems from [Poetry Foundation](https://www.poetryfoundation.org/). \n\nWe first have to do some data cleaning with pandas to get the newline Unicode characters consistent, and concatenate all the poems into a single text file.\n\n```\nLine terminators found in the poems:\n----------------------------------------\nMac Classic (\\r only): 417494 occurrences\nDouble CR + LF (\\r\\r\\n): 379146 occurrences\nWindows (\\r\\n): 379146 occurrences\nReversed (\\n\\r): 19556 occurrences\nDouble CR (\\r\\r): 10527 occurrences\nLine Separator (U+2028): 419 occurrences\nParagraph Separator (U+2029): 1 occurrences\n----------------------------------------\n\nSuccessfully concatenated 13854 poems into 'all_poems_cleaned.txt'\n\nSample cleaning (poem at index 101):\nOriginal: '\\r\\r\\nThe sound of a guitar drifts through the air.\\r\\r\\nCupped in my hand, a snowflake quivers lightly.\\r\\r'...\nCleaned: 'The sound of a guitar drifts through the air.\\nCupped in my hand, a snowflake quivers lightly.\\nThick '...\n```\n\nThis file is around 20MB and 400,000 lines long, which is 20 times the size and 10 times the number of lines of the Shakespeare dataset.\n\n## Training Results on Poetry Dataset\n\n```\nstep 0: train loss 10.8247, val loss 10.8255\nstep 100: train loss 6.2405, val loss 6.5146\nstep 200: train loss 5.7141, val loss 6.2657\nstep 300: train loss 5.5298, val loss 6.1561\nstep 400: train loss 5.4678, val loss 5.8279\nstep 500: train loss 5.5526, val loss 5.7174\nstep 600: train loss 5.2497, val loss 5.6578\nstep 700: train loss 5.1549, val loss 5.6028\nstep 800: train loss 5.0009, val loss 5.7194\nstep 900: train loss 5.2021, val loss 5.5778\nstep 1000: train loss 4.8988, val loss 5.5454\nstep 1100: train loss 4.9935, val loss 5.4814\nstep 1200: train loss 4.9520, val loss 5.3551\nstep 1300: train loss 5.0129, val loss 5.3685\nstep 1400: train loss 4.9534, val loss 5.3807\nstep 1500: train loss 4.7947, val loss 5.3912\nstep 1600: train loss 5.0557, val loss 5.2993\nstep 1700: train loss 4.8419, val loss 5.2611\nstep 1800: train loss 4.7657, val loss 5.0324\nstep 1900: train loss 4.4551, val loss 5.0815\nstep 2000: train loss 4.6674, val loss 5.1306\nstep 2100: train loss 4.6318, val loss 5.0484\nstep 2200: train loss 4.6357, val loss 5.2188\nstep 2300: train loss 4.5006, val loss 4.9910\nstep 2400: train loss 4.3960, val loss 5.0353\nstep 2500: train loss 4.4227, val loss 5.0051\nstep 2600: train loss 4.4193, val loss 4.9974\nstep 2700: train loss 4.4097, val loss 4.9683\nstep 2800: train loss 4.2323, val loss 4.9965\nstep 2900: train loss 4.3671, val loss 4.8900\nstep 3000: train loss 4.3548, val loss 4.8972\nstep 3100: train loss 4.3923, val loss 4.7957\nstep 3200: train loss 4.4397, val loss 4.7694\nstep 3300: train loss 4.3573, val loss 4.9981\nstep 3400: train loss 4.3772, val loss 4.8391\nstep 3500: train loss 4.3908, val loss 4.8069\nstep 3600: train loss 4.2172, val loss 4.7400\nstep 3700: train loss 4.1613, val loss 4.8643\nstep 3800: train loss 4.2559, val loss 4.9014\nstep 3900: train loss 4.0760, val loss 4.7784\nstep 4000: train loss 3.9681, val loss 4.7814\nstep 4100: train loss 4.0662, val loss 4.8351\nstep 4200: train loss 4.1156, val loss 4.9005\nstep 4300: train loss 4.1036, val loss 4.7409\nstep 4400: train loss 4.0790, val loss 4.6941\nstep 4500: train loss 4.1168, val loss 4.8207\nstep 4600: train loss 3.8932, val loss 4.7466\nstep 4700: train loss 4.0971, val loss 4.9108\nstep 4800: train loss 4.0825, val loss 4.8164\nstep 4900: train loss 4.0249, val loss 4.7728\nstep 4999: train loss 4.0157, val loss 4.7919\n```\n\nThe model generates much more diverse and poetic text:\n\n```\n! but ...  \nbeautise on a head, everything's\nserved, seemed—\nLewash on the back of the garbled pllesh,\nare not to be vigilant\nby food, tears, eyes burnt,\ndust and screaming. Just say —I've brought\nGrandter aesthetics—\nmaybe God like John Fanny or that.7.\nEverything's that alonghed in.\nButoried to me\nwhen he yelled.\n\nJust urboilherer appeared. Then he were falling, sits crying and prayed to his brother, being missing. They started on time they went up, though\nGod said his father had to stop a good reporters\nstand, locked, hooves, feet up tighter, \"Is smooth?\"\nThe Morning\nSeason explores the world of the walls\nGazels through the light. Later each day,\nA reasonably thing is beauties\nAt evening a while Frains unself from a walk.Zusted in light old\nThe way used to the land. On the snow?\nThere said, \"Not offensive.\" \"but that's the trietry, \"Nor does\nsay, but it\" with footage,\n Maoaching the crowd.\"You like the power actual\nor on in their apartmentrelices, and the vinyl themselves lie together by that the Jewish sound.\" So, in one hand,\nyou could folding it backdroaches and wall you made\nhad keep what'﻿s done.\" and I stared, picking a grump offpour, and stared on the smoke, tapped to a wheel and the twos winkboat.\nThe house was a cigar on my stopes of dilelict and\nthose threatened hunger from how to be at moving steadily, could never believe\nas white but a man with pens would not take  to. I could remember that night.\nThe moon was thick, the wind rivening the waves of the sea, the swift dark whiteerly recedes\nfrom are it and its silkency of    paths comprising sleep, a rail of o'clock. Outspread it somewhere,\nand if it ever repeated; soon, just angularling forward, the magager and schimology.The rifle wasn't wobbling paths and blinked,\n```\n\n## Further Training Attempts\n\nTo try and further improve, we try training longer and with a learning rate that is ten times smaller. However, both training and validation seem to have saturated. We also try a cosine learning rate with multiple restarts, but the results are not very good:\n\n```python\nstep_lr = config.learning_rate*np.cos(0.5*np.pi*iter/(config.max_iters-1))\nfor group in optimizer.param_groups:\n  group['lr'] = step_lr\n```\n\n```\nstep 0: train loss 10.8202, val loss 10.8287\n...\nstep 3000: train loss 4.5538, val loss 5.0448\n...\nstep 1000: train loss 4.5272, val loss 4.9344\n...\nstep 999: train loss 4.2557, val loss 4.8072\n...\nstep 999: train loss 4.0713, val loss 4.7289\n...\nstep 999: train loss 4.1581, val loss 4.8483\n```\n\nNext, we try reshuffling the training and validation split of the input data by using the first 10% for validation instead. We find that training loss is now higher, another sign we were overfitting. After further training, we end up with training loss and validation loss being about 4. The final result is decent:\n\n```\nSummer dazed\nwith its theatrical designs, tubes, guts.\nI clump the bripscrewed black.\nI share the precise whatever\nnot actually needed.\nI say when it grazed the time,\nthings myself between them\nand there's little wings forth\nEnglish stains, the slick cold force.\nRunning the basement that moved\nnever. I wonder whether it meant\nthat this sad adventure within and\nunlemishes the dead.\nI was writing that Old Friend was\nalways working on I have\nseen many plans seeing unsatisfied,\nshe was always ashamed\nof this region.\nI think of the Second Coming\nasleep clear as I thought\nwaswitch recognized yet because of himself\nwere not afraid of me\nbut this sequence after the war\nthat had been doing to me\ndon't hurt me I meant so many years ago.\nI wasn't my brother.\n```\n\n## Finetuning on Shakespeare\n\nLastly, we try finetuning on the tiny Shakespeare dataset we started with. We have essentially pretrained our larger model on the larger poetry dataset and now use a much lower learning rate (3e-6) and 10,000 training iterations to refine on the smaller tiny Shakespeare dataset.\n\n```\nstep 10000: train loss 3.1057, val loss 4.1152\n```\n\nThis is much better than when we just tried to train our larger model with the GPT-2 tokenizer on the Shakespeare data alone. This is the advantage of pretraining! Here are the results:\n\n```\nIs he contented himself with his exercise,\nEven he that fall'd upon his tent with him\nThe ground o'er his bed, his dole from the north,\nAnd cries 'He well.'\nAnd Tranio, which consummate honour, in turn\nProclaims him on his way; and he cried,\nLay him that comments mortals play out.\n\nLADY CAPULET:\nMy lords, I will not hear my traitor.\n\nDUKE VINCENTIO:\nWrut, away! thou shalt never weep;\nAnd yet I have received thy royal presence,\nHaving done the corse. But thou'll hear be\nMy father's son, blessing in the cruel gown,\nthough mine own buried heir\nOur father's poor son, my poor daughter, my child,\nWhere I am resign'd me; and I will be king,\nAnd pain'd the house of this churchyard call.\nThy father, and I seek thee here;\nSo, newly wench'd with that great aspect,\nAnd thy servile' mortal presence made me,\nAnd thou wouldst board thy mortal bones\nIn readiness, he may hear it bear.\nAh, art thou law to be made my nurse.\n\nLADY CAPULET:\nAy, I'll win thee would not woe awhile.\n\nDUKE OF YORK:\nO gentle Plantagenet queen,\n\nISABELLA:\nAh, my lord, thou art safe and younger!\n\nDUKE OF YORK:\nFarewell with us, bleated those that have said,\nThat we should plague thee for many days.\n\nKING RICHARD II:\nGive me my father to your brother's house,\nOr else send him by your fortunes to the street.\nWhat art thou slain?\n\nGLOUCESTER:\nHarry of York, I tell thee that kill thy life:\nAnd thou, contracted hither to my farm,\nThou art so happy to be repelling'd at thy mind!\nAs on a day that pageant thou sufficiency,\nNow let thy father speak before thy function.\n\nKING RICHARD II:\nO coward to this day; there comes Hereford with thee!\n```\n\nWow! While it's still not perfectly coherent, it seems much better than our output from the character-level tokenizer. Some characters even have multiple lines!\n\n## Key Insights\n\nThis part demonstrates several important concepts in modern language model training:\n\n1. **Dataset Scale Matters**: The larger poetry dataset (20x bigger) significantly improved text quality and diversity.\n\n2. **Data Cleaning is Critical**: Proper handling of line terminators and text formatting is essential for good training data.\n\n3. **Pretraining + Finetuning Works**: The two-stage approach of pretraining on a large diverse corpus, then finetuning on a specific domain, produces better results than training on the target domain alone.\n\n4. **Overfitting Remains a Challenge**: Even with more data, the model still shows signs of overfitting, indicating we need even more data or better regularization.\n\n5. **Modern Tokenization Benefits**: Subword tokens produce more natural-looking text even when the model isn't perfectly trained.\n\nThe progression from character-level Shakespeare generation to subword-level poetry and then Shakespeare finetuning shows the power of scaling both model size and training data in the right sequence.","title":"Building GPT from Scratch - Part 6: Pretraining and Finetuning","date":"2025-08-11","excerpt":"","tags":["transformers","tutorial"]}},"__N_SSG":true}