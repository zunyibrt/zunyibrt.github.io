{"pageProps":{"post":{"slug":"09_transformers_4","content":"*This is Part 4 of our GPT from scratch series. Having built a complete transformer in Parts 1-3, we now look at further optimizations for more efficient training.*\n\n## Restructuring and Configuration\n\nNow that we have successfully implemented a transformer network, we can make a number of adjustments to optimize for more efficient training.\n\nFirst, we restructure our script and put all hyperparameters into a config class:\n\n```python\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'using device: {DEVICE}')\n\nconfig = Config(\n  batch_size = 64,\n  block_size = 64,\n  max_iters = 5000,\n  eval_interval = 500,\n  learning_rate = 3e-4,\n  eval_iters = 10,\n  n_embd = 128,\n  n_head = 4,\n  n_layer = 6,\n  dropout = 0.2,\n  vocab_size = vocab_size,\n)\n\nmodel = GPTLanguageModel(config, device=DEVICE)\nmodel.to(DEVICE)\nmodel = torch.compile(model) # requires PyTorch 2.0\n```\n\nAs you can see, we also compile the model before training to improve training speed.\n\n## Efficient Multi-Head Attention\n\nThe next big change we make is to compute the keys, queries, and values for the multiple heads all in a single matrix multiplication. This means we combine the 'Head' and 'Multi-Head' classes into one single class, which processes the heads as a new batch dimension.\n\n```python\nclass MultiHeadAttention(nn.Module):\n  \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n  def __init__(self, config):\n    super().__init__()\n    self.n_head = config.n_head\n    self.n_embd = config.n_embd\n    self.dropout = config.dropout\n\n    # key, query, value projections for all heads, but in a batch\n    self.attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n    self.proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n    self.attn_dropout = nn.Dropout(config.dropout)\n    self.resid_dropout = nn.Dropout(config.dropout)\n    self.register_buffer(\"bias\",\n      torch.tril(torch.ones(1, 1, config.block_size, config.block_size)))\n\n  def forward(self, x):\n    B, T, C = x.size()\n    q, k, v  = self.attn(x).split(self.n_embd, dim=2)\n    # reshape and move head dimension forward using einops\n    k = rearrange(k, 'b t (nh hs) -> b nh t hs', nh=self.n_head)\n    q = rearrange(q, 'b t (nh hs) -> b nh t hs', nh=self.n_head)\n    v = rearrange(v, 'b t (nh hs) -> b nh t hs', nh=self.n_head)\n\n    att = (q @ k.transpose(-2, -1)) * k.size(-1)**-0.5\n    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n    att = F.softmax(att, dim=-1)\n    att = self.attn_dropout(att)\n    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n\n    # re-assemble all head outputs side by side\n    y = rearrange(y, 'b nh t hs -> b t (nh hs)')\n    y = self.resid_dropout(self.proj(y))\n    return y\n```\n\n## Flash Attention\n\nFinally, we implement flash attention, which reorders the attention computation such that computations can be tiled which greatly speeds up the attention step.\n\n```python\n# In MultiHeadAttention class\nself.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n# ... in forward pass ...\ny = torch.nn.functional.scaled_dot_product_attention(q, k, v, \n                                                     attn_mask=None, \n                                                     dropout_p=dropout_p,\n                                                     is_causal=True)\n```\n\n## Training Performance and Scaling\n\nTraining on a T4 GPU, a 1.2M parameters model now trains in 2 minutes. We are ready to scale up to the size shown in the tutorial video:\n\n```python\nconfig = Config(\n  batch_size = 64,\n  block_size = 256,\n  max_iters = 5000,\n  eval_interval = 500,\n  learning_rate = 3e-4,\n  eval_iters = 10,\n  n_embd = 384,\n  n_head = 6,\n  n_layer = 6,\n  dropout = 0.2,\n  vocab_size = vocab_size,\n)\n```\n\nThis has 10 million parameters and trains in 30 minutes! Let's take a look at the output:\n\n```\nBRAKENBURY:\nDespite of this seven sit in the noble\nOf Lord Hastings, and my grave is my charged mine;\nFor George shall not speak not pass'd it:\nThe valour upon it. Is deliver'd it with me?\n\nBRUTUS:\nYea, beggar, by your voices and hearts,\nBut since she changed in your packs and bloody,\nYour joy your might in him writ\nBe punk'd between pains. I am struckcomment\nTo son I writ you that yet you did love:\nIf it were example to your knees to express\nBy your fleships of state? Exeter, me\nAnd wring, my son, come on, my sorrow ladys,\nWhose profession joy wings and me down,\n```\n\nPretty good! Most words are recognizable and the sentences have more structure. The overall form is also consistent with the training data with readable names (Brutus is a familiar one!)\n\n## Bias-Variance Trade-off\nIf we plot the losses against training steps for our small 0.3M model and our final 10M model:\n![Validation loss comparison](/images/training_validation_comparison.png)\nThe larger model generalizes better (has a lower validation loss) but overfits more (training loss is much lower than validation loss). This represents a classic **bias-variance trade-off** in neural scaling. The larger model's increased parameter capacity enables better feature representation learning and pattern recognition, resulting in lower validation error. However, this same capacity creates **memorization potential** for training data.\n\n## Conclusion\n\nThese optimizations demonstrate how production transformer implementations differ from educational versions. The key improvements - configuration management, batched attention computation, model compilation, and flash attention - provide significant training speedups while maintaining the same underlying architecture.\n\nThe scaling results show that our simple transformer can achieve reasonable quality when given sufficient parameters and training time, following the same principles that power modern large language models.","title":"Building GPT from Scratch - Part 4: Further Optimizations","date":"2025-08-07","excerpt":"","tags":["transformers","tutorial"]}},"__N_SSG":true}