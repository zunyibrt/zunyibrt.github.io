{"pageProps":{"post":{"slug":"06_transformers_1","content":"\n*This is Part 1 of a 3-part series on building a Generative Pretrained Transformer from scratch, based on Andrej Karpathy's excellent tutorial.*\n\n## Introduction\n\nThe goal of this series is to build a 'Generative Pretrained Transformer' from scratch, following the attention architecture from the original 2017 [**Attention is all you need**](https://arxiv.org/abs/1706.03762) paper and OpenAI's GPT-2. We'll be building a character-level generative model - essentially a next character predictor that can generate Shakespeare-like text.\n\nThis tutorial is based on Andrej Karpathy's video [**Let's build GPT: from scratch, in code, spelled out**](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=10s), which has gained 6 million views in 2 years! Transformers get their name from their original use case of machine translation.\n\n## Data Loading\n\nThe first step in building any language model is preparing our data. We'll use the tiny Shakespeare dataset for this tutorial.\n\n```python\n# Download tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# read and inspect the file\nwith open('input.txt', 'r') as f:\n  text = f.read()\nprint(\"Length of dataset in characters: \", len(text))\nprint(text[:1000])\n```\n\n### Tokenization\n\nThe first interesting concept is **tokenization**. We want to convert our input text into integers that our model can work with. The model will predict the next integer, which we can then decode back into a character.\n\nOur simple approach creates a dictionary of all available characters (including newline) and assigns each an integer. For more sophisticated treatments, you'd want to look at SentencePiece by Google or tiktoken by OpenAI. There's always a tradeoff between dictionary size and encoding lengths.\n\n```python\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(f\"{vocab_size} characters in dictionary : {''.join(chars)}\")\n# First one is a new line!\n\n# Create a mapping from characters to integers (A Tokenizer)\n# This is called tokenizing. Google uses SentencePiece. OpenAI and GPT uses tiktoken\nitos = { i:ch for i, ch in enumerate(chars)}\nstoi = { ch:i for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s] # encoder: convert string to list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: convert list of integers to string\n\n# Now encode the whole text dataset and store it as a pytorch Tensor\nimport torch\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:100])\nprint(decode([int(v) for v in (data[:100].data)]))\n\n# Split the data into training and validation sets\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n```\n\n### Block Size and Context Windows\n\nThe second important concept is **block size**, which sets the context window length. This is the size of the 'memory' of the model. A larger block size would give more long-range associations, but is probably more expensive to train and would require more data.\n\nWith block size defined, we can input batch_size rows of block_size integers for each data batch. Training in batches is faster since multiple independent blocks can be processed simultaneously.\n\n```python\nblock_size = 8 # or alternatively context lengths\ntrain_data[:block_size+1]\n# in a sample of 9 characters, we have 8 examples with different context lengths.\n\ntorch.manual_seed(1337)\nbatch_size = 4\nblock_size = 8\n\ndef get_batch(split):\n  # generate a small batch of data of inputs x and targets y\n  data = train_data if split == 'train' else val_data\n  ix = torch.randint(len(data) - block_size , (batch_size,)) # should have a -1\n  x = torch.stack([data[i:i+block_size] for i in ix])\n  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n  return x, y\n```\n\n## The Bigram Model\n\nNow let's implement our first model - a simple bigram model. This model predicts the next character based only on the current character (context length of 1) and stores probabilities for the next character.\n\nWe use a log-likelihood/cross-entropy loss. Initially, we expect the loss to be around ln(1/65) â‰ˆ 4.17, since we have 65 characters and initially each character should be equally likely.\n\n```python\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\nfrom einops import rearrange\n\nclass BigramLanguageModel(nn.Module):\n\n  def __init__(self, vocab_size):\n    super().__init__() # initialize the parent class Module\n    # each token directly reads off the logits for the next token from a lookup table\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n  def forward(self, idx, targets=None):\n    # idx and targets and both (B,T) tensors of integers\n    logits = self.token_embedding_table(idx) # (B,T,C) batch time channel\n\n    if targets is None:\n      loss = None\n    else:\n      logits = rearrange(logits,'b t c -> (b t) c')\n      targets = rearrange(targets, 'b t -> (b t)')\n      loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\n  def generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indicies in the current context\n    for _ in range(max_new_tokens):\n      # get the predictions\n      logits, loss = self(idx)\n      # focus only on last time step\n      logits = logits[:, -1 , :] # becomes B x C\n      # apply softmax to get probabilties\n      probs = F.softmax(logits, dim=1) # becomes B x C\n      # sample from distribution\n      idx_next = torch.multinomial(probs, num_samples=1) # B x 1\n      # append sampled index to running sequence\n      idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1,1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```\n\nNote that for PyTorch, when we subclass 'nn.Module', calling the model (including 'self') actually calls the 'forward' method!\n\n## Training the Bigram Model\n\nWe can train our model by choosing an **optimizer** and a **learning rate**. A good learning rate is typically 1e-3, but for smaller networks you can get away with higher rates.\n\n```python\n# training\nbatch_size = 32\n\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nfor steps in range(10000):\n  # sample a batch of data\n  xb, yb = get_batch('train')\n\n  logits, loss = m(xb, yb)\n  optimizer.zero_grad(set_to_none=True)\n  loss.backward()\n  optimizer.step()\n\nprint(loss.item())\n```\n\n## Results\n\nUnfortunately, the results using the bigram model aren't great:\n\n```\nIyoteng h hasbe pave pirance\nRie hicomyonthar's\nPlinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\nKIN d pe wither vouprrouthercc.\nhathe; d!\nMy hind tt hinig t ouchos tes; st yo\n# Not quite Shakespeare\n```\n\nThe bigram model is too simple - it can only look at one character back to predict the next. We need something more sophisticated that can consider longer context windows and learn more complex patterns.\n\n## What's Next?\n\nIn the next part of this series, we'll introduce the key mathematical trick that makes transformers possible: the attention mechanism. We'll start with a mathematical foundation for weighted aggregation and build up to single-head self-attention.\n\nThe bigram model gives us a solid foundation to build upon, and more importantly, it establishes our training pipeline and data handling. Now we're ready to dive into the real magic of transformers!","title":"Building GPT from Scratch - Part 1: Data Loading and the Bigram Model","date":"2025-08-04","excerpt":"","tags":["transformers","tutorial"]}},"__N_SSG":true}