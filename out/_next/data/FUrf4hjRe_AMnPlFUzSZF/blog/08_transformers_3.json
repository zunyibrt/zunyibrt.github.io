{"pageProps":{"post":{"slug":"08_transformers_3","content":"*This is Part 3 of a 3-part series on building a Generative Pretrained Transformer from scratch. In Parts 1 and 2, we built the data pipeline and attention mechanism. Now we'll complete our transformer with the remaining key components.*\n\n## Feed-Forward Networks\n\nAfter our attention mechanism learns to communicate between tokens, we need to give the model computation time to process what it has learned. This is where feed-forward networks come in - they're simply MLPs with ReLU activation that allow the model to \"think\" about the information it just gathered.\n\n```python\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n\nWe add this to our model after the attention:\n\n```python\n# In our model:\nself.sa_head = MultiHeadAttention(4, n_embd//4)\nself.ffwd = FeedFoward(n_embd) # MLP with RELU\n\n# In forward pass:\nx = self.sa_head(x)\nx = self.ffwd(x)\nlogits = self.lm_head(x)\n```\n\n## Transformer Blocks: Communication + Computation\n\nThe pattern of attention (communication) followed by feed-forward (computation) is so fundamental that we package it into reusable **Transformer Blocks**. These blocks can be stacked to create deeper networks that alternate between communication and computation phases.\n\n```python\nn_head = 4\nn_layer = 2\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n```\n\nWe can then stack multiple blocks:\n\n```python\n# In our model:\nself.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n\n# In forward pass:\nx = tok_emb + pos_emb # (B,T,C)\nx = self.blocks(x)\nlogits = self.lm_head(x)\n```\n\nAt this point, our model starts generating recognizable English words!\n\n```\nRILELE\nMAssell use you.\nNt I RYan lake off.\n\nFicink'd\nhote carewtledeche quie to whanl Gatt Mejesery ely:\nThe if, bet leveses it theave be ry skit you file.\n\nKay wred tome dake stance, suks,\nAdech JORo!\n\nALOUSBET:\nWis brake grourst and ald creapsss,\nAndite\nnoat,\nAmothery are doreast is\n```\n\n## Training Optimizations\n\nAs our network gets deeper with multiple transformer blocks, we need optimizations to improve training stability and performance.\n\n### Residual Connections\n\nThe first optimization is **residual/skip connections**, introduced in the [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) paper. These connections allow gradients to flow directly through the network, avoiding training bottlenecks in deep networks.\n\nInstead of 'x = self.sa(x)', we do 'x = x + self.sa(x)'. The computation now returns a residual that gets added to the original input:\n\n```python\nclass Block(nn.Module):\n    def forward(self, x):\n        x = x + self.sa(x)  # residual connection\n        x = x + self.ffwd(x)  # residual connection\n        return x\n```\n\n### Projection Layers\n\nWe also add projection layers to map back to the embedding space after our multi-head attention and expand our feed-forward networks as in the original paper:\n\n```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),  # expand by 4x as in the paper\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),  # project back down\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n\n### Layer Normalization\n\nLayer normalization helps stabilize training by normalizing inputs to have zero mean and unit variance. We apply it before both the self-attention and feed-forward computations in each block, and once more after all blocks:\n\n```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n```\n\n### Dropout\n\nFinally, we add **dropout** for regularization, which randomly sets some activations to zero during training to prevent overfitting:\n\n```python\nclass FeedFoward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),  # add dropout\n        )\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n```\n\n## Final Model and Hyperparameters\n\nWe now have a complete decoder-only transformer ready to scale and train! Here are the hyperparameters for our final model:\n\n```python\n# hyperparameters\nbatch_size = 32\nblock_size = 32\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\neval_iters = 200\nn_embd = 64\nn_head = 6\nn_layer = 6\ndropout = 0.2\n```\n\n## Final Results\n\nAfter training our complete transformer, we get much more structured output:\n\n```\nDOF GlLN:\nThey good, then then, bladgy bone not thindnes\nI way Jeain, fainly!\n\nISABELLA:\nI way, thou fourd to havary flown dews'm-sine.\n\nNowZALLET:\nWhich here old thy's warring hiod\nOn dearys hory be wive to more; greseli!\n\nBut nighd Wart, prance:\nBarch,\nAnd prayem not welld she you, coldinger:\nO I the oldst God somed:\nSirry is let never To be whith, I new'n be thy limpiny\nWord: where deblitiss for give upon the conqueennifult,\nAnd so pobeterl. by they she thy truge,\nIf you his let a brotess.\n```\n\nWhile still not perfect, the improvement is dramatic! The model now:\n- Uses proper character names (ISABELLA, etc.)\n- Maintains somewhat consistent dramatic structure\n- Shows more coherent word formation\n- Demonstrates longer-range dependencies\n\n## What We've Accomplished\n\nIn this three-part series, we've built a complete transformer from scratch:\n\n1. **Part 1**: Data loading, tokenization, and a simple bigram baseline\n2. **Part 2**: The attention mechanism - the core innovation of transformers\n3. **Part 3**: Complete transformer architecture with all the essential components\n\n### Key Components We've Implemented:\n\n- **Self-attention mechanisms** for learning relationships between tokens\n- **Multi-head attention** for capturing different types of relationships\n- **Feed-forward networks** for computation after communication\n- **Transformer blocks** that stack attention and computation\n- **Residual connections** for stable training of deep networks\n- **Layer normalization** for training stability\n- **Positional embeddings** so the model understands sequence order\n- **Dropout** for regularization and preventing overfitting\n\n## The Path Forward\n\nOur small transformer demonstrates the core principles, but to achieve GPT-level performance, you'd need to scale up significantly:\n\n- **More parameters**: Modern language models have billions of parameters\n- **More data**: Training on much larger text corpora\n- **More compute**: Training for weeks or months on powerful hardware\n- **Better tokenization**: Using subword tokenizers like BPE or SentencePiece\n- **Longer context windows**: Supporting much longer sequences\n\n## Key Takeaways\n\n1. **Attention is the core innovation**: The self-attention mechanism allows tokens to communicate and share information based on content rather than just position.\n\n2. **Transformers are surprisingly simple**: The architecture is just attention + feed-forward blocks stacked together with some normalization and residual connections.\n\n3. **Scale matters**: The same architecture that gives us semi-coherent Shakespeare can generate human-level text when scaled up with more parameters, data, and compute.\n\n4. **Training stability is crucial**: Residual connections, layer normalization, and proper weight initialization are essential for training deep networks.\n\n## Conclusion\n\nThis tutorial has shown how to build a transformer model from scratch using PyTorch. We've implemented all the key components of the decoder transformer architecture and seen how they work together to create a language model capable of generating structured text.\n\nThe progression from random gibberish (bigram model) to semi-coherent Shakespeare-like text (full transformer) demonstrates the power of the attention mechanism and proper architectural choices. While our small model doesn't generate fully coherent text yet, scaling up the parameters, training data, and computation would lead to increasingly capable language models.\n\nThe complete nanoGPT implementation, with additional optimizations and the ability to train larger models, can be found in [Andrej Karpathy's repository](https://github.com/karpathy/nanoGPT). This serves as an excellent foundation for understanding and experimenting with transformer architectures.","title":"Building GPT from Scratch - Part 3: Building the Complete Transformer","date":"2025-08-06","excerpt":"","tags":["transformers","tutorial"]}},"__N_SSG":true}