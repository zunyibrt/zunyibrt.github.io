{"pageProps":{"post":{"slug":"10_transformers_5","content":"*This is Part 5 of our GPT from scratch series. Having built and optimized our transformer in Parts 1-4, we now upgrade from character-level tokenization to modern subword tokenization using tiktoken.*\n\n## What is Tokenization?\n\n**Tokenization** is the process of breaking text into smaller units called *tokens*. A *tokenizer* handles this task and assigns each token a unique integer ID from its vocabulary. Andrej Karpathy gives a great overview of tokenization in [this lecture](https://www.youtube.com/watch?v=zduSFxRajkE).\n\n## Tokenizer Vocabulary\n\nThe tokenizer's vocabulary is the full set of tokens it knows how to process. Tokenizers are *trained* on large text corpora—though this training differs from training a neural network. You can even train your own tokenizer and control its vocabulary size and tokenization rules.\n\n## How Strings Are Tokenized\n\nIn English, tokens usually range from single characters to whole words—like `\"t\"` or `\" great\"`. Different languages and models may result in longer or shorter tokens. Most modern LLMs use *subword* tokenization methods, which balance vocabulary size with coverage of possible inputs. Some common tokenization algorithms include **Byte Pair Encoding (BPE)**, **WordPiece** and **Unigram**.\n\n## Why Tokenization Matters\n\nIf your input text includes tokens *not* in the tokenizer's vocabulary, things can go wrong—often in frustrating or subtle ways. Many quirks and failures in LLM behavior can be traced back to poor or unexpected tokenization.\n\n> Tokenization issues are a major source of bugs in LLM applications.\n\n## Using Tiktoken\n\nWe'll use [`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md), OpenAI's fast, open-source tokenizer. Given a text (e.g., `\"tiktoken is great!\"`) and an encoding (e.g., `\"cl100k_base\"`), it produces a tokenized output like:\n\n```python\n[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]\n```\n\nYou can test tokenizations interactively using:\n\n- [**OpenAI Tokenizer Tool**](https://beta.openai.com/tokenizer)\n- [**Tiktokenizer Web App**](https://tiktokenizer.vercel.app/)\n\nThis is as simple as letting\n\n```python\nenc = tiktoken.encoding_for_model(\"gpt-2\")\n```\n\nand then calling `enc.encode` and `enc.decode`.\n\n## Scaling Challenges with Larger Vocabularies\n\nLet's try `o200k_base`, which is used in `gpt-4o`. The vocab size is now 200,000 and our number of parameters in the model jumps to 164M! This is because the embedding and unembedding layers scale with the vocab size. Since `o200k_base` is too big, let's try the GPT-2 model which has a vocab size of around 50,000. Our model is now reduced to 50M parameters, still much bigger than what we had previously, with most of the weights in the embedding and unembedding layers. \n\nCurrently, our dataset that we train on is small, so this is overkill. Most tokens probably don't even appear! We can see this disparity when we try to train the model:\n\n```\nstep 0: train loss 10.8323, val loss 10.8315\nstep 500: train loss 4.8271, val loss 5.3269\nstep 1000: train loss 3.9008, val loss 4.8286\nstep 1500: train loss 3.3493, val loss 4.7968\nstep 2000: train loss 2.7602, val loss 4.9501\nstep 2500: train loss 2.1606, val loss 5.2748\nstep 3000: train loss 1.4815, val loss 5.7240\n```\n\nThe validation loss is terrible, i.e. next token prediction is bad. Note that our tokens no longer correspond to individual characters. The training loss keeps going down since we now have an even more extreme example of overfitting to the data with increased parameters. \n\n## Improved Text Generation\n\nInterestingly, the output is still quite readable since tokens are sub-word and not character level, showing the advantage of an increased vocabulary:\n\n```\nwhat stay to-morrow! I must desire her:\nHard shrowa thousand pound a book\nOur friend. The one cockues!\n\nFLORDS:\nAy, good testimony, come.\n\nServant:\nMy lord!\n\nMIONwell, my lord, the heavens to cry't: look down,\nAnd occupations speak again, I do come on,\nIs't speak to see you. All my lord--\nAlack, your lords! my lords, who, I have\nUpon my heart:--your convey him, my lord,\nMowius; from my blood against your hand,\nOut on your stables there! O injurious sin!\n\nISABELLHORIZELO:\nStay, good heaven;\nMy soul is some good's liver for her true self,\nWhose settled good edencing trade here.\n```\n\nDespite the overfitting issues, the subword tokenization produces more natural-sounding text compared to our character-level approach. Words are more complete and the overall flow is more coherent.\n\n## Key Takeaways\n\nWe have improved the tokenization part of our model, but we need to train on a larger dataset to properly utilize the expanded vocabulary. The main insights from this upgrade:\n\n1. **Modern tokenization** uses subword algorithms that balance vocabulary efficiency with coverage\n2. **Vocabulary size directly impacts model parameters** - larger vocabularies mean more embedding parameters\n3. **Dataset size must match vocabulary size** - small datasets can't effectively train large vocabularies\n4. **Subword tokens produce more natural text** even when the model is overfitting\n\nIn the next section, we'll address the dataset limitation by training on a much larger corpus to properly leverage our improved tokenization.","title":"Building GPT from Scratch - Part 5: Tokenization","date":"2025-08-08","excerpt":"","tags":["transformers","tutorial"]}},"__N_SSG":true}