{"pageProps":{"post":{"slug":"07_transformers_2","content":"*This is Part 2 of a 3-part series on building a Generative Pretrained Transformer from scratch. In Part 1, we built a simple bigram model. Now we'll introduce the core innovation that makes transformers work: Attention.*\n\n## The Mathematical Foundation: Weighted Aggregation\n\nBefore diving into attention, let's understand a neat mathematical trick that forms its foundation. By multiplying a row-normalized T×T lower triangular matrix with a T×C matrix, we can get a T×C matrix which calculates a running average of the preceding rows.\n\nWhen we combine this with softmax (setting -inf instead of 0), we get a more general weighted aggregation of past elements:\n\n```python\n# Weighted aggregation with softmax\ntorch.manual_seed(1337)\nB,T,C = 4,8,2\nx = torch.randn(B,T,C)\n\ntril = torch.tril(torch.ones(T,T)) # lower triangular\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril==0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nres = wei @ x\nres.shape\n```\n\nThis is the mathematical foundation of attention - we're learning to compute weighted averages of past tokens, where the weights are determined by the content of the tokens themselves.\n\n## Single Head Self-Attention\n\nNow for the meat of transformers: the attention mechanism. We create an attention space of dimension 'head_size'. We project each token into this space using three matrices: **Key (K)**, **Query (Q)**, and **Value (V)**.\n\nHere's the intuition:\n- **Query (Q)**: What the token is looking for\n- **Key (K)**: What the token contains/offers\n- **Value (V)**: What the token communicates if paid attention to\n\nWhen K and Q are similar (measured using dot product), they have high affinity and receive higher attention weights. We don't aggregate tokens directly - we aggregate V(X), which represents what each token communicates when attended to.\n\nThe normalization by 1/√(head_size) is crucial. When Q and K have unit variance, this ensures the attention weights also have unit variance. Without this normalization, softmax would saturate towards one-hot vectors, meaning tokens would only aggregate information from a single previous token.\n\n```python\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ rearrange(k, 'b t h -> b h t') * C**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n```\n\nNote that we register 'tril' as a buffer since it's not a learnable parameter of the model.\n\n## Important Notes About Attention\n\nHere are some crucial points about how attention works:\n\n- **Note 1**: Attention is a communication mechanism. You have a directed graph and you want every node to take weighted sums of every node pointing to it.\n\n- **Note 2**: Positional encoding must be embedded in the nodes, since the structure of the attention mechanism doesn't inherently understand position.\n\n- **Note 3**: Batches are independent - there's no cross-communication between different sequences in a batch.\n\n- **Note 4**: Here we ignore future tokens using triangular masking. If you were doing sentiment analysis, you might want to allow this communication. Encoder blocks would allow bidirectional attention, decoder blocks would not. The difference is just the triangular masking.\n\n- **Note 5**: This is called \"self-attention\" because K, Q, V all come from the same input X. If the keys and values came from somewhere else, you'd have \"cross-attention\", which is used when there's a separate source of information to pull from.\n\n## Building the Complete Single-Head Model\n\nLet's put everything together into a complete model with single-head attention. We'll make several improvements to our training setup:\n\n```python\n# hyperparameters\nbatch_size = 32\nblock_size = 8 \nmax_iters = 5000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32\ntorch.manual_seed(1337)\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n```\n\nKey improvements in this version:\n\n1. **GPU Support**: We move data and model to GPU using 'device = 'cuda''\n2. **Better Loss Estimation**: 'estimate_loss()' averages losses over multiple batches for more accurate estimates\n3. **No Gradient Context**: '@torch.no_grad()' tells PyTorch this function won't be backpropagated through\n4. **Embedding Dimension**: Our logits are now embedded in an embedding dimension, so we need an \"unembedding\" linear layer\n5. **Positional Embeddings**: We add position information so tokens know where they are in the sequence\n\n```python\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_head = Head(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.sa_head(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```\n\n## Results with Single-Head Attention\n\nAfter training, we get improved results that show more structure, though still not quite Shakespeare:\n\n```\nArrg hor; ho, ho aak.\n\nLUCKUCARE Fqhelree ndel te:\nThece waplt ko.\nFOCI\nMHABRIEKY tur waverer orid betievis dyof b.\n\nBo,\nABIUCE:\nN-, evesune athid nt cobasr!, Go hern, alsemin rsin varit ther I;\nANher en:ouingaroua lis py Bh mithe ast prird band bad youun theis pioat hed man ile ere sty hanonoue avillars d ty hon I:\nSo Ilar chy w'guma my tono yat d,\nI more hathive, ha utithe baag tee\nIile athadog at y hoke hay hate whinsintoourtarenol usdon co whe sou; mer wif yoh hlele coud illesen?o I choteban\n```\n\nThe attention mechanism has given our model the ability to look back at previous characters and make more informed predictions. We can see more English-like structure emerging.\n\n## Multi-Head Attention\n\nInstead of using a single attention head, we can use multiple smaller heads in parallel. This allows the model to attend to different types of relationships simultaneously - some heads might focus on syntax, others on semantics, etc.\n\n```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n```\n\nIn our model, we replace the single head with:\n```python\nself.sa_head = MultiHeadAttention(4, n_embd//4)\n```\n\nThis gives us slightly better loss and more diverse attention patterns.\n\n## What's Next?\n\nWe now have a working attention mechanism! However, our model is still missing several key components that make modern transformers work well:\n\n- **Feed-forward networks** for computation after attention\n- **Transformer blocks** that repeat the attention + computation pattern\n- **Residual connections** and **layer normalization** for training stability\n- **Dropout** for regularization\n\nIn Part 3, we'll add these final components and see our model start generating much more coherent text. The attention mechanism we've built here is the foundation - everything else builds on top of it to create the full transformer architecture.","title":"Building GPT from Scratch - Part 2: The Attention Mechanism","date":"2025-08-05","excerpt":"","tags":["transformers","tutorial"]}},"__N_SSG":true}