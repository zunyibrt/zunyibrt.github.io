<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="description" content="Brent&#x27;s homepage"/><meta name="author" content="Brent Tan"/><title>Posts - Brent Tan</title><meta name="twitter:title" content="Posts - Brent Tan"/><meta property="og:title" content="Posts - Brent Tan"/><meta name="next-head-count" content="7"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/e9556718e3f0f4cf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e9556718e3f0f4cf.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-c7e8ec73d08c57f3.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-676fc21436baeb6f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-cb76b27212f05dea.js" defer=""></script><script src="/_next/static/chunks/925-f007557d552aa794.js" defer=""></script><script src="/_next/static/chunks/pages/writing-a5fb7bce5e11f557.js" defer=""></script><script src="/_next/static/FFpp1ErOPN9PuGJ0QVGRn/_buildManifest.js" defer=""></script><script src="/_next/static/FFpp1ErOPN9PuGJ0QVGRn/_ssgManifest.js" defer=""></script><style id="__jsx-fed65fd97a038da">@import url("https://fonts.googleapis.com/css2?family=M+PLUS+Rounded+1c:wght@300;700&display=swap");@import url("https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@100..900&display=swap");</style></head><body><script id="chakra-script">!(function(){try{var a=function(c){var v="(prefers-color-scheme: dark)",h=window.matchMedia(v).matches?"dark":"light",r=c==="system"?h:c,o=document.documentElement,s=document.body,l="chakra-ui-light",d="chakra-ui-dark",i=r==="dark";return s.classList.add(i?d:l),s.classList.remove(i?l:d),o.style.colorScheme=r,o.dataset.theme=r,r},n=a,m="dark",e="chakra-ui-color-mode",t=localStorage.getItem(e);t?a(t):localStorage.setItem(e,a(m))}catch(a){}})();</script><div id="__next"><style data-emotion="css-global 93m1n8">:host,:root,[data-theme]{--chakra-ring-inset:var(--chakra-empty,/*!*/ /*!*/);--chakra-ring-offset-width:0px;--chakra-ring-offset-color:#fff;--chakra-ring-color:rgba(66, 153, 225, 0.6);--chakra-ring-offset-shadow:0 0 #0000;--chakra-ring-shadow:0 0 #0000;--chakra-space-x-reverse:0;--chakra-space-y-reverse:0;--chakra-colors-transparent:transparent;--chakra-colors-current:currentColor;--chakra-colors-black:#000000;--chakra-colors-white:#FFFFFF;--chakra-colors-whiteAlpha-50:rgba(255, 255, 255, 0.04);--chakra-colors-whiteAlpha-100:rgba(255, 255, 255, 0.06);--chakra-colors-whiteAlpha-200:rgba(255, 255, 255, 0.08);--chakra-colors-whiteAlpha-300:rgba(255, 255, 255, 0.16);--chakra-colors-whiteAlpha-400:rgba(255, 255, 255, 0.24);--chakra-colors-whiteAlpha-500:rgba(255, 255, 255, 0.36);--chakra-colors-whiteAlpha-600:rgba(255, 255, 255, 0.48);--chakra-colors-whiteAlpha-700:rgba(255, 255, 255, 0.64);--chakra-colors-whiteAlpha-800:rgba(255, 255, 255, 0.80);--chakra-colors-whiteAlpha-900:rgba(255, 255, 255, 0.92);--chakra-colors-blackAlpha-50:rgba(0, 0, 0, 0.04);--chakra-colors-blackAlpha-100:rgba(0, 0, 0, 0.06);--chakra-colors-blackAlpha-200:rgba(0, 0, 0, 0.08);--chakra-colors-blackAlpha-300:rgba(0, 0, 0, 0.16);--chakra-colors-blackAlpha-400:rgba(0, 0, 0, 0.24);--chakra-colors-blackAlpha-500:rgba(0, 0, 0, 0.36);--chakra-colors-blackAlpha-600:rgba(0, 0, 0, 0.48);--chakra-colors-blackAlpha-700:rgba(0, 0, 0, 0.64);--chakra-colors-blackAlpha-800:rgba(0, 0, 0, 0.80);--chakra-colors-blackAlpha-900:rgba(0, 0, 0, 0.92);--chakra-colors-gray-50:#F7FAFC;--chakra-colors-gray-100:#EDF2F7;--chakra-colors-gray-200:#E2E8F0;--chakra-colors-gray-300:#CBD5E0;--chakra-colors-gray-400:#A0AEC0;--chakra-colors-gray-500:#718096;--chakra-colors-gray-600:#4A5568;--chakra-colors-gray-700:#2D3748;--chakra-colors-gray-800:#1A202C;--chakra-colors-gray-900:#171923;--chakra-colors-red-50:#FFF5F5;--chakra-colors-red-100:#FED7D7;--chakra-colors-red-200:#FEB2B2;--chakra-colors-red-300:#FC8181;--chakra-colors-red-400:#F56565;--chakra-colors-red-500:#E53E3E;--chakra-colors-red-600:#C53030;--chakra-colors-red-700:#9B2C2C;--chakra-colors-red-800:#822727;--chakra-colors-red-900:#63171B;--chakra-colors-orange-50:#FFFAF0;--chakra-colors-orange-100:#FEEBC8;--chakra-colors-orange-200:#FBD38D;--chakra-colors-orange-300:#F6AD55;--chakra-colors-orange-400:#ED8936;--chakra-colors-orange-500:#DD6B20;--chakra-colors-orange-600:#C05621;--chakra-colors-orange-700:#9C4221;--chakra-colors-orange-800:#7B341E;--chakra-colors-orange-900:#652B19;--chakra-colors-yellow-50:#FFFFF0;--chakra-colors-yellow-100:#FEFCBF;--chakra-colors-yellow-200:#FAF089;--chakra-colors-yellow-300:#F6E05E;--chakra-colors-yellow-400:#ECC94B;--chakra-colors-yellow-500:#D69E2E;--chakra-colors-yellow-600:#B7791F;--chakra-colors-yellow-700:#975A16;--chakra-colors-yellow-800:#744210;--chakra-colors-yellow-900:#5F370E;--chakra-colors-green-50:#F0FFF4;--chakra-colors-green-100:#C6F6D5;--chakra-colors-green-200:#9AE6B4;--chakra-colors-green-300:#68D391;--chakra-colors-green-400:#48BB78;--chakra-colors-green-500:#38A169;--chakra-colors-green-600:#2F855A;--chakra-colors-green-700:#276749;--chakra-colors-green-800:#22543D;--chakra-colors-green-900:#1C4532;--chakra-colors-teal-50:#E6FFFA;--chakra-colors-teal-100:#B2F5EA;--chakra-colors-teal-200:#81E6D9;--chakra-colors-teal-300:#4FD1C5;--chakra-colors-teal-400:#38B2AC;--chakra-colors-teal-500:#319795;--chakra-colors-teal-600:#2C7A7B;--chakra-colors-teal-700:#285E61;--chakra-colors-teal-800:#234E52;--chakra-colors-teal-900:#1D4044;--chakra-colors-blue-50:#ebf8ff;--chakra-colors-blue-100:#bee3f8;--chakra-colors-blue-200:#90cdf4;--chakra-colors-blue-300:#63b3ed;--chakra-colors-blue-400:#4299e1;--chakra-colors-blue-500:#3182ce;--chakra-colors-blue-600:#2b6cb0;--chakra-colors-blue-700:#2c5282;--chakra-colors-blue-800:#2a4365;--chakra-colors-blue-900:#1A365D;--chakra-colors-cyan-50:#EDFDFD;--chakra-colors-cyan-100:#C4F1F9;--chakra-colors-cyan-200:#9DECF9;--chakra-colors-cyan-300:#76E4F7;--chakra-colors-cyan-400:#0BC5EA;--chakra-colors-cyan-500:#00B5D8;--chakra-colors-cyan-600:#00A3C4;--chakra-colors-cyan-700:#0987A0;--chakra-colors-cyan-800:#086F83;--chakra-colors-cyan-900:#065666;--chakra-colors-purple-50:#FAF5FF;--chakra-colors-purple-100:#E9D8FD;--chakra-colors-purple-200:#D6BCFA;--chakra-colors-purple-300:#B794F4;--chakra-colors-purple-400:#9F7AEA;--chakra-colors-purple-500:#805AD5;--chakra-colors-purple-600:#6B46C1;--chakra-colors-purple-700:#553C9A;--chakra-colors-purple-800:#44337A;--chakra-colors-purple-900:#322659;--chakra-colors-pink-50:#FFF5F7;--chakra-colors-pink-100:#FED7E2;--chakra-colors-pink-200:#FBB6CE;--chakra-colors-pink-300:#F687B3;--chakra-colors-pink-400:#ED64A6;--chakra-colors-pink-500:#D53F8C;--chakra-colors-pink-600:#B83280;--chakra-colors-pink-700:#97266D;--chakra-colors-pink-800:#702459;--chakra-colors-pink-900:#521B41;--chakra-colors-linkedin-50:#E8F4F9;--chakra-colors-linkedin-100:#CFEDFB;--chakra-colors-linkedin-200:#9BDAF3;--chakra-colors-linkedin-300:#68C7EC;--chakra-colors-linkedin-400:#34B3E4;--chakra-colors-linkedin-500:#00A0DC;--chakra-colors-linkedin-600:#008CC9;--chakra-colors-linkedin-700:#0077B5;--chakra-colors-linkedin-800:#005E93;--chakra-colors-linkedin-900:#004471;--chakra-colors-facebook-50:#E8F4F9;--chakra-colors-facebook-100:#D9DEE9;--chakra-colors-facebook-200:#B7C2DA;--chakra-colors-facebook-300:#6482C0;--chakra-colors-facebook-400:#4267B2;--chakra-colors-facebook-500:#385898;--chakra-colors-facebook-600:#314E89;--chakra-colors-facebook-700:#29487D;--chakra-colors-facebook-800:#223B67;--chakra-colors-facebook-900:#1E355B;--chakra-colors-messenger-50:#D0E6FF;--chakra-colors-messenger-100:#B9DAFF;--chakra-colors-messenger-200:#A2CDFF;--chakra-colors-messenger-300:#7AB8FF;--chakra-colors-messenger-400:#2E90FF;--chakra-colors-messenger-500:#0078FF;--chakra-colors-messenger-600:#0063D1;--chakra-colors-messenger-700:#0052AC;--chakra-colors-messenger-800:#003C7E;--chakra-colors-messenger-900:#002C5C;--chakra-colors-whatsapp-50:#dffeec;--chakra-colors-whatsapp-100:#b9f5d0;--chakra-colors-whatsapp-200:#90edb3;--chakra-colors-whatsapp-300:#65e495;--chakra-colors-whatsapp-400:#3cdd78;--chakra-colors-whatsapp-500:#22c35e;--chakra-colors-whatsapp-600:#179848;--chakra-colors-whatsapp-700:#0c6c33;--chakra-colors-whatsapp-800:#01421c;--chakra-colors-whatsapp-900:#001803;--chakra-colors-twitter-50:#E5F4FD;--chakra-colors-twitter-100:#C8E9FB;--chakra-colors-twitter-200:#A8DCFA;--chakra-colors-twitter-300:#83CDF7;--chakra-colors-twitter-400:#57BBF5;--chakra-colors-twitter-500:#1DA1F2;--chakra-colors-twitter-600:#1A94DA;--chakra-colors-twitter-700:#1681BF;--chakra-colors-twitter-800:#136B9E;--chakra-colors-twitter-900:#0D4D71;--chakra-colors-telegram-50:#E3F2F9;--chakra-colors-telegram-100:#C5E4F3;--chakra-colors-telegram-200:#A2D4EC;--chakra-colors-telegram-300:#7AC1E4;--chakra-colors-telegram-400:#47A9DA;--chakra-colors-telegram-500:#0088CC;--chakra-colors-telegram-600:#007AB8;--chakra-colors-telegram-700:#006BA1;--chakra-colors-telegram-800:#005885;--chakra-colors-telegram-900:#003F5E;--chakra-colors-grassTeal:#88ccca;--chakra-borders-none:0;--chakra-borders-1px:1px solid;--chakra-borders-2px:2px solid;--chakra-borders-4px:4px solid;--chakra-borders-8px:8px solid;--chakra-fonts-heading:'M PLUS Rounded 1c';--chakra-fonts-body:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--chakra-fonts-mono:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--chakra-fontSizes-3xs:0.45rem;--chakra-fontSizes-2xs:0.625rem;--chakra-fontSizes-xs:0.75rem;--chakra-fontSizes-sm:0.875rem;--chakra-fontSizes-md:1rem;--chakra-fontSizes-lg:1.125rem;--chakra-fontSizes-xl:1.25rem;--chakra-fontSizes-2xl:1.5rem;--chakra-fontSizes-3xl:1.875rem;--chakra-fontSizes-4xl:2.25rem;--chakra-fontSizes-5xl:3rem;--chakra-fontSizes-6xl:3.75rem;--chakra-fontSizes-7xl:4.5rem;--chakra-fontSizes-8xl:6rem;--chakra-fontSizes-9xl:8rem;--chakra-fontWeights-hairline:100;--chakra-fontWeights-thin:200;--chakra-fontWeights-light:300;--chakra-fontWeights-normal:400;--chakra-fontWeights-medium:500;--chakra-fontWeights-semibold:600;--chakra-fontWeights-bold:700;--chakra-fontWeights-extrabold:800;--chakra-fontWeights-black:900;--chakra-letterSpacings-tighter:-0.05em;--chakra-letterSpacings-tight:-0.025em;--chakra-letterSpacings-normal:0;--chakra-letterSpacings-wide:0.025em;--chakra-letterSpacings-wider:0.05em;--chakra-letterSpacings-widest:0.1em;--chakra-lineHeights-3:.75rem;--chakra-lineHeights-4:1rem;--chakra-lineHeights-5:1.25rem;--chakra-lineHeights-6:1.5rem;--chakra-lineHeights-7:1.75rem;--chakra-lineHeights-8:2rem;--chakra-lineHeights-9:2.25rem;--chakra-lineHeights-10:2.5rem;--chakra-lineHeights-normal:normal;--chakra-lineHeights-none:1;--chakra-lineHeights-shorter:1.25;--chakra-lineHeights-short:1.375;--chakra-lineHeights-base:1.5;--chakra-lineHeights-tall:1.625;--chakra-lineHeights-taller:2;--chakra-radii-none:0;--chakra-radii-sm:0.125rem;--chakra-radii-base:0.25rem;--chakra-radii-md:0.375rem;--chakra-radii-lg:0.5rem;--chakra-radii-xl:0.75rem;--chakra-radii-2xl:1rem;--chakra-radii-3xl:1.5rem;--chakra-radii-full:9999px;--chakra-space-1:0.25rem;--chakra-space-2:0.5rem;--chakra-space-3:0.75rem;--chakra-space-4:1rem;--chakra-space-5:1.25rem;--chakra-space-6:1.5rem;--chakra-space-7:1.75rem;--chakra-space-8:2rem;--chakra-space-9:2.25rem;--chakra-space-10:2.5rem;--chakra-space-12:3rem;--chakra-space-14:3.5rem;--chakra-space-16:4rem;--chakra-space-20:5rem;--chakra-space-24:6rem;--chakra-space-28:7rem;--chakra-space-32:8rem;--chakra-space-36:9rem;--chakra-space-40:10rem;--chakra-space-44:11rem;--chakra-space-48:12rem;--chakra-space-52:13rem;--chakra-space-56:14rem;--chakra-space-60:15rem;--chakra-space-64:16rem;--chakra-space-72:18rem;--chakra-space-80:20rem;--chakra-space-96:24rem;--chakra-space-px:1px;--chakra-space-0-5:0.125rem;--chakra-space-1-5:0.375rem;--chakra-space-2-5:0.625rem;--chakra-space-3-5:0.875rem;--chakra-shadows-xs:0 0 0 1px rgba(0, 0, 0, 0.05);--chakra-shadows-sm:0 1px 2px 0 rgba(0, 0, 0, 0.05);--chakra-shadows-base:0 1px 3px 0 rgba(0, 0, 0, 0.1),0 1px 2px 0 rgba(0, 0, 0, 0.06);--chakra-shadows-md:0 4px 6px -1px rgba(0, 0, 0, 0.1),0 2px 4px -1px rgba(0, 0, 0, 0.06);--chakra-shadows-lg:0 10px 15px -3px rgba(0, 0, 0, 0.1),0 4px 6px -2px rgba(0, 0, 0, 0.05);--chakra-shadows-xl:0 20px 25px -5px rgba(0, 0, 0, 0.1),0 10px 10px -5px rgba(0, 0, 0, 0.04);--chakra-shadows-2xl:0 25px 50px -12px rgba(0, 0, 0, 0.25);--chakra-shadows-outline:0 0 0 3px rgba(66, 153, 225, 0.6);--chakra-shadows-inner:inset 0 2px 4px 0 rgba(0,0,0,0.06);--chakra-shadows-none:none;--chakra-shadows-dark-lg:rgba(0, 0, 0, 0.1) 0px 0px 0px 1px,rgba(0, 0, 0, 0.2) 0px 5px 10px,rgba(0, 0, 0, 0.4) 0px 15px 40px;--chakra-sizes-1:0.25rem;--chakra-sizes-2:0.5rem;--chakra-sizes-3:0.75rem;--chakra-sizes-4:1rem;--chakra-sizes-5:1.25rem;--chakra-sizes-6:1.5rem;--chakra-sizes-7:1.75rem;--chakra-sizes-8:2rem;--chakra-sizes-9:2.25rem;--chakra-sizes-10:2.5rem;--chakra-sizes-12:3rem;--chakra-sizes-14:3.5rem;--chakra-sizes-16:4rem;--chakra-sizes-20:5rem;--chakra-sizes-24:6rem;--chakra-sizes-28:7rem;--chakra-sizes-32:8rem;--chakra-sizes-36:9rem;--chakra-sizes-40:10rem;--chakra-sizes-44:11rem;--chakra-sizes-48:12rem;--chakra-sizes-52:13rem;--chakra-sizes-56:14rem;--chakra-sizes-60:15rem;--chakra-sizes-64:16rem;--chakra-sizes-72:18rem;--chakra-sizes-80:20rem;--chakra-sizes-96:24rem;--chakra-sizes-px:1px;--chakra-sizes-0-5:0.125rem;--chakra-sizes-1-5:0.375rem;--chakra-sizes-2-5:0.625rem;--chakra-sizes-3-5:0.875rem;--chakra-sizes-max:max-content;--chakra-sizes-min:min-content;--chakra-sizes-full:100%;--chakra-sizes-3xs:14rem;--chakra-sizes-2xs:16rem;--chakra-sizes-xs:20rem;--chakra-sizes-sm:24rem;--chakra-sizes-md:28rem;--chakra-sizes-lg:32rem;--chakra-sizes-xl:36rem;--chakra-sizes-2xl:42rem;--chakra-sizes-3xl:48rem;--chakra-sizes-4xl:56rem;--chakra-sizes-5xl:64rem;--chakra-sizes-6xl:72rem;--chakra-sizes-7xl:80rem;--chakra-sizes-8xl:90rem;--chakra-sizes-prose:60ch;--chakra-sizes-container-sm:640px;--chakra-sizes-container-md:768px;--chakra-sizes-container-lg:1024px;--chakra-sizes-container-xl:1280px;--chakra-zIndices-hide:-1;--chakra-zIndices-auto:auto;--chakra-zIndices-base:0;--chakra-zIndices-docked:10;--chakra-zIndices-dropdown:1000;--chakra-zIndices-sticky:1100;--chakra-zIndices-banner:1200;--chakra-zIndices-overlay:1300;--chakra-zIndices-modal:1400;--chakra-zIndices-popover:1500;--chakra-zIndices-skipLink:1600;--chakra-zIndices-toast:1700;--chakra-zIndices-tooltip:1800;--chakra-transition-property-common:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform;--chakra-transition-property-colors:background-color,border-color,color,fill,stroke;--chakra-transition-property-dimensions:width,height;--chakra-transition-property-position:left,right,top,bottom;--chakra-transition-property-background:background-color,background-image,background-position;--chakra-transition-easing-ease-in:cubic-bezier(0.4, 0, 1, 1);--chakra-transition-easing-ease-out:cubic-bezier(0, 0, 0.2, 1);--chakra-transition-easing-ease-in-out:cubic-bezier(0.4, 0, 0.2, 1);--chakra-transition-duration-ultra-fast:50ms;--chakra-transition-duration-faster:100ms;--chakra-transition-duration-fast:150ms;--chakra-transition-duration-normal:200ms;--chakra-transition-duration-slow:300ms;--chakra-transition-duration-slower:400ms;--chakra-transition-duration-ultra-slow:500ms;--chakra-blur-none:0;--chakra-blur-sm:4px;--chakra-blur-base:8px;--chakra-blur-md:12px;--chakra-blur-lg:16px;--chakra-blur-xl:24px;--chakra-blur-2xl:40px;--chakra-blur-3xl:64px;--chakra-breakpoints-base:0em;--chakra-breakpoints-sm:30em;--chakra-breakpoints-md:48em;--chakra-breakpoints-lg:62em;--chakra-breakpoints-xl:80em;--chakra-breakpoints-2xl:96em;}.chakra-ui-light :host:not([data-theme]),.chakra-ui-light :root:not([data-theme]),.chakra-ui-light [data-theme]:not([data-theme]),[data-theme=light] :host:not([data-theme]),[data-theme=light] :root:not([data-theme]),[data-theme=light] [data-theme]:not([data-theme]),:host[data-theme=light],:root[data-theme=light],[data-theme][data-theme=light]{--chakra-colors-chakra-body-text:var(--chakra-colors-gray-800);--chakra-colors-chakra-body-bg:var(--chakra-colors-white);--chakra-colors-chakra-border-color:var(--chakra-colors-gray-200);--chakra-colors-chakra-inverse-text:var(--chakra-colors-white);--chakra-colors-chakra-subtle-bg:var(--chakra-colors-gray-100);--chakra-colors-chakra-subtle-text:var(--chakra-colors-gray-600);--chakra-colors-chakra-placeholder-color:var(--chakra-colors-gray-500);}.chakra-ui-dark :host:not([data-theme]),.chakra-ui-dark :root:not([data-theme]),.chakra-ui-dark [data-theme]:not([data-theme]),[data-theme=dark] :host:not([data-theme]),[data-theme=dark] :root:not([data-theme]),[data-theme=dark] [data-theme]:not([data-theme]),:host[data-theme=dark],:root[data-theme=dark],[data-theme][data-theme=dark]{--chakra-colors-chakra-body-text:var(--chakra-colors-whiteAlpha-900);--chakra-colors-chakra-body-bg:var(--chakra-colors-gray-800);--chakra-colors-chakra-border-color:var(--chakra-colors-whiteAlpha-300);--chakra-colors-chakra-inverse-text:var(--chakra-colors-gray-800);--chakra-colors-chakra-subtle-bg:var(--chakra-colors-gray-700);--chakra-colors-chakra-subtle-text:var(--chakra-colors-gray-400);--chakra-colors-chakra-placeholder-color:var(--chakra-colors-whiteAlpha-400);}</style><style data-emotion="css-global fubdgu">html{line-height:1.5;-webkit-text-size-adjust:100%;font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;-moz-osx-font-smoothing:grayscale;touch-action:manipulation;}body{position:relative;min-height:100%;margin:0;font-feature-settings:"kern";}:where(*, *::before, *::after){border-width:0;border-style:solid;box-sizing:border-box;word-wrap:break-word;}main{display:block;}hr{border-top-width:1px;box-sizing:content-box;height:0;overflow:visible;}:where(pre, code, kbd,samp){font-family:SFMono-Regular,Menlo,Monaco,Consolas,monospace;font-size:1em;}a{background-color:transparent;color:inherit;-webkit-text-decoration:inherit;text-decoration:inherit;}abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;}:where(b, strong){font-weight:bold;}small{font-size:80%;}:where(sub,sup){font-size:75%;line-height:0;position:relative;vertical-align:baseline;}sub{bottom:-0.25em;}sup{top:-0.5em;}img{border-style:none;}:where(button, input, optgroup, select, textarea){font-family:inherit;font-size:100%;line-height:1.15;margin:0;}:where(button, input){overflow:visible;}:where(button, select){text-transform:none;}:where(
          button::-moz-focus-inner,
          [type="button"]::-moz-focus-inner,
          [type="reset"]::-moz-focus-inner,
          [type="submit"]::-moz-focus-inner
        ){border-style:none;padding:0;}fieldset{padding:0.35em 0.75em 0.625em;}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;}progress{vertical-align:baseline;}textarea{overflow:auto;}:where([type="checkbox"], [type="radio"]){box-sizing:border-box;padding:0;}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{-webkit-appearance:none!important;}input[type="number"]{-moz-appearance:textfield;}input[type="search"]{-webkit-appearance:textfield;outline-offset:-2px;}input[type="search"]::-webkit-search-decoration{-webkit-appearance:none!important;}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;}details{display:block;}summary{display:-webkit-box;display:-webkit-list-item;display:-ms-list-itembox;display:list-item;}template{display:none;}[hidden]{display:none!important;}:where(
          blockquote,
          dl,
          dd,
          h1,
          h2,
          h3,
          h4,
          h5,
          h6,
          hr,
          figure,
          p,
          pre
        ){margin:0;}button{background:transparent;padding:0;}fieldset{margin:0;padding:0;}:where(ol, ul){margin:0;padding:0;}textarea{resize:vertical;}:where(button, [role="button"]){cursor:pointer;}button::-moz-focus-inner{border:0!important;}table{border-collapse:collapse;}:where(h1, h2, h3, h4, h5, h6){font-size:inherit;font-weight:inherit;}:where(button, input, optgroup, select, textarea){padding:0;line-height:inherit;color:inherit;}:where(img, svg, video, canvas, audio, iframe, embed, object){display:block;}:where(img, video){max-width:100%;height:auto;}[data-js-focus-visible] :focus:not([data-focus-visible-added]):not(
          [data-focus-visible-disabled]
        ){outline:none;box-shadow:none;}select::-ms-expand{display:none;}:root,:host{--chakra-vh:100vh;}@supports (height: -webkit-fill-available){:root,:host{--chakra-vh:-webkit-fill-available;}}@supports (height: -moz-fill-available){:root,:host{--chakra-vh:-moz-fill-available;}}@supports (height: 100dvh){:root,:host{--chakra-vh:100dvh;}}</style><style data-emotion="css-global lbfpqv">body{font-family:var(--chakra-fonts-body);color:var(--chakra-colors-chakra-body-text);background:#202023;transition-property:background-color;transition-duration:var(--chakra-transition-duration-normal);line-height:var(--chakra-lineHeights-base);}*::-webkit-input-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*::-moz-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*:-ms-input-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*::placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*,*::before,::after{border-color:var(--chakra-colors-chakra-border-color);}</style><style data-emotion="css 1lp32oh">.css-1lp32oh{padding-bottom:var(--chakra-space-8);}</style><main class="css-1lp32oh"><style data-emotion="css e1s8q">.css-e1s8q{position:fixed;width:100%;background:#20202380;z-index:2;-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px);}</style><nav path="/writing" class="css-e1s8q"><style data-emotion="css 1wkbond">.css-1wkbond{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:var(--chakra-space-2);max-width:var(--chakra-sizes-container-md);}</style><div class="chakra-container css-1wkbond" wrap="wrap" align="center" justify="space-between"><style data-emotion="css qi9cid">.css-qi9cid{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-right:var(--chakra-space-5);}</style><div class="css-qi9cid"><style data-emotion="css 10zaf98">.css-10zaf98{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-2xl);line-height:1.33;letter-spacing:var(--chakra-letterSpacings-tighter);}@media screen and (min-width: 48em){.css-10zaf98{font-size:var(--chakra-fontSizes-3xl);line-height:1.2;}}</style><h1 class="chakra-heading css-10zaf98"><a href="/"><style data-emotion="css lr1k0q">.css-lr1k0q{font-weight:bold;font-size:18px;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:30px;line-height:20px;padding:10px;}.css-lr1k0q >svg{-webkit-transition:500ms ease;transition:500ms ease;}.css-lr1k0q:hover>svg{-webkit-transform:rotate(90deg);-moz-transform:rotate(90deg);-ms-transform:rotate(90deg);transform:rotate(90deg);}</style><span class="css-lr1k0q"><svg width="20" height="20" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><circle cx="10" cy="10" r="10" fill="#333A3F"></circle><path d="M 9 0 L 9 20 M 9 8 L 20 8" stroke="#FFFFFF" stroke-width="0.8"></path><circle cx="5.5" cy="3.9" r=".3" fill="#FFFFFF"></circle><circle cx="17" cy="5" r=".2" fill="#FFFFFF"></circle><circle cx="5" cy="17" r=".2" fill="#FFFFFF"></circle><circle cx="15" cy="15" r=".3" fill="#FFFFFF"></circle></svg><style data-emotion="css ug13ir">.css-ug13ir{margin-left:var(--chakra-space-3);color:var(--chakra-colors-whiteAlpha-900);font-family:M PLUS Rounded 1c;font-weight:var(--chakra-fontWeights-bold);}</style><p class="chakra-text css-ug13ir">Brent Tan</p></span></a></h1></div><style data-emotion="css 4r1ia7">.css-4r1ia7{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:0.5rem;width:var(--chakra-sizes-full);-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;margin-top:var(--chakra-space-4);}@media screen and (min-width: 48em){.css-4r1ia7{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;width:auto;margin-top:0px;}}</style><div class="chakra-stack css-4r1ia7"><style data-emotion="css qw566">.css-qw566{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;text-underline-offset:3px;padding:var(--chakra-space-2);background:var(--chakra-colors-grassTeal);color:#202023;}.css-qw566:hover,.css-qw566[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-qw566:focus-visible,.css-qw566[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-qw566" href="/writing">Posts</a><style data-emotion="css fvhngf">.css-fvhngf{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;text-underline-offset:3px;padding:var(--chakra-space-2);color:var(--chakra-colors-whiteAlpha-900);}.css-fvhngf:hover,.css-fvhngf[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-fvhngf:focus-visible,.css-fvhngf[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-fvhngf" href="/papers">Projects</a><a class="chakra-link css-fvhngf" href="/research">Research</a><style data-emotion="css sy0160">.css-sy0160{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;text-underline-offset:3px;padding:var(--chakra-space-2);color:var(--chakra-colors-whiteAlpha-900);display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:var(--chakra-space-2);}.css-sy0160:hover,.css-sy0160[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-sy0160:focus-visible,.css-sy0160[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a target="_blank" class="chakra-link css-sy0160" style="gap:4px" href="https://github.com/zunyibrt"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M256 32C132.3 32 32 134.9 32 261.7c0 101.5 64.2 187.5 153.2 217.9a17.56 17.56 0 0 0 3.8.4c8.3 0 11.5-6.1 11.5-11.4 0-5.5-.2-19.9-.3-39.1a102.4 102.4 0 0 1-22.6 2.7c-43.1 0-52.9-33.5-52.9-33.5-10.2-26.5-24.9-33.6-24.9-33.6-19.5-13.7-.1-14.1 1.4-14.1h.1c22.5 2 34.3 23.8 34.3 23.8 11.2 19.6 26.2 25.1 39.6 25.1a63 63 0 0 0 25.6-6c2-14.8 7.8-24.9 14.2-30.7-49.7-5.8-102-25.5-102-113.5 0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8a18.64 18.64 0 0 1 5-.5c8.1 0 26.4 3.1 56.6 24.1a208.21 208.21 0 0 1 112.2 0c30.2-21 48.5-24.1 56.6-24.1a18.64 18.64 0 0 1 5 .5c12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6 0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5 0 30.7-.3 55.5-.3 63 0 5.4 3.1 11.5 11.4 11.5a19.35 19.35 0 0 0 4-.4C415.9 449.2 480 363.1 480 261.7 480 134.9 379.7 32 256 32z"></path></svg></a></div><style data-emotion="css 1rr4qq7">.css-1rr4qq7{-webkit-flex:1;-ms-flex:1;flex:1;}</style><div align="right" class="css-1rr4qq7"><div style="display:inline-block;opacity:1;will-change:transform,opacity;transform:none"><style data-emotion="css j7xjat">.css-j7xjat{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:relative;white-space:nowrap;vertical-align:middle;outline:2px solid transparent;outline-offset:2px;line-height:1.2;border-radius:var(--chakra-radii-md);font-weight:var(--chakra-fontWeights-semibold);transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);height:var(--chakra-sizes-10);min-width:var(--chakra-sizes-10);font-size:var(--chakra-fontSizes-md);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);background:var(--chakra-colors-orange-200);color:var(--chakra-colors-gray-800);padding:0px;}.css-j7xjat:focus-visible,.css-j7xjat[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}.css-j7xjat:disabled,.css-j7xjat[disabled],.css-j7xjat[aria-disabled=true],.css-j7xjat[data-disabled]{opacity:0.4;cursor:not-allowed;box-shadow:var(--chakra-shadows-none);}.css-j7xjat:hover,.css-j7xjat[data-hover]{background:var(--chakra-colors-orange-300);}.css-j7xjat:hover:disabled,.css-j7xjat[data-hover]:disabled,.css-j7xjat:hover[disabled],.css-j7xjat[data-hover][disabled],.css-j7xjat:hover[aria-disabled=true],.css-j7xjat[data-hover][aria-disabled=true],.css-j7xjat:hover[data-disabled],.css-j7xjat[data-hover][data-disabled]{background:var(--chakra-colors-orange-200);}.css-j7xjat:active,.css-j7xjat[data-active]{background:var(--chakra-colors-orange-400);}</style><button type="button" class="chakra-button css-j7xjat" aria-label="Toggle theme"><style data-emotion="css onkibi">.css-onkibi{width:1em;height:1em;display:inline-block;line-height:1em;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:currentColor;vertical-align:middle;}</style><svg viewBox="0 0 24 24" focusable="false" class="chakra-icon css-onkibi" aria-hidden="true"><g stroke-linejoin="round" stroke-linecap="round" stroke-width="2" fill="none" stroke="currentColor"><circle cx="12" cy="12" r="5"></circle><path d="M12 1v2"></path><path d="M12 21v2"></path><path d="M4.22 4.22l1.42 1.42"></path><path d="M18.36 18.36l1.42 1.42"></path><path d="M1 12h2"></path><path d="M21 12h2"></path><path d="M4.22 19.78l1.42-1.42"></path><path d="M18.36 5.64l1.42-1.42"></path></g></svg></button></div><style data-emotion="css 1q8cyil">.css-1q8cyil{margin-left:var(--chakra-space-2);display:inline-block;}@media screen and (min-width: 48em){.css-1q8cyil{display:none;}}</style><div class="css-1q8cyil"><style data-emotion="css 1wjcgnw">.css-1wjcgnw{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:relative;white-space:nowrap;vertical-align:middle;outline:2px solid transparent;outline-offset:2px;line-height:1.2;border-radius:var(--chakra-radii-md);font-weight:var(--chakra-fontWeights-semibold);transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);height:var(--chakra-sizes-10);min-width:var(--chakra-sizes-10);font-size:var(--chakra-fontSizes-md);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);border:1px solid;border-color:var(--chakra-colors-whiteAlpha-300);color:var(--chakra-colors-whiteAlpha-900);padding:0px;}.css-1wjcgnw:focus-visible,.css-1wjcgnw[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}.css-1wjcgnw:disabled,.css-1wjcgnw[disabled],.css-1wjcgnw[aria-disabled=true],.css-1wjcgnw[data-disabled]{opacity:0.4;cursor:not-allowed;box-shadow:var(--chakra-shadows-none);}.css-1wjcgnw:hover,.css-1wjcgnw[data-hover]{background:var(--chakra-colors-whiteAlpha-200);}.css-1wjcgnw:hover:disabled,.css-1wjcgnw[data-hover]:disabled,.css-1wjcgnw:hover[disabled],.css-1wjcgnw[data-hover][disabled],.css-1wjcgnw:hover[aria-disabled=true],.css-1wjcgnw[data-hover][aria-disabled=true],.css-1wjcgnw:hover[data-disabled],.css-1wjcgnw[data-hover][data-disabled]{background:initial;}.chakra-button__group[data-attached][data-orientation=horizontal]>.css-1wjcgnw:not(:last-of-type){-webkit-margin-end:-1px;margin-inline-end:-1px;}.chakra-button__group[data-attached][data-orientation=vertical]>.css-1wjcgnw:not(:last-of-type){margin-bottom:-1px;}.css-1wjcgnw:active,.css-1wjcgnw[data-active]{background:var(--chakra-colors-whiteAlpha-300);}</style><button type="button" class="chakra-button chakra-menu__menu-button css-1wjcgnw" aria-label="Options" id="menu-button-navbar-menu" aria-expanded="false" aria-haspopup="menu" aria-controls="menu-list-navbar-menu"><svg viewBox="0 0 24 24" focusable="false" class="chakra-icon css-onkibi" aria-hidden="true"><path fill="currentColor" d="M 3 5 A 1.0001 1.0001 0 1 0 3 7 L 21 7 A 1.0001 1.0001 0 1 0 21 5 L 3 5 z M 3 11 A 1.0001 1.0001 0 1 0 3 13 L 21 13 A 1.0001 1.0001 0 1 0 21 11 L 3 11 z M 3 17 A 1.0001 1.0001 0 1 0 3 19 L 21 19 A 1.0001 1.0001 0 1 0 21 17 L 3 17 z"></path></svg></button><style data-emotion="css r6z5ec">.css-r6z5ec{z-index:1;}</style><div style="visibility:hidden;position:absolute;min-width:max-content;inset:0 auto auto 0" class="css-r6z5ec"><style data-emotion="css 1kfu8nn">.css-1kfu8nn{outline:2px solid transparent;outline-offset:2px;--menu-bg:#fff;--menu-shadow:var(--chakra-shadows-sm);color:inherit;min-width:var(--chakra-sizes-3xs);padding-top:var(--chakra-space-2);padding-bottom:var(--chakra-space-2);z-index:1;border-radius:var(--chakra-radii-md);border-width:1px;background:var(--menu-bg);box-shadow:var(--menu-shadow);}.chakra-ui-dark .css-1kfu8nn:not([data-theme]),[data-theme=dark] .css-1kfu8nn:not([data-theme]),.css-1kfu8nn[data-theme=dark]{--menu-bg:var(--chakra-colors-gray-700);--menu-shadow:var(--chakra-shadows-dark-lg);}</style><div class="chakra-menu__menu-list css-1kfu8nn" tabindex="-1" role="menu" id="menu-list-navbar-menu" aria-orientation="vertical" style="transform-origin:var(--popper-transform-origin);opacity:0;visibility:hidden;transform:scale(0.8)"></div></div></div></div></div></nav><style data-emotion="css 11nbm5x">.css-11nbm5x{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);max-width:var(--chakra-sizes-container-md);padding-top:var(--chakra-space-14);}</style><div class="chakra-container css-11nbm5x"><article style="position:relative;opacity:0;will-change:opacity,transform;transform:translateX(0px) translateY(20px)"><style data-emotion="css vhc7ah">.css-vhc7ah{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-prose);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);}</style><div class="chakra-container css-vhc7ah"><style data-emotion="css l8wsnv">.css-l8wsnv{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);line-height:1.33;font-size:20px;margin-bottom:var(--chakra-space-4);margin-top:var(--chakra-space-5);}@media screen and (min-width: 48em){.css-l8wsnv{line-height:1.2;}}</style><h3 class="chakra-heading css-l8wsnv">Posts</h3><style data-emotion="css cmsmo4">.css-cmsmo4{margin-bottom:var(--chakra-space-6);}.css-cmsmo4 transition{duration:0.8px;delay:0.2px;}</style><div class="css-cmsmo4" style="opacity:0;will-change:transform,opacity;transform:translateY(10px)"><style data-emotion="css dxssqe">.css-dxssqe{margin-top:var(--chakra-space-4);margin-bottom:var(--chakra-space-4);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><div class="css-dxssqe"><style data-emotion="css a838nk">.css-a838nk{font-size:var(--chakra-fontSizes-sm);color:var(--chakra-colors-gray-500);display:inline;margin-right:var(--chakra-space-3);min-width:100px;}</style><p class="chakra-text css-a838nk">Aug 7, 2025</p><style data-emotion="css 1qdxmbw">.css-1qdxmbw{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:#ff63c3;text-underline-offset:3px;}.css-1qdxmbw:hover,.css-1qdxmbw[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-1qdxmbw:focus-visible,.css-1qdxmbw[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-1qdxmbw" href="/blog/09_transfomers_4">Building GPT from Scratch - Part 4: Further Optimizations</a></div></div><style data-emotion="css 1lk3n2t">.css-1lk3n2t{margin-bottom:var(--chakra-space-6);}.css-1lk3n2t transition{duration:0.8px;delay:0.30000000000000004px;}</style><div class="css-1lk3n2t" style="opacity:0;will-change:transform,opacity;transform:translateY(10px)"><div class="css-dxssqe"><p class="chakra-text css-a838nk">Aug 6, 2025</p><a class="chakra-link css-1qdxmbw" href="/blog/08_transfomers_3">Building GPT from Scratch - Part 3: Building the Complete Transformer</a></div></div><style data-emotion="css y49q7a">.css-y49q7a{margin-bottom:var(--chakra-space-6);}.css-y49q7a transition{duration:0.8px;delay:0.4px;}</style><div class="css-y49q7a" style="opacity:0;will-change:transform,opacity;transform:translateY(10px)"><div class="css-dxssqe"><p class="chakra-text css-a838nk">Aug 5, 2025</p><a class="chakra-link css-1qdxmbw" href="/blog/07_transformers_2">Building GPT from Scratch - Part 2: The Attention Mechanism</a></div></div><style data-emotion="css 6di3z2">.css-6di3z2{margin-bottom:var(--chakra-space-6);}.css-6di3z2 transition{duration:0.8px;delay:0.5px;}</style><div class="css-6di3z2" style="opacity:0;will-change:transform,opacity;transform:translateY(10px)"><div class="css-dxssqe"><p class="chakra-text css-a838nk">Aug 4, 2025</p><a class="chakra-link css-1qdxmbw" href="/blog/06_transformers_1">Building GPT from Scratch - Part 1: Data Loading and the Bigram Model</a></div></div><style data-emotion="css ygkgw0">.css-ygkgw0{margin-bottom:var(--chakra-space-6);}.css-ygkgw0 transition{duration:0.8px;delay:0.6px;}</style><div class="css-ygkgw0" style="opacity:0;will-change:transform,opacity;transform:translateY(10px)"><div class="css-dxssqe"><p class="chakra-text css-a838nk">Jul 11, 2025</p><a class="chakra-link css-1qdxmbw" href="/blog/05_mech_interp">Mech Interp: Toy Models Part I</a></div></div><style data-emotion="css mcpxch">.css-mcpxch{margin-bottom:var(--chakra-space-6);}.css-mcpxch transition{duration:0.8px;delay:0.7000000000000001px;}</style><div class="css-mcpxch" style="opacity:0;will-change:transform,opacity;transform:translateY(10px)"><div class="css-dxssqe"><p class="chakra-text css-a838nk">Jul 10, 2025</p><a class="chakra-link css-1qdxmbw" href="/blog/04_mech_interp">Mech Interp: Paradigms</a></div></div><style data-emotion="css nx9hms">.css-nx9hms{margin-bottom:var(--chakra-space-6);}.css-nx9hms transition{duration:0.8px;delay:0.8px;}</style><div class="css-nx9hms" style="opacity:0;will-change:transform,opacity;transform:translateY(10px)"><div class="css-dxssqe"><p class="chakra-text css-a838nk">Jul 9, 2025</p><a class="chakra-link css-1qdxmbw" href="/blog/03_mech_interp">Mech Interp Day 0: Motivations</a></div></div><style data-emotion="css cubaqk">.css-cubaqk{margin-bottom:var(--chakra-space-6);}.css-cubaqk transition{duration:0.8px;delay:0.9px;}</style><div class="css-cubaqk" style="opacity:0;will-change:transform,opacity;transform:translateY(10px)"><div class="css-dxssqe"><p class="chakra-text css-a838nk">Jul 8, 2025</p><a class="chakra-link css-1qdxmbw" href="/blog/02_animation">Building Interactive Web Experiences</a></div></div><style data-emotion="css gfmfmw">.css-gfmfmw{margin-bottom:var(--chakra-space-6);}.css-gfmfmw transition{duration:0.8px;delay:1px;}</style><div class="css-gfmfmw" style="opacity:0;will-change:transform,opacity;transform:translateY(10px)"><div class="css-dxssqe"><p class="chakra-text css-a838nk">Jul 7, 2025</p><a class="chakra-link css-1qdxmbw" href="/blog/01_implementation">Blog Implementation Summary</a></div></div><style data-emotion="css 1fn0d3n">.css-1fn0d3n{margin-bottom:var(--chakra-space-6);}.css-1fn0d3n transition{duration:0.8px;delay:1.1px;}</style><div class="css-1fn0d3n" style="opacity:0;will-change:transform,opacity;transform:translateY(10px)"><div class="css-dxssqe"><p class="chakra-text css-a838nk">Jul 6, 2025</p><a class="chakra-link css-1qdxmbw" href="/blog/00_hello_world">Hello World</a></div></div></div><style data-emotion="css-global aaigsc">.grid-item-thumbnail{border-radius:12px;}</style></article><style data-emotion="css 17cqjj4">.css-17cqjj4{opacity:0.4;font-size:var(--chakra-fontSizes-sm);}</style><div align="center" class="css-17cqjj4">Build based on <a href="https://www.craftz.dog/" target="_blank">Takuya Matsuyama</a>.</div></div></main><span></span><span id="__chakra_env" hidden=""></span></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"slug":"09_transfomers_4","content":"*This is Part 4 of our GPT from scratch series. Having built a complete transformer in Parts 1-3, we now look at further optimizations for more efficient training.*\n\n## Restructuring and Configuration\n\nNow that we have successfully implemented a transformer network, we can make a number of adjustments to optimize for more efficient training.\n\nFirst, we restructure our script and put all hyperparameters into a config class:\n\n```python\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'using device: {DEVICE}')\n\nconfig = Config(\n  batch_size = 64,\n  block_size = 64,\n  max_iters = 5000,\n  eval_interval = 500,\n  learning_rate = 3e-4,\n  eval_iters = 10,\n  n_embd = 128,\n  n_head = 4,\n  n_layer = 6,\n  dropout = 0.2,\n  vocab_size = vocab_size,\n)\n\nmodel = GPTLanguageModel(config, device=DEVICE)\nmodel.to(DEVICE)\nmodel = torch.compile(model) # requires PyTorch 2.0\n```\n\nAs you can see, we also compile the model before training to improve training speed.\n\n## Efficient Multi-Head Attention\n\nThe next big change we make is to compute the keys, queries, and values for the multiple heads all in a single matrix multiplication. This means we combine the 'Head' and 'Multi-Head' classes into one single class, which processes the heads as a new batch dimension.\n\n```python\nclass MultiHeadAttention(nn.Module):\n  \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n  def __init__(self, config):\n    super().__init__()\n    self.n_head = config.n_head\n    self.n_embd = config.n_embd\n    self.dropout = config.dropout\n\n    # key, query, value projections for all heads, but in a batch\n    self.attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n    self.proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n    self.attn_dropout = nn.Dropout(config.dropout)\n    self.resid_dropout = nn.Dropout(config.dropout)\n    self.register_buffer(\"bias\",\n      torch.tril(torch.ones(1, 1, config.block_size, config.block_size)))\n\n  def forward(self, x):\n    B, T, C = x.size()\n    q, k, v  = self.attn(x).split(self.n_embd, dim=2)\n    # reshape and move head dimension forward using einops\n    k = rearrange(k, 'b t (nh hs) -\u003e b nh t hs', nh=self.n_head)\n    q = rearrange(q, 'b t (nh hs) -\u003e b nh t hs', nh=self.n_head)\n    v = rearrange(v, 'b t (nh hs) -\u003e b nh t hs', nh=self.n_head)\n\n    att = (q @ k.transpose(-2, -1)) * k.size(-1)**-0.5\n    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n    att = F.softmax(att, dim=-1)\n    att = self.attn_dropout(att)\n    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -\u003e (B, nh, T, hs)\n\n    # re-assemble all head outputs side by side\n    y = rearrange(y, 'b nh t hs -\u003e b t (nh hs)')\n    y = self.resid_dropout(self.proj(y))\n    return y\n```\n\n## Flash Attention\n\nFinally, we implement flash attention, which reorders the attention computation such that computations can be tiled which greatly speeds up the attention step.\n\n```python\n# In MultiHeadAttention class\nself.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n# ... in forward pass ...\ny = torch.nn.functional.scaled_dot_product_attention(q, k, v, \n                                                     attn_mask=None, \n                                                     dropout_p=dropout_p,\n                                                     is_causal=True)\n```\n\n## Training Performance and Scaling\n\nTraining on a T4 GPU, a 1.2M parameters model now trains in 2 minutes. We are ready to scale up to the size shown in the tutorial video:\n\n```python\nconfig = Config(\n  batch_size = 64,\n  block_size = 256,\n  max_iters = 5000,\n  eval_interval = 500,\n  learning_rate = 3e-4,\n  eval_iters = 10,\n  n_embd = 384,\n  n_head = 6,\n  n_layer = 6,\n  dropout = 0.2,\n  vocab_size = vocab_size,\n)\n```\n\nThis has 10 million parameters and trains in 30 minutes! Let's take a look at the output:\n\n```\nBRAKENBURY:\nDespite of this seven sit in the noble\nOf Lord Hastings, and my grave is my charged mine;\nFor George shall not speak not pass'd it:\nThe valour upon it. Is deliver'd it with me?\n\nBRUTUS:\nYea, beggar, by your voices and hearts,\nBut since she changed in your packs and bloody,\nYour joy your might in him writ\nBe punk'd between pains. I am struckcomment\nTo son I writ you that yet you did love:\nIf it were example to your knees to express\nBy your fleships of state? Exeter, me\nAnd wring, my son, come on, my sorrow ladys,\nWhose profession joy wings and me down,\n```\n\nPretty good! Most words are recognizable and the sentences have more structure. The overall form is also consistent with the training data with readable names (Brutus is a familiar one!)\n\n## Bias-Variance Trade-off\nIf we plot the losses against training steps for our small 0.3M model and our final 10M model:\n![Validation loss comparison](/images/training_validation_comparison.png)\nThe larger model generalizes better (has a lower validation loss) but overfits more (training loss is much lower than validation loss). This represents a classic **bias-variance trade-off** in neural scaling. The larger model's increased parameter capacity enables better feature representation learning and pattern recognition, resulting in lower validation error. However, this same capacity creates **memorization potential** for training data.\n\n## Conclusion\n\nThese optimizations demonstrate how production transformer implementations differ from educational versions. The key improvements - configuration management, batched attention computation, model compilation, and flash attention - provide significant training speedups while maintaining the same underlying architecture.\n\nThe scaling results show that our simple transformer can achieve reasonable quality when given sufficient parameters and training time, following the same principles that power modern large language models.","title":"Building GPT from Scratch - Part 4: Further Optimizations","date":"2025-08-07","excerpt":"","tags":["transformers","tutorial"]},{"slug":"08_transfomers_3","content":"*This is Part 3 of a 3-part series on building a Generative Pretrained Transformer from scratch. In Parts 1 and 2, we built the data pipeline and attention mechanism. Now we'll complete our transformer with the remaining key components.*\n\n## Feed-Forward Networks\n\nAfter our attention mechanism learns to communicate between tokens, we need to give the model computation time to process what it has learned. This is where feed-forward networks come in - they're simply MLPs with ReLU activation that allow the model to \"think\" about the information it just gathered.\n\n```python\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n\nWe add this to our model after the attention:\n\n```python\n# In our model:\nself.sa_head = MultiHeadAttention(4, n_embd//4)\nself.ffwd = FeedFoward(n_embd) # MLP with RELU\n\n# In forward pass:\nx = self.sa_head(x)\nx = self.ffwd(x)\nlogits = self.lm_head(x)\n```\n\n## Transformer Blocks: Communication + Computation\n\nThe pattern of attention (communication) followed by feed-forward (computation) is so fundamental that we package it into reusable **Transformer Blocks**. These blocks can be stacked to create deeper networks that alternate between communication and computation phases.\n\n```python\nn_head = 4\nn_layer = 2\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n```\n\nWe can then stack multiple blocks:\n\n```python\n# In our model:\nself.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n\n# In forward pass:\nx = tok_emb + pos_emb # (B,T,C)\nx = self.blocks(x)\nlogits = self.lm_head(x)\n```\n\nAt this point, our model starts generating recognizable English words!\n\n```\nRILELE\nMAssell use you.\nNt I RYan lake off.\n\nFicink'd\nhote carewtledeche quie to whanl Gatt Mejesery ely:\nThe if, bet leveses it theave be ry skit you file.\n\nKay wred tome dake stance, suks,\nAdech JORo!\n\nALOUSBET:\nWis brake grourst and ald creapsss,\nAndite\nnoat,\nAmothery are doreast is\n```\n\n## Training Optimizations\n\nAs our network gets deeper with multiple transformer blocks, we need optimizations to improve training stability and performance.\n\n### Residual Connections\n\nThe first optimization is **residual/skip connections**, introduced in the [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) paper. These connections allow gradients to flow directly through the network, avoiding training bottlenecks in deep networks.\n\nInstead of 'x = self.sa(x)', we do 'x = x + self.sa(x)'. The computation now returns a residual that gets added to the original input:\n\n```python\nclass Block(nn.Module):\n    def forward(self, x):\n        x = x + self.sa(x)  # residual connection\n        x = x + self.ffwd(x)  # residual connection\n        return x\n```\n\n### Projection Layers\n\nWe also add projection layers to map back to the embedding space after our multi-head attention and expand our feed-forward networks as in the original paper:\n\n```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),  # expand by 4x as in the paper\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),  # project back down\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n\n### Layer Normalization\n\nLayer normalization helps stabilize training by normalizing inputs to have zero mean and unit variance. We apply it before both the self-attention and feed-forward computations in each block, and once more after all blocks:\n\n```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n```\n\n### Dropout\n\nFinally, we add **dropout** for regularization, which randomly sets some activations to zero during training to prevent overfitting:\n\n```python\nclass FeedFoward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),  # add dropout\n        )\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n```\n\n## Final Model and Hyperparameters\n\nWe now have a complete decoder-only transformer ready to scale and train! Here are the hyperparameters for our final model:\n\n```python\n# hyperparameters\nbatch_size = 32\nblock_size = 32\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\neval_iters = 200\nn_embd = 64\nn_head = 6\nn_layer = 6\ndropout = 0.2\n```\n\n## Final Results\n\nAfter training our complete transformer, we get much more structured output:\n\n```\nDOF GlLN:\nThey good, then then, bladgy bone not thindnes\nI way Jeain, fainly!\n\nISABELLA:\nI way, thou fourd to havary flown dews'm-sine.\n\nNowZALLET:\nWhich here old thy's warring hiod\nOn dearys hory be wive to more; greseli!\n\nBut nighd Wart, prance:\nBarch,\nAnd prayem not welld she you, coldinger:\nO I the oldst God somed:\nSirry is let never To be whith, I new'n be thy limpiny\nWord: where deblitiss for give upon the conqueennifult,\nAnd so pobeterl. by they she thy truge,\nIf you his let a brotess.\n```\n\nWhile still not perfect, the improvement is dramatic! The model now:\n- Uses proper character names (ISABELLA, etc.)\n- Maintains somewhat consistent dramatic structure\n- Shows more coherent word formation\n- Demonstrates longer-range dependencies\n\n## What We've Accomplished\n\nIn this three-part series, we've built a complete transformer from scratch:\n\n1. **Part 1**: Data loading, tokenization, and a simple bigram baseline\n2. **Part 2**: The attention mechanism - the core innovation of transformers\n3. **Part 3**: Complete transformer architecture with all the essential components\n\n### Key Components We've Implemented:\n\n- **Self-attention mechanisms** for learning relationships between tokens\n- **Multi-head attention** for capturing different types of relationships\n- **Feed-forward networks** for computation after communication\n- **Transformer blocks** that stack attention and computation\n- **Residual connections** for stable training of deep networks\n- **Layer normalization** for training stability\n- **Positional embeddings** so the model understands sequence order\n- **Dropout** for regularization and preventing overfitting\n\n## The Path Forward\n\nOur small transformer demonstrates the core principles, but to achieve GPT-level performance, you'd need to scale up significantly:\n\n- **More parameters**: Modern language models have billions of parameters\n- **More data**: Training on much larger text corpora\n- **More compute**: Training for weeks or months on powerful hardware\n- **Better tokenization**: Using subword tokenizers like BPE or SentencePiece\n- **Longer context windows**: Supporting much longer sequences\n\n## Key Takeaways\n\n1. **Attention is the core innovation**: The self-attention mechanism allows tokens to communicate and share information based on content rather than just position.\n\n2. **Transformers are surprisingly simple**: The architecture is just attention + feed-forward blocks stacked together with some normalization and residual connections.\n\n3. **Scale matters**: The same architecture that gives us semi-coherent Shakespeare can generate human-level text when scaled up with more parameters, data, and compute.\n\n4. **Training stability is crucial**: Residual connections, layer normalization, and proper weight initialization are essential for training deep networks.\n\n## Conclusion\n\nThis tutorial has shown how to build a transformer model from scratch using PyTorch. We've implemented all the key components of the decoder transformer architecture and seen how they work together to create a language model capable of generating structured text.\n\nThe progression from random gibberish (bigram model) to semi-coherent Shakespeare-like text (full transformer) demonstrates the power of the attention mechanism and proper architectural choices. While our small model doesn't generate fully coherent text yet, scaling up the parameters, training data, and computation would lead to increasingly capable language models.\n\nThe complete nanoGPT implementation, with additional optimizations and the ability to train larger models, can be found in [Andrej Karpathy's repository](https://github.com/karpathy/nanoGPT). This serves as an excellent foundation for understanding and experimenting with transformer architectures.","title":"Building GPT from Scratch - Part 3: Building the Complete Transformer","date":"2025-08-06","excerpt":"","tags":["transformers","tutorial"]},{"slug":"07_transformers_2","content":"*This is Part 2 of a 3-part series on building a Generative Pretrained Transformer from scratch. In Part 1, we built a simple bigram model. Now we'll introduce the core innovation that makes transformers work: Attention.*\n\n## The Mathematical Foundation: Weighted Aggregation\n\nBefore diving into attention, let's understand a neat mathematical trick that forms its foundation. By multiplying a row-normalized T×T lower triangular matrix with a T×C matrix, we can get a T×C matrix which calculates a running average of the preceding rows.\n\nWhen we combine this with softmax (setting -inf instead of 0), we get a more general weighted aggregation of past elements:\n\n```python\n# Weighted aggregation with softmax\ntorch.manual_seed(1337)\nB,T,C = 4,8,2\nx = torch.randn(B,T,C)\n\ntril = torch.tril(torch.ones(T,T)) # lower triangular\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril==0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nres = wei @ x\nres.shape\n```\n\nThis is the mathematical foundation of attention - we're learning to compute weighted averages of past tokens, where the weights are determined by the content of the tokens themselves.\n\n## Single Head Self-Attention\n\nNow for the meat of transformers: the attention mechanism. We create an attention space of dimension 'head_size'. We project each token into this space using three matrices: **Key (K)**, **Query (Q)**, and **Value (V)**.\n\nHere's the intuition:\n- **Query (Q)**: What the token is looking for\n- **Key (K)**: What the token contains/offers\n- **Value (V)**: What the token communicates if paid attention to\n\nWhen K and Q are similar (measured using dot product), they have high affinity and receive higher attention weights. We don't aggregate tokens directly - we aggregate V(X), which represents what each token communicates when attended to.\n\nThe normalization by 1/√(head_size) is crucial. When Q and K have unit variance, this ensures the attention weights also have unit variance. Without this normalization, softmax would saturate towards one-hot vectors, meaning tokens would only aggregate information from a single previous token.\n\n```python\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ rearrange(k, 'b t h -\u003e b h t') * C**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -\u003e (B, T, hs)\n        return out\n```\n\nNote that we register 'tril' as a buffer since it's not a learnable parameter of the model.\n\n## Important Notes About Attention\n\nHere are some crucial points about how attention works:\n\n- **Note 1**: Attention is a communication mechanism. You have a directed graph and you want every node to take weighted sums of every node pointing to it.\n\n- **Note 2**: Positional encoding must be embedded in the nodes, since the structure of the attention mechanism doesn't inherently understand position.\n\n- **Note 3**: Batches are independent - there's no cross-communication between different sequences in a batch.\n\n- **Note 4**: Here we ignore future tokens using triangular masking. If you were doing sentiment analysis, you might want to allow this communication. Encoder blocks would allow bidirectional attention, decoder blocks would not. The difference is just the triangular masking.\n\n- **Note 5**: This is called \"self-attention\" because K, Q, V all come from the same input X. If the keys and values came from somewhere else, you'd have \"cross-attention\", which is used when there's a separate source of information to pull from.\n\n## Building the Complete Single-Head Model\n\nLet's put everything together into a complete model with single-head attention. We'll make several improvements to our training setup:\n\n```python\n# hyperparameters\nbatch_size = 32\nblock_size = 8 \nmax_iters = 5000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32\ntorch.manual_seed(1337)\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n```\n\nKey improvements in this version:\n\n1. **GPU Support**: We move data and model to GPU using 'device = 'cuda''\n2. **Better Loss Estimation**: 'estimate_loss()' averages losses over multiple batches for more accurate estimates\n3. **No Gradient Context**: '@torch.no_grad()' tells PyTorch this function won't be backpropagated through\n4. **Embedding Dimension**: Our logits are now embedded in an embedding dimension, so we need an \"unembedding\" linear layer\n5. **Positional Embeddings**: We add position information so tokens know where they are in the sequence\n\n```python\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_head = Head(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.sa_head(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```\n\n## Results with Single-Head Attention\n\nAfter training, we get improved results that show more structure, though still not quite Shakespeare:\n\n```\nArrg hor; ho, ho aak.\n\nLUCKUCARE Fqhelree ndel te:\nThece waplt ko.\nFOCI\nMHABRIEKY tur waverer orid betievis dyof b.\n\nBo,\nABIUCE:\nN-, evesune athid nt cobasr!, Go hern, alsemin rsin varit ther I;\nANher en:ouingaroua lis py Bh mithe ast prird band bad youun theis pioat hed man ile ere sty hanonoue avillars d ty hon I:\nSo Ilar chy w'guma my tono yat d,\nI more hathive, ha utithe baag tee\nIile athadog at y hoke hay hate whinsintoourtarenol usdon co whe sou; mer wif yoh hlele coud illesen?o I choteban\n```\n\nThe attention mechanism has given our model the ability to look back at previous characters and make more informed predictions. We can see more English-like structure emerging.\n\n## Multi-Head Attention\n\nInstead of using a single attention head, we can use multiple smaller heads in parallel. This allows the model to attend to different types of relationships simultaneously - some heads might focus on syntax, others on semantics, etc.\n\n```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n```\n\nIn our model, we replace the single head with:\n```python\nself.sa_head = MultiHeadAttention(4, n_embd//4)\n```\n\nThis gives us slightly better loss and more diverse attention patterns.\n\n## What's Next?\n\nWe now have a working attention mechanism! However, our model is still missing several key components that make modern transformers work well:\n\n- **Feed-forward networks** for computation after attention\n- **Transformer blocks** that repeat the attention + computation pattern\n- **Residual connections** and **layer normalization** for training stability\n- **Dropout** for regularization\n\nIn Part 3, we'll add these final components and see our model start generating much more coherent text. The attention mechanism we've built here is the foundation - everything else builds on top of it to create the full transformer architecture.","title":"Building GPT from Scratch - Part 2: The Attention Mechanism","date":"2025-08-05","excerpt":"","tags":["transformers","tutorial"]},{"slug":"06_transformers_1","content":"\n*This is Part 1 of a 3-part series on building a Generative Pretrained Transformer from scratch, based on Andrej Karpathy's excellent tutorial.*\n\n## Introduction\n\nThe goal of this series is to build a 'Generative Pretrained Transformer' from scratch, following the attention architecture from the original 2017 [**Attention is all you need**](https://arxiv.org/abs/1706.03762) paper and OpenAI's GPT-2. We'll be building a character-level generative model - essentially a next character predictor that can generate Shakespeare-like text.\n\nThis tutorial is based on Andrej Karpathy's video [**Let's build GPT: from scratch, in code, spelled out**](https://www.youtube.com/watch?v=kCc8FmEb1nY\u0026t=10s), which has gained 6 million views in 2 years! Transformers get their name from their original use case of machine translation.\n\n## Data Loading\n\nThe first step in building any language model is preparing our data. We'll use the tiny Shakespeare dataset for this tutorial.\n\n```python\n# Download tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# read and inspect the file\nwith open('input.txt', 'r') as f:\n  text = f.read()\nprint(\"Length of dataset in characters: \", len(text))\nprint(text[:1000])\n```\n\n### Tokenization\n\nThe first interesting concept is **tokenization**. We want to convert our input text into integers that our model can work with. The model will predict the next integer, which we can then decode back into a character.\n\nOur simple approach creates a dictionary of all available characters (including newline) and assigns each an integer. For more sophisticated treatments, you'd want to look at SentencePiece by Google or tiktoken by OpenAI. There's always a tradeoff between dictionary size and encoding lengths.\n\n```python\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(f\"{vocab_size} characters in dictionary : {''.join(chars)}\")\n# First one is a new line!\n\n# Create a mapping from characters to integers (A Tokenizer)\n# This is called tokenizing. Google uses SentencePiece. OpenAI and GPT uses tiktoken\nitos = { i:ch for i, ch in enumerate(chars)}\nstoi = { ch:i for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s] # encoder: convert string to list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: convert list of integers to string\n\n# Now encode the whole text dataset and store it as a pytorch Tensor\nimport torch\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:100])\nprint(decode([int(v) for v in (data[:100].data)]))\n\n# Split the data into training and validation sets\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n```\n\n### Block Size and Context Windows\n\nThe second important concept is **block size**, which sets the context window length. This is the size of the 'memory' of the model. A larger block size would give more long-range associations, but is probably more expensive to train and would require more data.\n\nWith block size defined, we can input batch_size rows of block_size integers for each data batch. Training in batches is faster since multiple independent blocks can be processed simultaneously.\n\n```python\nblock_size = 8 # or alternatively context lengths\ntrain_data[:block_size+1]\n# in a sample of 9 characters, we have 8 examples with different context lengths.\n\ntorch.manual_seed(1337)\nbatch_size = 4\nblock_size = 8\n\ndef get_batch(split):\n  # generate a small batch of data of inputs x and targets y\n  data = train_data if split == 'train' else val_data\n  ix = torch.randint(len(data) - block_size , (batch_size,)) # should have a -1\n  x = torch.stack([data[i:i+block_size] for i in ix])\n  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n  return x, y\n```\n\n## The Bigram Model\n\nNow let's implement our first model - a simple bigram model. This model predicts the next character based only on the current character (context length of 1) and stores probabilities for the next character.\n\nWe use a log-likelihood/cross-entropy loss. Initially, we expect the loss to be around ln(1/65) ≈ 4.17, since we have 65 characters and initially each character should be equally likely.\n\n```python\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\nfrom einops import rearrange\n\nclass BigramLanguageModel(nn.Module):\n\n  def __init__(self, vocab_size):\n    super().__init__() # initialize the parent class Module\n    # each token directly reads off the logits for the next token from a lookup table\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n  def forward(self, idx, targets=None):\n    # idx and targets and both (B,T) tensors of integers\n    logits = self.token_embedding_table(idx) # (B,T,C) batch time channel\n\n    if targets is None:\n      loss = None\n    else:\n      logits = rearrange(logits,'b t c -\u003e (b t) c')\n      targets = rearrange(targets, 'b t -\u003e (b t)')\n      loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\n  def generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indicies in the current context\n    for _ in range(max_new_tokens):\n      # get the predictions\n      logits, loss = self(idx)\n      # focus only on last time step\n      logits = logits[:, -1 , :] # becomes B x C\n      # apply softmax to get probabilties\n      probs = F.softmax(logits, dim=1) # becomes B x C\n      # sample from distribution\n      idx_next = torch.multinomial(probs, num_samples=1) # B x 1\n      # append sampled index to running sequence\n      idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1,1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```\n\nNote that for PyTorch, when we subclass 'nn.Module', calling the model (including 'self') actually calls the 'forward' method!\n\n## Training the Bigram Model\n\nWe can train our model by choosing an **optimizer** and a **learning rate**. A good learning rate is typically 1e-3, but for smaller networks you can get away with higher rates.\n\n```python\n# training\nbatch_size = 32\n\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nfor steps in range(10000):\n  # sample a batch of data\n  xb, yb = get_batch('train')\n\n  logits, loss = m(xb, yb)\n  optimizer.zero_grad(set_to_none=True)\n  loss.backward()\n  optimizer.step()\n\nprint(loss.item())\n```\n\n## Results\n\nUnfortunately, the results using the bigram model aren't great:\n\n```\nIyoteng h hasbe pave pirance\nRie hicomyonthar's\nPlinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\nKIN d pe wither vouprrouthercc.\nhathe; d!\nMy hind tt hinig t ouchos tes; st yo\n# Not quite Shakespeare\n```\n\nThe bigram model is too simple - it can only look at one character back to predict the next. We need something more sophisticated that can consider longer context windows and learn more complex patterns.\n\n## What's Next?\n\nIn the next part of this series, we'll introduce the key mathematical trick that makes transformers possible: the attention mechanism. We'll start with a mathematical foundation for weighted aggregation and build up to single-head self-attention.\n\nThe bigram model gives us a solid foundation to build upon, and more importantly, it establishes our training pipeline and data handling. Now we're ready to dive into the real magic of transformers!","title":"Building GPT from Scratch - Part 1: Data Loading and the Bigram Model","date":"2025-08-04","excerpt":"","tags":["transformers","tutorial"]},{"slug":"05_mech_interp","content":"\nThis is a pretty long paper, and is divided into 12 sections (although the last 3 are discussion, related work and comments).\n\n## Section 1: Background \u0026 Motivation\n\nThe first section provides an introduction and summary of the main results in this paper. They begin by mentioning that it would be nice if each neuron had a one to one correspondence with a specific feature such as dog’s snout. This has been shown empirically to sometimes happen, but this is not always the case especially in LLMs. So the motivating question is when and why this happens (or doesn’t happen). By using toy models which are small ReLU networks trained to learn five features of varying importance and varying sparsity, they show that models can store more features than they have dimensions (**superposition**), and they can even perform computations in this superposed state. They hypothesize that the small networks simulate large sparse networks.\n\nThis seems like an interesting way to view superposition —- as a way of compressing information through continuous representation, but also as a computation mechanism (cue the name superposition and some possible relation to QM). When they say `no interference', this is what I think of as a circuit, with each neuron activating on, and only on, specific features.\n\nThey go on to say the main contribution is that they directly demonstrate superposition occurs relatively natural and offer a theory (phase diagram!) of when this happens. They make lots of parallels to physics phenomena, which I am excited to read given my physics background. There is an interesting note for example, that superposition organizes features into geometric structures, which immediately signals to be some kind of energy (loss function) minimizing geometries. They have not defined **sparsity** yet, although they discuss it, but I’m sure that is coming next. \n\n## Section 1b: Definitions and Motivation: Features, Directions, and Superposition\n\nThe next part of this section kicks off by discussing the Linear Representation Hypothesis ([see here](https://arxiv.org/abs/2311.03658)), which is the idea that certain directions in a representation space correspond to specific high level concepts, in other words that they carry semantic meaning. The claim that these are linear also seems important since that makes them easy to compose and decompose, and even to do vector arithmetic. This is more an empirical observation, e,g, see Mikolov et al for a famous example in word embeddings.\n\nIn their words, ‘features of the input’ are captured as ‘directions in activation space’. If we can figure out these vector representations, we can decompose activations into something we can more easily understand — as a basis of concepts each activated with a certain strength. \n\nOne natural basis in a neural network by virtue of its structure is having each neuron correspond to one feature. Naturally this would be limited to the number of neurons you have (This is a privileged basis, and there are incentives to used this basis such as activation functions, but this doesn’t mean that the basis will be used)! However, if you have more features than neutrons, a different strategy is required. This is where the idea of superposition comes in, where the features are captured by a certain direction in the high dimensional input space. I assume this is akin to creating a non-orthogonal basis. \n\nThere is some interesting discussion of what actually constitutes a feature, since `concept' is a pretty vague word. They settle on defining it as a property which will be an individual neuron in a sufficiently large network for the same problem. Does this mean that any neural network that exhibits superposition has an equivalent much larger neural network that works in the basis where each neuron is one feature? What implications does that have for discrete valued/low bit neural networks? What are the trade offs for being able to do this network compression?\n\nNext, a strong case is made for linear representations, of which I like the argument that linear representations make features linearly accessible, meaning that since neural network layers are linear functions followed by a non-linearity, a feature must be linear in order for a neuron to consistently excite or inhibit on that feature in a single step. \n\nLastly, there is more discussion on **the superposition hypothesis**, which I think is the core of this paper.  Basically, if a neural network needs to represent more features than there are neurons, features can’t align with the basis and hence polysemanticity (single neuron represents to multiple geatures) is unavoidable. While you can only have n orthogonal vectors in n dimensions, you can have an exponential exp(n) almost orthogonal vectors. So for large-n, this would be very efficient at cramming more features in than neurons! The cost is that each feature activating creates some small amount of noise by making it looks as some other features slightly activating. \n\nThe other big idea here is that sparsity is important. From compressed sensing, it is generally difficult to reconstruct a high dimensional vector projected into a low dimensional space since information is lost, but it is possible if you can assume the original vector is sparse. So sparsity here means that activations don’t happen together, and that means this `noise' is minimized. If the activations are not sparse, multiple present features will interfere.\n\nAgain the hypothesis is that small networks with polysemanticity are low dimensional projections of larger networks. The requirements for this to be effective seem pretty strict, so it would nice to be able to better quantify them and to understand how important this is in most networks.","title":"Mech Interp: Toy Models Part I","date":"2025-07-11","excerpt":"","tags":["mech interp"]},{"slug":"04_mech_interp","content":"\nWith so much to look at, it is difficult to decide where to start, but it is probably best to just start somewhere. To start off, I think I will read [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) (Elhage et al 2022). I remember really liking the findings and approach of the paper, since trying to understand simple toy models seems like a very intuitive approach to me. I have found this very useful even in my own research, where instead of directly trying to understand the results of large simulations with lots of moving pieces, we try to understand much simple toy model simulations to build a scaffold for understanding the complex behavior we observe. Since the work deals with toy models, I should also be able to play around and run some of these experiments fairly easily. This should also be a natural start to understanding sparse autoencoders (SAEs) since they seem to be the current main approach towards untangling this problem of superposition.\n\nBefore that I would like to read some resource that gives a higher level view of the history of the field and where it is going, to get a better sense of the bird’s eye view landscape. At the same time I am keeping a list of concepts I want to learn more about as I come across them, and hopefully this list does not get too long.\n\nI decided to begin by reading [this blog post](https://www.alignmentforum.org/posts/beREnXhBnzxbJtr8k/mech-interp-is-not-pre-paradigmatic) on the alignment forum by Lee Sharkey titled “Mech interp is not pre-paradigmatic”. In particular, since I am looking for some initial paradigm in which to frame mech interp research, the title of this post indicated it would present some form of such a framework. So as the title suggests, this post basically begins by arguing that the field of mech interp is **not** in what might be called a pre-paradigmatic phase, and instead is actually at the mature stage of a second mini-paradigmic wave. Pre-paradigmatic here refers to a stage in the development of a scientific field before it has established a dominant theoretical framework or \"paradigm.\" This concept comes from philosopher Thomas Kuhn's influential work \"The Structure of Scientific Revolutions”, where he outlines how a field progresses through these stages. While some argue mech interp, being a relatively new field, is in this pre-paradigmatic phase, this post pushes the idea that mech interp actually begun as an offshoot of computational neuroscience and hence inherits many of the ideas and concepts established there. In fact, the argument is made that it has been rediscovering many of the same ideas explored in neuroscience! I certainly lack sufficient knowledge in either area to judge this statement, but I don’t find this claim surprising given the parallels. Further, given the greater tractability of working with neural networks, perhaps progress might actually eventually flow in the other direction! I think the worry lies in the fact that neuroscience seems to be struggling to make recent progress and that mech interp might reach a similar wall. The article suggests the following **Three Waves of Mech Interp**:\n\n1. **First Wave** (2010s): Focused on demonstrating that interpretable structure exists in deep neural networks. 'Features are the fundamental unit of neural networks' and 'features are connected by weights, forming circuits' . This wave ended when researchers discovered polysemantic neurons (neurons that respond to multiple unrelated concepts).\n2. **Second Wave** (2022-present): Emerged after the \"Toy Models of Superposition\" paper, introducing sparse dictionary learning (SDLs) to address polysemanticity. However, this wave now faces its own anomalies.\n3. **Potential Third Wave?** The post suggests \"Parameter Decomposition\" as a promising approach that could resolve Second-Wave anomalies by decomposing neural network parameters into interpretable components representing computational mechanisms. Worth nothing that this is what the author is working on at present.\n\nAt the same time, I think the transition from first to second wave lines up with the rise of LLMs, since much early work was done with CNNs. Much larger LLMs clearly showed much richer representation and required understanding polysemanticity and superposition. As for parameter decomposition, I will have to come back to this again since I don’t understand enough to appreciate its arguments. This outline also makes me feel like the Toy Models paper is indeed the best place to start if it marks this big transition in the thinking in the field. I think it would be good for me to try and define mech interp, superposition and polysemancity, perhaps after going through the paper, and for example think about this particular quote from the post:\n\n\u003e The idea that 'networks represent more features than they have neurons'. It is a natural corollary of the superposition hypothesis that neurons would exhibit polysemanticity, since there cannot be a one-to-one relationship between neurons and 'features'.\n\nThe article also presents a list of anomalies in this Second Wave that I would like to return to once I have built more understanding, since these should be the open problems the field is looking at now. While this post is just one perspective, I think it is a sufficiently good mental framework to start with, with lots of references and ideas to think about. Alright, onwards to Toy Models!","title":"Mech Interp: Paradigms","date":"2025-07-10","excerpt":"","tags":["mech interp"]},{"slug":"03_mech_interp","content":"\n## Starting my Mech Interp Journey\n\nChange is never easy, and stepping away from astronomy after many years feels like giving up part of my identity. I don't regret my decision to pursue graduate school, those years have been nothing but fulfilling. It's a great privilege to dedicate time and effort to a pursuit of such intellectual purity. However, at the end of graduate school, one must once again cast thoughts toward the future.\nRemaining on the academic path brings uncertainty in many areas of life and often demands personal sacrifices: constant relocation, distance from loved ones, evolving responsibilities as teaching loads and funding pressures mount. Balancing these demands with other personal priorities is far from easy, and I found myself unable to envision navigating this path happily. I've also always been acutely aware that while astronomy is fascinating, there are infinite equally interesting areas of study and work—many of more immediate relevance to humanity.\nWhile changing trajectory has been difficult, deciding what to do next has been even more challenging. One area I'm strongly interested in pursuing is mechanistic interpretability.\n\n## The Rise of Neural Networks\n\nOver the years, I've observed the explosion of machine learning, or more accurately, the rise of neural networks, deep learning, and large language models as they evolved from esoteric topics to universal adoption, taking root in all aspects of society. The social implications are tremendous, and it really does appear to be a watershed moment in how humans interact with technology.\nYet so much remains mysterious about how these networks actually work. We train these systems on vast amounts of data, but their resultant capabilities have repeatedly exceeded expectations while theoretical understanding struggles to catch up. This reminds me of complex systems that display emergent behavior even with simple, local rule-based evolution. We know the full state of a trained neural network (every weight, bias, and computation that flows through it), but its overall capabilities still baffle us.\nInformation is being processed in ways that seem opaque to us, so a field has crystallized around making sense of this opacity in ways we can understand and interpret. This is mechanistic interpretability (or \"mech interp\"), the science of understanding how machine learning networks learn to process information, much like neuroscience tries to understand how the biological brain does something similar.\nWhile young, this field is moving at lightning speed. I strongly believe that progress in mech interp is among the most important research being done today, given the reach and rate of growth of this technology. Preventing these systems from pursuing unintended goals (known as AI alignment) surely requires developing an understanding of how these networks do what they do.\n\n## A New Experience\n\nWhile I've maintained interest in machine learning and neural networks over the years, my direct experience has been limited. The applications to my research were never convincing or promising enough (a lack of interpretability makes neural networks problematic for theoretical applications).\nRecently, however, I experimented with using Neural ODEs as a natural way of extending our usual process of modeling physical systems with differential equations through deep learning, while exploring symbolic regression to improve interpretability and generalizability. I also attended NeurIPS 2024 and got a feel for what the field was excited about, including in the context of scientific applications.\nIt's been over half a year since then, and many of the big ideas such as MCP, multimodal inputs, and agentic AI have dominated advances in that time. Along the way, I've repeatedly encountered work being done on mechanistic interpretability, including papers from the Anthropic team. Beyond skimming these papers, I haven't devoted time to thinking more deeply about these ideas.\nGiven that this is a field I'm interested in pursuing, I've decided to invest time in diving deeper into the ideas and research in the literature, and exploring where I might be able to contribute.\n\n## Why This Blog\n\nStarting this research blog serves three purposes.\n1. Mainly, I hope it will document my thoughts and ideas as they change and evolve while I learn and explore this new field. As I progress, it will be useful to return to earlier thoughts.\n2. Second, I want to improve the clarity of my writing, since I often find it difficult to express thoughts without extensive refinement.\n3. Finally, I'm hoping this imposes some level of self-accountability to keep at it regularly, since this will be on the side and doesn't overlap with my current work.","title":"Mech Interp Day 0: Motivations","date":"2025-07-09","excerpt":"Starting my mech interp journey","tags":["mech interp"]},{"slug":"02_animation","content":"\nThe previous iteration of this website featured an interactive 3D pulsating sphere built using Three.js. For the sake of a cleaner feel, I decided to remove it for this iteration, but found the experience rewarding. The animation I originally had can be found at the botto of this post! The capability for 3D graphics that Three.js provide is rich in potential, especially since modern web development has evolved far beyond static pages. Today's users expect rich, interactive experiences that feel more like native applications than traditional websites.\n\n## The Technology Stack\n\nCombining several powerful technologies can create truly engaging web experiences. The combination I initially went with included:\n\n- **Three.js** for 3D graphics and WebGL rendering\n- **Framer Motion** for smooth animations and transitions\n- **React** for component-based architecture\n- **Next.js** for performance optimization\n\n## Why 3D on the Web?\n\nThree.js has revolutionized how we think about web interfaces. Here's a simple example of creating a rotating cube:\n\n```javascript\nimport * as THREE from 'three';\n\nconst scene = new THREE.Scene();\nconst camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\nconst renderer = new THREE.WebGLRenderer();\n\nconst geometry = new THREE.BoxGeometry();\nconst material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });\nconst cube = new THREE.Mesh(geometry, material);\n\nscene.add(cube);\ncamera.position.z = 5;\n\nfunction animate() {\n    requestAnimationFrame(animate);\n    cube.rotation.x += 0.01;\n    cube.rotation.y += 0.01;\n    renderer.render(scene, camera);\n}\n\nanimate();\n```\n\n## Animation with Framer Motion\n\nFramer Motion makes it incredibly easy to add smooth animations to React components:\n\n```jsx\nimport { motion } from 'framer-motion';\n\nconst AnimatedCard = () =\u003e {\n  return (\n    \n      Interactive Card\n      This card animates on load and hover!\n    \n  );\n};\n```\n\n## My Experience\n\nThe combination of Three.js and Framer Motion in a React/Next.js environment provides an incredibly powerful toolkit for creating memorable web experiences. The key is finding the right balance between visual impact and performance. The rest of this post will go through how I set up my initial animation, with the final product at the end!\n\n## Uniform Sampling of Points on A Sphere\n\nThe key to creating a convincing sphere lies in proper point distribution. Unlike naive approaches that create clustering at poles, we use **spherical coordinates** with uniform random sampling:\n\n```javascript\nconst points = [];\nfor (var i = 0; i \u003c 1500; i++) {\n  var vertex = new THREE.Vector3();\n\n  // Uniform random sampling on sphere surface\n  var u = THREE.MathUtils.randFloat(0, 1);\n  var v = THREE.MathUtils.randFloat(0, 1);\n  var theta = 2 * Math.PI * u;           // Azimuthal angle\n  var phi = Math.acos(2 * v - 1);        // Polar angle\n\n  // Convert to Cartesian coordinates\n  vertex.x = 3.5 * Math.sin(phi) * Math.cos(theta);\n  vertex.y = 3.5 * Math.sin(phi) * Math.sin(theta);\n  vertex.z = 3.5 * Math.cos(phi);\n\n  points.push(vertex);\n}\n```\n\nThis approach ensures **uniform distribution** across the sphere surface, avoiding the visual artifacts of simpler random placement methods.\n\n## GLSL Shader\n\nThe pulsating light effect is achieved through custom **vertex and fragment shaders** that run directly on the GPU:\n\n### Shader Uniforms Setup\n\n```javascript\nconst shaderPoint = THREE.ShaderLib.points;\nconst uniforms = THREE.UniformsUtils.clone(shaderPoint.uniforms);\nuniforms.time = { value: 0 };\nuniforms.color = { type: \"v3\", value: colorValue };\n\nconst pMaterial = new THREE.ShaderMaterial({\n  uniforms,\n  transparent: true,\n  depthWrite: false,\n  blending: THREE.AdditiveBlending,  // Creates light emission effect\n  vertexShader,\n  fragmentShader,\n});\n```\n\n### The Shader Pipeline\n\nThis particle system consists of two interconnected shaders:\n\n- **Vertex Shader** (\"vert.glsl\") - Handles particle positioning, sizing, and movement\n- **Fragment Shader** (\"frag.glsl\") - Controls particle appearance, color, and glow effects\n\n### 3D Simplex Noise Implementation\n\nThe vertex shader begins with a complete **Simplex noise** implementation - a sophisticated algorithm for generating natural-looking randomness:\n\n```glsl\nfloat snoise(vec3 v) {\n    // 85 lines of math\n    // Creates smooth, continuous 3D noise\n    return 42.0 * dot(m*m, vec4(dot(p0,x0), dot(p1,x1), dot(p2,x2), dot(p3,x3)));\n}\n```\n\n**Why Simplex Noise?** Unlike basic random functions, Simplex noise provides:\n- **Continuous gradients** - no sudden jumps or artifacts\n- **3D coherence** - neighboring points have similar values\n- **Performance optimization** - faster than Perlin noise\n- **Natural patterns** - mimics organic movement and growth\n\n### Organic Particle Movement\n\nThe main vertex shader transforms each particle's position using time-based trigonometric functions:\n\n```glsl\nvec3 newPos = position;\n\nnewPos.x += sin(time + position.x * position.y) * 0.08;\nnewPos.y += cos(time + position.x * position.y * 1.1) * 0.08;\nnewPos.z += cos(time + position.x * position.y * 1.3) * 0.08;\n```\n\n**Mathematical Breakdown:**\n- **Base oscillation**: \"sin(time)\" and \"cos(time)\" create rhythmic movement\n- **Position coupling**: \"position.x * position.y\" makes each particle's movement unique\n- **Frequency variation**: Multipliers prevent synchronized motion\n- **Amplitude control**: \"0.08\" keeps movement subtle and elegant\n\nThis creates a **Lissajous-like pattern** where each particle follows its own complex orbital path, determined by its starting position.\n\n### Dynamic Particle Sizing\n\nThe most sophisticated aspect is the noise-driven size variation:\n\n```glsl\ngl_PointSize = 50. + snoise(position * 0.05 + vec3(0, 0, vtime * 0.1)) * 50.;\ngl_PointSize *= 0.5;\n```\n\n**Size Calculation Explained:**\n- **Base size**: \"50.\" pixels provides consistent minimum visibility\n- **Noise sampling**: \"snoise(position * 0.05 + ...)\" creates spatial variation\n- **Temporal drift**: \"vec3(0, 0, vtime * 0.1)\" makes noise evolve over time\n- **Amplitude**: \"* 50.\" allows sizes to range from 0 to 100 pixels\n- **Final scaling**: \"* 0.5\" reduces overall scale to 0-50 pixels\n\nThe result is particles that **breathe** - growing and shrinking organically as the noise field evolves through time.\n\n### Time-Based Animation\n\nThe pulsating effect is driven by a time uniform that updates every frame:\n\n```javascript\nconst animate = (time) =\u003e {\n  // Update shader time for pulsating effect\n  pMaterial.uniforms.time.value = time * 0.004;\n  \n  // Continue animation loop\n  requestAnimationFrame(animate);\n  renderer.render(scene, camera);\n}\n```\n\nThe **AdditiveBlending** mode creates the characteristic light emission, making particles appear to glow and blend naturally when they overlap.\n\n## Responsive Color System\n\nThe particle system adapts to the user's color mode preference through dynamic uniform updates:\n\n```javascript\n// Real-time color mode detection and adaptation\nif (localStorage.getItem(\"chakra-ui-color-mode\") === \"dark\") {\n  pMaterial.uniforms.color.value = new THREE.Color(0xffffff);  // White particles\n} else {\n  pMaterial.uniforms.color.value = new THREE.Color(0x000000);  // Black particles\n}\n```\n\nThis creates a seamless experience where the 3D scene automatically adapts to the user's interface preferences without requiring page refreshes.\n\n## Smooth Camera Animations\n\nThe initial camera movement uses an **easing function** to create natural motion:\n\n```javascript\nfunction easeOutCirc(x) {\n  return Math.sqrt(1 - Math.pow(x - 1, 4));\n}\n\n// Camera animation during first 100 frames\nif (frame \u003c= 100) {\n  const rotSpeed = -easeOutCirc(frame / 120) * Math.PI * 20;\n  \n  camera.position.x = p.x * Math.cos(rotSpeed) + p.z * Math.sin(rotSpeed);\n  camera.position.z = p.z * Math.cos(rotSpeed) - p.x * Math.sin(rotSpeed);\n  camera.lookAt(target);\n} else {\n  // Switch to user-controlled orbit after animation\n  controls.update();\n}\n```\n\nAfter the initial animation completes, control transitions to **OrbitControls** for user interaction, with automatic rotation enabled.\n\n## Performance Optimizations\n\n### GPU-Accelerated Rendering\n- **ShaderMaterial** for GPU-based calculations\n\n### Efficient Animation Loop\n```javascript\nlet req = null;\nconst animate = (time) =\u003e {\n  req = requestAnimationFrame(animate);\n  \n  // Minimal CPU calculations\n  // GPU handles particle transformations\n  \n  renderer.render(scene, camera);\n}\n\n// Proper cleanup\nreturn () =\u003e {\n  cancelAnimationFrame(req);\n  renderer.domElement.remove();\n  renderer.dispose();\n}\n```\n\n## Final Product\nHere is the final animation, best viewed in dark mode!\n\u003chr /\u003e\n\u003cvoxel-art /\u003e","title":"Building Interactive Web Experiences","date":"2025-07-08","excerpt":"Exploring the combination of Three.js, Framer Motion, and React for creating engaging user interfaces","tags":["threejs","framer-motion","webgl","animation"]},{"slug":"01_implementation","content":"\nThis document summarizes the complete implementation of a markdown-based blog system using Next.js, Chakra UI, react-markdown, and KaTeX for mathematical equations.\n\n## Tech Stack\n\n- **Next.js** - React framework with static site generation\n- **Chakra UI** - Component library for styling\n- **react-markdown** - Markdown to React component converter\n- **KaTeX** - LaTeX equation rendering\n- **gray-matter** - Frontmatter parsing\n- **Framer Motion** - Animations\n\n## Required Dependencies\n\n```bash\nnpm install react-markdown remark-gfm rehype-highlight rehype-raw gray-matter katex rehype-katex remark-math\n```\n\n## File Structure\n\n```\nproject/\n├── components/\n│   └── BlogPost.jsx\n├── lib/\n│   └── posts.js\n├── pages/\n│   ├── writing.js (blog index)\n│   └── blog/\n│       └── [slug].js (individual posts)\n├── posts/\n│   ├── my-first-post.md\n│   └── second-blog-post.md\n└── pages/_app.js\n```\n\n## Core Components\n\n### lib/posts.js - File System Functions\n```javascript\nimport fs from 'fs';\nimport path from 'path';\nimport matter from 'gray-matter';\n\nconst postsDirectory = path.join(process.cwd(), 'posts');\n\nexport function getAllPosts() {\n  const fileNames = fs.readdirSync(postsDirectory);\n  const allPostsData = fileNames.map((fileName) =\u003e {\n    const slug = fileName.replace(/\\.md$/, '');\n    const fullPath = path.join(postsDirectory, fileName);\n    const fileContents = fs.readFileSync(fullPath, 'utf8');\n    const { data, content } = matter(fileContents);\n\n    return {\n      slug,\n      content,\n      ...data,\n    };\n  });\n\n  return allPostsData.sort((a, b) =\u003e (a.date \u003c b.date ? 1 : -1));\n}\n\nexport function getPostBySlug(slug) {\n  const fullPath = path.join(postsDirectory, `${slug}.md`);\n  const fileContents = fs.readFileSync(fullPath, 'utf8');\n  const { data, content } = matter(fileContents);\n\n  return {\n    slug,\n    content,\n    ...data,\n  };\n}\n```\n\n### components/BlogPost.jsx - Markdown Renderer\n```javascript\nimport ReactMarkdown from 'react-markdown';\nimport remarkGfm from 'remark-gfm';\nimport remarkMath from 'remark-math';\nimport rehypeHighlight from 'rehype-highlight';\nimport rehypeKatex from 'rehype-katex';\nimport { \n  Box, \n  Heading, \n  Text, \n  Code, \n  Divider, \n  Link,\n  UnorderedList,\n  OrderedList,\n  ListItem,\n  useColorModeValue\n} from '@chakra-ui/react';\n\nconst BlogPost = ({ content }) =\u003e {\n  const blockquoteBg = useColorModeValue('gray.50', 'gray.700');\n  const inlineCodeBg = useColorModeValue('gray.100', 'gray.600');\n  const inlineCodeColor = useColorModeValue('gray.800', 'gray.100');\n\n  return (\n    \u003cBox maxW=\"800px\" mx=\"auto\" p={6}\u003e\n      \u003cReactMarkdown\n        remarkPlugins={[remarkGfm, remarkMath]}\n        rehypePlugins={[rehypeHighlight, rehypeKatex]}\n        components={{\n          h1: ({ children }) =\u003e (\n            \u003cHeading as=\"h1\" size=\"2xl\" mb={6} mt={8}\u003e\n              {children}\n            \u003c/Heading\u003e\n          ),\n          h2: ({ children }) =\u003e (\n            \u003cHeading as=\"h2\" size=\"xl\" mb={4} mt={6}\u003e\n              {children}\n            \u003c/Heading\u003e\n          ),\n          h3: ({ children }) =\u003e (\n            \u003cHeading as=\"h3\" size=\"lg\" mb={3} mt={5}\u003e\n              {children}\n            \u003c/Heading\u003e\n          ),\n          p: ({ children }) =\u003e (\n            \u003cText mb={4} lineHeight=\"1.7\"\u003e\n              {children}\n            \u003c/Text\u003e\n          ),\n          ul: ({ children }) =\u003e (\n            \u003cUnorderedList mb={4} spacing={2}\u003e\n              {children}\n            \u003c/UnorderedList\u003e\n          ),\n          ol: ({ children }) =\u003e (\n            \u003cOrderedList mb={4} spacing={2}\u003e\n              {children}\n            \u003c/OrderedList\u003e\n          ),\n          li: ({ children }) =\u003e (\n            \u003cListItem\u003e{children}\u003c/ListItem\u003e\n          ),\n          code: ({ inline, children, className }) =\u003e {\n            return inline ? (\n              \u003cCode \n                px={2} \n                py={1} \n                bg={inlineCodeBg}\n                color={inlineCodeColor}\n                borderRadius=\"md\"\n                suppressHydrationWarning={true}\n              \u003e\n                {children}\n              \u003c/Code\u003e\n            ) : (\n              \u003cBox mb={4}\u003e\n                \u003cCode\n                  as=\"pre\"\n                  display=\"block\"\n                  p={4}\n                  bg=\"gray.900\"\n                  color=\"white\"\n                  _dark={{ bg: \"gray.800\" }}\n                  borderRadius=\"md\"\n                  overflow=\"auto\"\n                  suppressHydrationWarning={true}\n                \u003e\n                  {children}\n                \u003c/Code\u003e\n              \u003c/Box\u003e\n            );\n          },\n          a: ({ href, children }) =\u003e (\n            \u003cLink href={href} color=\"blue.500\" isExternal\u003e\n              {children}\n            \u003c/Link\u003e\n          ),\n          blockquote: ({ children }) =\u003e (\n            \u003cBox\n              as=\"blockquote\"\n              borderLeft=\"4px solid\"\n              borderColor=\"blue.500\"\n              pl={4}\n              py={2}\n              mb={4}\n              fontStyle=\"italic\"\n              bg={blockquoteBg}\n              _dark={{ bg: \"gray.700\", borderColor: \"blue.300\" }}\n              borderRadius=\"md\"\n              suppressHydrationWarning={true}\n            \u003e\n              {children}\n            \u003c/Box\u003e\n          ),\n          hr: () =\u003e \u003cDivider my={6} /\u003e,\n        }}\n      \u003e\n        {content}\n      \u003c/ReactMarkdown\u003e\n    \u003c/Box\u003e\n  );\n};\n\nexport default BlogPost;\n```\n\n### pages/writing.js - Blog Index\n```javascript\nimport NextLink from 'next/link'\nimport { Box, Container, Heading, Link, Text } from '@chakra-ui/react'\nimport Layout from '../components/layouts/article'\nimport Section from '../components/section'\nimport { getAllPosts } from '../lib/posts'\n\nconst Writing = ({ posts = [] }) =\u003e {\n  return (\n    \u003cLayout title=\"Posts\"\u003e\n      \u003cContainer\u003e\n        \u003cHeading as=\"h3\" fontSize={20} mb={4} mt={5}\u003e\n          Posts\n        \u003c/Heading\u003e\n\n        {posts \u0026\u0026 posts.map((post, index) =\u003e (\n          \u003cSection key={post.slug} delay={0.1 + (index + 1) * 0.1}\u003e\n            \u003cBox my={4}\u003e\n              \u003cText fontSize=\"sm\" color=\"gray.500\" display=\"inline\" mr={3}\u003e\n                {new Date(post.date + 'T00:00:00').toLocaleDateString('en-US', {\n                  year: 'numeric',\n                  month: 'short',\n                  day: 'numeric',\n                  timeZone: 'UTC'\n                })}\n              \u003c/Text\u003e\n              \u003cLink as={NextLink} href={`/blog/${post.slug}`}\u003e\n                {post.title}\n              \u003c/Link\u003e\n            \u003c/Box\u003e\n          \u003c/Section\u003e\n        ))}\n\n      \u003c/Container\u003e\n    \u003c/Layout\u003e\n  )\n}\n\nexport async function getStaticProps() {\n  try {\n    const posts = getAllPosts();\n    return {\n      props: {\n        posts,\n      },\n    };\n  } catch (error) {\n    console.error('Error in getStaticProps:', error);\n    return {\n      props: {\n        posts: [],\n      },\n    };\n  }\n}\n\nexport default Writing\n```\n\n### pages/blog/[slug].js - Individual Post Pages\n```javascript\nimport { getAllPosts, getPostBySlug } from '../../lib/posts';\nimport BlogPost from '../../components/BlogPost';\nimport { Box, Heading, Text } from '@chakra-ui/react';\n\nexport default function Post({ post }) {\n  return (\n    \u003cBox\u003e\n      \u003cBox textAlign=\"center\" mb={8}\u003e\n        \u003cHeading as=\"h1\" size=\"3xl\" mb={4}\u003e\n          {post.title}\n        \u003c/Heading\u003e\n        \u003cText color=\"gray.600\"\u003e{post.date}\u003c/Text\u003e\n      \u003c/Box\u003e\n      \u003cBlogPost content={post.content} /\u003e\n    \u003c/Box\u003e\n  );\n}\n\nexport async function getStaticProps({ params }) {\n  const post = getPostBySlug(params.slug);\n  return {\n    props: {\n      post,\n    },\n  };\n}\n\nexport async function getStaticPaths() {\n  const posts = getAllPosts();\n  const paths = posts.map((post) =\u003e ({\n    params: { slug: post.slug },\n  }));\n\n  return {\n    paths,\n    fallback: false,\n  };\n}\n```\n\n## Markdown Features Supported\n\n### Basic Formatting\n- **Bold text** and *italic text*\n- Headers (H1, H2, H3)\n- Paragraphs with proper spacing\n- Horizontal rules\n- Links (internal and external)\n\n### Lists\n- Unordered lists with bullet points\n- Ordered lists with numbers\n- Proper spacing between items\n\n### Code\n- Multi-line code blocks with language-specific highlighting\n- Dark/light mode responsive styling\n\n### Blockquotes\n\u003e Styled blockquotes with left border. Dark/light mode responsive\n\n### Mathematical Equations\n- Inline math: $E = mc^2$\n- Block equations: $$\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$$\n- Complex expressions with proper LaTeX rendering\n\n## Frontmatter Structure\n```yaml\n---\ntitle: \"Post Title\"\ndate: \"2025-01-15\"\nexcerpt: \"Brief description of the post\"\ntags: [\"tag1\", \"tag2\", \"tag3\"]\n---\n```\n\n## Development Workflow\n\n1. Create markdown files in posts/ directory\n2. Posts automatically appear on /writing page\n\n## Troubleshooting Notes\n\n- **Date Issues**: Use `new Date(post.date + 'T00:00:00')` to prevent timezone shifts\n- Inline equations not working","title":"Blog Implementation Summary","date":"2025-07-07","excerpt":"Complete guide to implementing a markdown blog with Next.js, Chakra UI, and mathematical equations","tags":["nextjs","react-markdown","chakra-ui","katex","blog"]},{"slug":"00_hello_world","content":"\n## Welcome to My Blog\n\nThis is my first blog post! I will just test functionality in this post, and write up how I implemented post functionality in a future post.\n\n## Code Example\n\n```python\ndef hello_world():\n    print(\"Hello, World!\")\n    return True\n``` \n\n## More Examples\n\n\u003e This is a blockquote with some important information.\n\nCheck out [this link](https://example.com) for more info.\n\nHere's some **bold text** and *italic text*.\n\n### Lists\n\nUnordered list:\n- Item 1\n- Item 2\n- Item 3\n\nOrdered list:\n1. First item\n2. Second item\n3. Third item \n\n### Equations\nThis is an inline equation: $E = mc^2$.\n\nIn web animations, we often use trigonometric functions. The general form of a sine wave is:\n\n$$\ny(x,t) = A \\sin(kx - \\omega t + \\phi)\n$$\n\nWhere:\n- $A$ is the amplitude\n- $k$ is the wave number  \n- $\\omega$ is the angular frequency\n- $\\phi$ is the phase shift\n\nThis equation is perfect for creating smooth, natural animations in Three.js!\n\nThat's all for now!\n","title":"Hello World","date":"2025-07-06","excerpt":"This is my first blog post using react-markdown","tags":["react","nextjs","markdown"]}]},"__N_SSG":true},"page":"/writing","query":{},"buildId":"FFpp1ErOPN9PuGJ0QVGRn","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>