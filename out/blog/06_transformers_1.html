<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="description" content="Brent&#x27;s homepage"/><meta name="author" content="Brent Tan"/><title>Brent Tan - Homepage</title><meta name="next-head-count" content="5"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/e9556718e3f0f4cf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e9556718e3f0f4cf.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-c7e8ec73d08c57f3.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-676fc21436baeb6f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-cb76b27212f05dea.js" defer=""></script><script src="/_next/static/chunks/175675d1-254dc21e030ba2ac.js" defer=""></script><script src="/_next/static/chunks/645-40e6d947d4f7bb86.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-9dc0ab71c3bdc569.js" defer=""></script><script src="/_next/static/bUrBysIoWJKwE5ZkPd0HO/_buildManifest.js" defer=""></script><script src="/_next/static/bUrBysIoWJKwE5ZkPd0HO/_ssgManifest.js" defer=""></script><style id="__jsx-fed65fd97a038da">@import url("https://fonts.googleapis.com/css2?family=M+PLUS+Rounded+1c:wght@300;700&display=swap");@import url("https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@100..900&display=swap");</style></head><body><script id="chakra-script">!(function(){try{var a=function(c){var v="(prefers-color-scheme: dark)",h=window.matchMedia(v).matches?"dark":"light",r=c==="system"?h:c,o=document.documentElement,s=document.body,l="chakra-ui-light",d="chakra-ui-dark",i=r==="dark";return s.classList.add(i?d:l),s.classList.remove(i?l:d),o.style.colorScheme=r,o.dataset.theme=r,r},n=a,m="dark",e="chakra-ui-color-mode",t=localStorage.getItem(e);t?a(t):localStorage.setItem(e,a(m))}catch(a){}})();</script><div id="__next"><style data-emotion="css-global 93m1n8">:host,:root,[data-theme]{--chakra-ring-inset:var(--chakra-empty,/*!*/ /*!*/);--chakra-ring-offset-width:0px;--chakra-ring-offset-color:#fff;--chakra-ring-color:rgba(66, 153, 225, 0.6);--chakra-ring-offset-shadow:0 0 #0000;--chakra-ring-shadow:0 0 #0000;--chakra-space-x-reverse:0;--chakra-space-y-reverse:0;--chakra-colors-transparent:transparent;--chakra-colors-current:currentColor;--chakra-colors-black:#000000;--chakra-colors-white:#FFFFFF;--chakra-colors-whiteAlpha-50:rgba(255, 255, 255, 0.04);--chakra-colors-whiteAlpha-100:rgba(255, 255, 255, 0.06);--chakra-colors-whiteAlpha-200:rgba(255, 255, 255, 0.08);--chakra-colors-whiteAlpha-300:rgba(255, 255, 255, 0.16);--chakra-colors-whiteAlpha-400:rgba(255, 255, 255, 0.24);--chakra-colors-whiteAlpha-500:rgba(255, 255, 255, 0.36);--chakra-colors-whiteAlpha-600:rgba(255, 255, 255, 0.48);--chakra-colors-whiteAlpha-700:rgba(255, 255, 255, 0.64);--chakra-colors-whiteAlpha-800:rgba(255, 255, 255, 0.80);--chakra-colors-whiteAlpha-900:rgba(255, 255, 255, 0.92);--chakra-colors-blackAlpha-50:rgba(0, 0, 0, 0.04);--chakra-colors-blackAlpha-100:rgba(0, 0, 0, 0.06);--chakra-colors-blackAlpha-200:rgba(0, 0, 0, 0.08);--chakra-colors-blackAlpha-300:rgba(0, 0, 0, 0.16);--chakra-colors-blackAlpha-400:rgba(0, 0, 0, 0.24);--chakra-colors-blackAlpha-500:rgba(0, 0, 0, 0.36);--chakra-colors-blackAlpha-600:rgba(0, 0, 0, 0.48);--chakra-colors-blackAlpha-700:rgba(0, 0, 0, 0.64);--chakra-colors-blackAlpha-800:rgba(0, 0, 0, 0.80);--chakra-colors-blackAlpha-900:rgba(0, 0, 0, 0.92);--chakra-colors-gray-50:#F7FAFC;--chakra-colors-gray-100:#EDF2F7;--chakra-colors-gray-200:#E2E8F0;--chakra-colors-gray-300:#CBD5E0;--chakra-colors-gray-400:#A0AEC0;--chakra-colors-gray-500:#718096;--chakra-colors-gray-600:#4A5568;--chakra-colors-gray-700:#2D3748;--chakra-colors-gray-800:#1A202C;--chakra-colors-gray-900:#171923;--chakra-colors-red-50:#FFF5F5;--chakra-colors-red-100:#FED7D7;--chakra-colors-red-200:#FEB2B2;--chakra-colors-red-300:#FC8181;--chakra-colors-red-400:#F56565;--chakra-colors-red-500:#E53E3E;--chakra-colors-red-600:#C53030;--chakra-colors-red-700:#9B2C2C;--chakra-colors-red-800:#822727;--chakra-colors-red-900:#63171B;--chakra-colors-orange-50:#FFFAF0;--chakra-colors-orange-100:#FEEBC8;--chakra-colors-orange-200:#FBD38D;--chakra-colors-orange-300:#F6AD55;--chakra-colors-orange-400:#ED8936;--chakra-colors-orange-500:#DD6B20;--chakra-colors-orange-600:#C05621;--chakra-colors-orange-700:#9C4221;--chakra-colors-orange-800:#7B341E;--chakra-colors-orange-900:#652B19;--chakra-colors-yellow-50:#FFFFF0;--chakra-colors-yellow-100:#FEFCBF;--chakra-colors-yellow-200:#FAF089;--chakra-colors-yellow-300:#F6E05E;--chakra-colors-yellow-400:#ECC94B;--chakra-colors-yellow-500:#D69E2E;--chakra-colors-yellow-600:#B7791F;--chakra-colors-yellow-700:#975A16;--chakra-colors-yellow-800:#744210;--chakra-colors-yellow-900:#5F370E;--chakra-colors-green-50:#F0FFF4;--chakra-colors-green-100:#C6F6D5;--chakra-colors-green-200:#9AE6B4;--chakra-colors-green-300:#68D391;--chakra-colors-green-400:#48BB78;--chakra-colors-green-500:#38A169;--chakra-colors-green-600:#2F855A;--chakra-colors-green-700:#276749;--chakra-colors-green-800:#22543D;--chakra-colors-green-900:#1C4532;--chakra-colors-teal-50:#E6FFFA;--chakra-colors-teal-100:#B2F5EA;--chakra-colors-teal-200:#81E6D9;--chakra-colors-teal-300:#4FD1C5;--chakra-colors-teal-400:#38B2AC;--chakra-colors-teal-500:#319795;--chakra-colors-teal-600:#2C7A7B;--chakra-colors-teal-700:#285E61;--chakra-colors-teal-800:#234E52;--chakra-colors-teal-900:#1D4044;--chakra-colors-blue-50:#ebf8ff;--chakra-colors-blue-100:#bee3f8;--chakra-colors-blue-200:#90cdf4;--chakra-colors-blue-300:#63b3ed;--chakra-colors-blue-400:#4299e1;--chakra-colors-blue-500:#3182ce;--chakra-colors-blue-600:#2b6cb0;--chakra-colors-blue-700:#2c5282;--chakra-colors-blue-800:#2a4365;--chakra-colors-blue-900:#1A365D;--chakra-colors-cyan-50:#EDFDFD;--chakra-colors-cyan-100:#C4F1F9;--chakra-colors-cyan-200:#9DECF9;--chakra-colors-cyan-300:#76E4F7;--chakra-colors-cyan-400:#0BC5EA;--chakra-colors-cyan-500:#00B5D8;--chakra-colors-cyan-600:#00A3C4;--chakra-colors-cyan-700:#0987A0;--chakra-colors-cyan-800:#086F83;--chakra-colors-cyan-900:#065666;--chakra-colors-purple-50:#FAF5FF;--chakra-colors-purple-100:#E9D8FD;--chakra-colors-purple-200:#D6BCFA;--chakra-colors-purple-300:#B794F4;--chakra-colors-purple-400:#9F7AEA;--chakra-colors-purple-500:#805AD5;--chakra-colors-purple-600:#6B46C1;--chakra-colors-purple-700:#553C9A;--chakra-colors-purple-800:#44337A;--chakra-colors-purple-900:#322659;--chakra-colors-pink-50:#FFF5F7;--chakra-colors-pink-100:#FED7E2;--chakra-colors-pink-200:#FBB6CE;--chakra-colors-pink-300:#F687B3;--chakra-colors-pink-400:#ED64A6;--chakra-colors-pink-500:#D53F8C;--chakra-colors-pink-600:#B83280;--chakra-colors-pink-700:#97266D;--chakra-colors-pink-800:#702459;--chakra-colors-pink-900:#521B41;--chakra-colors-linkedin-50:#E8F4F9;--chakra-colors-linkedin-100:#CFEDFB;--chakra-colors-linkedin-200:#9BDAF3;--chakra-colors-linkedin-300:#68C7EC;--chakra-colors-linkedin-400:#34B3E4;--chakra-colors-linkedin-500:#00A0DC;--chakra-colors-linkedin-600:#008CC9;--chakra-colors-linkedin-700:#0077B5;--chakra-colors-linkedin-800:#005E93;--chakra-colors-linkedin-900:#004471;--chakra-colors-facebook-50:#E8F4F9;--chakra-colors-facebook-100:#D9DEE9;--chakra-colors-facebook-200:#B7C2DA;--chakra-colors-facebook-300:#6482C0;--chakra-colors-facebook-400:#4267B2;--chakra-colors-facebook-500:#385898;--chakra-colors-facebook-600:#314E89;--chakra-colors-facebook-700:#29487D;--chakra-colors-facebook-800:#223B67;--chakra-colors-facebook-900:#1E355B;--chakra-colors-messenger-50:#D0E6FF;--chakra-colors-messenger-100:#B9DAFF;--chakra-colors-messenger-200:#A2CDFF;--chakra-colors-messenger-300:#7AB8FF;--chakra-colors-messenger-400:#2E90FF;--chakra-colors-messenger-500:#0078FF;--chakra-colors-messenger-600:#0063D1;--chakra-colors-messenger-700:#0052AC;--chakra-colors-messenger-800:#003C7E;--chakra-colors-messenger-900:#002C5C;--chakra-colors-whatsapp-50:#dffeec;--chakra-colors-whatsapp-100:#b9f5d0;--chakra-colors-whatsapp-200:#90edb3;--chakra-colors-whatsapp-300:#65e495;--chakra-colors-whatsapp-400:#3cdd78;--chakra-colors-whatsapp-500:#22c35e;--chakra-colors-whatsapp-600:#179848;--chakra-colors-whatsapp-700:#0c6c33;--chakra-colors-whatsapp-800:#01421c;--chakra-colors-whatsapp-900:#001803;--chakra-colors-twitter-50:#E5F4FD;--chakra-colors-twitter-100:#C8E9FB;--chakra-colors-twitter-200:#A8DCFA;--chakra-colors-twitter-300:#83CDF7;--chakra-colors-twitter-400:#57BBF5;--chakra-colors-twitter-500:#1DA1F2;--chakra-colors-twitter-600:#1A94DA;--chakra-colors-twitter-700:#1681BF;--chakra-colors-twitter-800:#136B9E;--chakra-colors-twitter-900:#0D4D71;--chakra-colors-telegram-50:#E3F2F9;--chakra-colors-telegram-100:#C5E4F3;--chakra-colors-telegram-200:#A2D4EC;--chakra-colors-telegram-300:#7AC1E4;--chakra-colors-telegram-400:#47A9DA;--chakra-colors-telegram-500:#0088CC;--chakra-colors-telegram-600:#007AB8;--chakra-colors-telegram-700:#006BA1;--chakra-colors-telegram-800:#005885;--chakra-colors-telegram-900:#003F5E;--chakra-colors-grassTeal:#88ccca;--chakra-borders-none:0;--chakra-borders-1px:1px solid;--chakra-borders-2px:2px solid;--chakra-borders-4px:4px solid;--chakra-borders-8px:8px solid;--chakra-fonts-heading:'M PLUS Rounded 1c';--chakra-fonts-body:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--chakra-fonts-mono:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--chakra-fontSizes-3xs:0.45rem;--chakra-fontSizes-2xs:0.625rem;--chakra-fontSizes-xs:0.75rem;--chakra-fontSizes-sm:0.875rem;--chakra-fontSizes-md:1rem;--chakra-fontSizes-lg:1.125rem;--chakra-fontSizes-xl:1.25rem;--chakra-fontSizes-2xl:1.5rem;--chakra-fontSizes-3xl:1.875rem;--chakra-fontSizes-4xl:2.25rem;--chakra-fontSizes-5xl:3rem;--chakra-fontSizes-6xl:3.75rem;--chakra-fontSizes-7xl:4.5rem;--chakra-fontSizes-8xl:6rem;--chakra-fontSizes-9xl:8rem;--chakra-fontWeights-hairline:100;--chakra-fontWeights-thin:200;--chakra-fontWeights-light:300;--chakra-fontWeights-normal:400;--chakra-fontWeights-medium:500;--chakra-fontWeights-semibold:600;--chakra-fontWeights-bold:700;--chakra-fontWeights-extrabold:800;--chakra-fontWeights-black:900;--chakra-letterSpacings-tighter:-0.05em;--chakra-letterSpacings-tight:-0.025em;--chakra-letterSpacings-normal:0;--chakra-letterSpacings-wide:0.025em;--chakra-letterSpacings-wider:0.05em;--chakra-letterSpacings-widest:0.1em;--chakra-lineHeights-3:.75rem;--chakra-lineHeights-4:1rem;--chakra-lineHeights-5:1.25rem;--chakra-lineHeights-6:1.5rem;--chakra-lineHeights-7:1.75rem;--chakra-lineHeights-8:2rem;--chakra-lineHeights-9:2.25rem;--chakra-lineHeights-10:2.5rem;--chakra-lineHeights-normal:normal;--chakra-lineHeights-none:1;--chakra-lineHeights-shorter:1.25;--chakra-lineHeights-short:1.375;--chakra-lineHeights-base:1.5;--chakra-lineHeights-tall:1.625;--chakra-lineHeights-taller:2;--chakra-radii-none:0;--chakra-radii-sm:0.125rem;--chakra-radii-base:0.25rem;--chakra-radii-md:0.375rem;--chakra-radii-lg:0.5rem;--chakra-radii-xl:0.75rem;--chakra-radii-2xl:1rem;--chakra-radii-3xl:1.5rem;--chakra-radii-full:9999px;--chakra-space-1:0.25rem;--chakra-space-2:0.5rem;--chakra-space-3:0.75rem;--chakra-space-4:1rem;--chakra-space-5:1.25rem;--chakra-space-6:1.5rem;--chakra-space-7:1.75rem;--chakra-space-8:2rem;--chakra-space-9:2.25rem;--chakra-space-10:2.5rem;--chakra-space-12:3rem;--chakra-space-14:3.5rem;--chakra-space-16:4rem;--chakra-space-20:5rem;--chakra-space-24:6rem;--chakra-space-28:7rem;--chakra-space-32:8rem;--chakra-space-36:9rem;--chakra-space-40:10rem;--chakra-space-44:11rem;--chakra-space-48:12rem;--chakra-space-52:13rem;--chakra-space-56:14rem;--chakra-space-60:15rem;--chakra-space-64:16rem;--chakra-space-72:18rem;--chakra-space-80:20rem;--chakra-space-96:24rem;--chakra-space-px:1px;--chakra-space-0-5:0.125rem;--chakra-space-1-5:0.375rem;--chakra-space-2-5:0.625rem;--chakra-space-3-5:0.875rem;--chakra-shadows-xs:0 0 0 1px rgba(0, 0, 0, 0.05);--chakra-shadows-sm:0 1px 2px 0 rgba(0, 0, 0, 0.05);--chakra-shadows-base:0 1px 3px 0 rgba(0, 0, 0, 0.1),0 1px 2px 0 rgba(0, 0, 0, 0.06);--chakra-shadows-md:0 4px 6px -1px rgba(0, 0, 0, 0.1),0 2px 4px -1px rgba(0, 0, 0, 0.06);--chakra-shadows-lg:0 10px 15px -3px rgba(0, 0, 0, 0.1),0 4px 6px -2px rgba(0, 0, 0, 0.05);--chakra-shadows-xl:0 20px 25px -5px rgba(0, 0, 0, 0.1),0 10px 10px -5px rgba(0, 0, 0, 0.04);--chakra-shadows-2xl:0 25px 50px -12px rgba(0, 0, 0, 0.25);--chakra-shadows-outline:0 0 0 3px rgba(66, 153, 225, 0.6);--chakra-shadows-inner:inset 0 2px 4px 0 rgba(0,0,0,0.06);--chakra-shadows-none:none;--chakra-shadows-dark-lg:rgba(0, 0, 0, 0.1) 0px 0px 0px 1px,rgba(0, 0, 0, 0.2) 0px 5px 10px,rgba(0, 0, 0, 0.4) 0px 15px 40px;--chakra-sizes-1:0.25rem;--chakra-sizes-2:0.5rem;--chakra-sizes-3:0.75rem;--chakra-sizes-4:1rem;--chakra-sizes-5:1.25rem;--chakra-sizes-6:1.5rem;--chakra-sizes-7:1.75rem;--chakra-sizes-8:2rem;--chakra-sizes-9:2.25rem;--chakra-sizes-10:2.5rem;--chakra-sizes-12:3rem;--chakra-sizes-14:3.5rem;--chakra-sizes-16:4rem;--chakra-sizes-20:5rem;--chakra-sizes-24:6rem;--chakra-sizes-28:7rem;--chakra-sizes-32:8rem;--chakra-sizes-36:9rem;--chakra-sizes-40:10rem;--chakra-sizes-44:11rem;--chakra-sizes-48:12rem;--chakra-sizes-52:13rem;--chakra-sizes-56:14rem;--chakra-sizes-60:15rem;--chakra-sizes-64:16rem;--chakra-sizes-72:18rem;--chakra-sizes-80:20rem;--chakra-sizes-96:24rem;--chakra-sizes-px:1px;--chakra-sizes-0-5:0.125rem;--chakra-sizes-1-5:0.375rem;--chakra-sizes-2-5:0.625rem;--chakra-sizes-3-5:0.875rem;--chakra-sizes-max:max-content;--chakra-sizes-min:min-content;--chakra-sizes-full:100%;--chakra-sizes-3xs:14rem;--chakra-sizes-2xs:16rem;--chakra-sizes-xs:20rem;--chakra-sizes-sm:24rem;--chakra-sizes-md:28rem;--chakra-sizes-lg:32rem;--chakra-sizes-xl:36rem;--chakra-sizes-2xl:42rem;--chakra-sizes-3xl:48rem;--chakra-sizes-4xl:56rem;--chakra-sizes-5xl:64rem;--chakra-sizes-6xl:72rem;--chakra-sizes-7xl:80rem;--chakra-sizes-8xl:90rem;--chakra-sizes-prose:60ch;--chakra-sizes-container-sm:640px;--chakra-sizes-container-md:768px;--chakra-sizes-container-lg:1024px;--chakra-sizes-container-xl:1280px;--chakra-zIndices-hide:-1;--chakra-zIndices-auto:auto;--chakra-zIndices-base:0;--chakra-zIndices-docked:10;--chakra-zIndices-dropdown:1000;--chakra-zIndices-sticky:1100;--chakra-zIndices-banner:1200;--chakra-zIndices-overlay:1300;--chakra-zIndices-modal:1400;--chakra-zIndices-popover:1500;--chakra-zIndices-skipLink:1600;--chakra-zIndices-toast:1700;--chakra-zIndices-tooltip:1800;--chakra-transition-property-common:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform;--chakra-transition-property-colors:background-color,border-color,color,fill,stroke;--chakra-transition-property-dimensions:width,height;--chakra-transition-property-position:left,right,top,bottom;--chakra-transition-property-background:background-color,background-image,background-position;--chakra-transition-easing-ease-in:cubic-bezier(0.4, 0, 1, 1);--chakra-transition-easing-ease-out:cubic-bezier(0, 0, 0.2, 1);--chakra-transition-easing-ease-in-out:cubic-bezier(0.4, 0, 0.2, 1);--chakra-transition-duration-ultra-fast:50ms;--chakra-transition-duration-faster:100ms;--chakra-transition-duration-fast:150ms;--chakra-transition-duration-normal:200ms;--chakra-transition-duration-slow:300ms;--chakra-transition-duration-slower:400ms;--chakra-transition-duration-ultra-slow:500ms;--chakra-blur-none:0;--chakra-blur-sm:4px;--chakra-blur-base:8px;--chakra-blur-md:12px;--chakra-blur-lg:16px;--chakra-blur-xl:24px;--chakra-blur-2xl:40px;--chakra-blur-3xl:64px;--chakra-breakpoints-base:0em;--chakra-breakpoints-sm:30em;--chakra-breakpoints-md:48em;--chakra-breakpoints-lg:62em;--chakra-breakpoints-xl:80em;--chakra-breakpoints-2xl:96em;}.chakra-ui-light :host:not([data-theme]),.chakra-ui-light :root:not([data-theme]),.chakra-ui-light [data-theme]:not([data-theme]),[data-theme=light] :host:not([data-theme]),[data-theme=light] :root:not([data-theme]),[data-theme=light] [data-theme]:not([data-theme]),:host[data-theme=light],:root[data-theme=light],[data-theme][data-theme=light]{--chakra-colors-chakra-body-text:var(--chakra-colors-gray-800);--chakra-colors-chakra-body-bg:var(--chakra-colors-white);--chakra-colors-chakra-border-color:var(--chakra-colors-gray-200);--chakra-colors-chakra-inverse-text:var(--chakra-colors-white);--chakra-colors-chakra-subtle-bg:var(--chakra-colors-gray-100);--chakra-colors-chakra-subtle-text:var(--chakra-colors-gray-600);--chakra-colors-chakra-placeholder-color:var(--chakra-colors-gray-500);}.chakra-ui-dark :host:not([data-theme]),.chakra-ui-dark :root:not([data-theme]),.chakra-ui-dark [data-theme]:not([data-theme]),[data-theme=dark] :host:not([data-theme]),[data-theme=dark] :root:not([data-theme]),[data-theme=dark] [data-theme]:not([data-theme]),:host[data-theme=dark],:root[data-theme=dark],[data-theme][data-theme=dark]{--chakra-colors-chakra-body-text:var(--chakra-colors-whiteAlpha-900);--chakra-colors-chakra-body-bg:var(--chakra-colors-gray-800);--chakra-colors-chakra-border-color:var(--chakra-colors-whiteAlpha-300);--chakra-colors-chakra-inverse-text:var(--chakra-colors-gray-800);--chakra-colors-chakra-subtle-bg:var(--chakra-colors-gray-700);--chakra-colors-chakra-subtle-text:var(--chakra-colors-gray-400);--chakra-colors-chakra-placeholder-color:var(--chakra-colors-whiteAlpha-400);}</style><style data-emotion="css-global fubdgu">html{line-height:1.5;-webkit-text-size-adjust:100%;font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;-moz-osx-font-smoothing:grayscale;touch-action:manipulation;}body{position:relative;min-height:100%;margin:0;font-feature-settings:"kern";}:where(*, *::before, *::after){border-width:0;border-style:solid;box-sizing:border-box;word-wrap:break-word;}main{display:block;}hr{border-top-width:1px;box-sizing:content-box;height:0;overflow:visible;}:where(pre, code, kbd,samp){font-family:SFMono-Regular,Menlo,Monaco,Consolas,monospace;font-size:1em;}a{background-color:transparent;color:inherit;-webkit-text-decoration:inherit;text-decoration:inherit;}abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;}:where(b, strong){font-weight:bold;}small{font-size:80%;}:where(sub,sup){font-size:75%;line-height:0;position:relative;vertical-align:baseline;}sub{bottom:-0.25em;}sup{top:-0.5em;}img{border-style:none;}:where(button, input, optgroup, select, textarea){font-family:inherit;font-size:100%;line-height:1.15;margin:0;}:where(button, input){overflow:visible;}:where(button, select){text-transform:none;}:where(
          button::-moz-focus-inner,
          [type="button"]::-moz-focus-inner,
          [type="reset"]::-moz-focus-inner,
          [type="submit"]::-moz-focus-inner
        ){border-style:none;padding:0;}fieldset{padding:0.35em 0.75em 0.625em;}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;}progress{vertical-align:baseline;}textarea{overflow:auto;}:where([type="checkbox"], [type="radio"]){box-sizing:border-box;padding:0;}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{-webkit-appearance:none!important;}input[type="number"]{-moz-appearance:textfield;}input[type="search"]{-webkit-appearance:textfield;outline-offset:-2px;}input[type="search"]::-webkit-search-decoration{-webkit-appearance:none!important;}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;}details{display:block;}summary{display:-webkit-box;display:-webkit-list-item;display:-ms-list-itembox;display:list-item;}template{display:none;}[hidden]{display:none!important;}:where(
          blockquote,
          dl,
          dd,
          h1,
          h2,
          h3,
          h4,
          h5,
          h6,
          hr,
          figure,
          p,
          pre
        ){margin:0;}button{background:transparent;padding:0;}fieldset{margin:0;padding:0;}:where(ol, ul){margin:0;padding:0;}textarea{resize:vertical;}:where(button, [role="button"]){cursor:pointer;}button::-moz-focus-inner{border:0!important;}table{border-collapse:collapse;}:where(h1, h2, h3, h4, h5, h6){font-size:inherit;font-weight:inherit;}:where(button, input, optgroup, select, textarea){padding:0;line-height:inherit;color:inherit;}:where(img, svg, video, canvas, audio, iframe, embed, object){display:block;}:where(img, video){max-width:100%;height:auto;}[data-js-focus-visible] :focus:not([data-focus-visible-added]):not(
          [data-focus-visible-disabled]
        ){outline:none;box-shadow:none;}select::-ms-expand{display:none;}:root,:host{--chakra-vh:100vh;}@supports (height: -webkit-fill-available){:root,:host{--chakra-vh:-webkit-fill-available;}}@supports (height: -moz-fill-available){:root,:host{--chakra-vh:-moz-fill-available;}}@supports (height: 100dvh){:root,:host{--chakra-vh:100dvh;}}</style><style data-emotion="css-global lbfpqv">body{font-family:var(--chakra-fonts-body);color:var(--chakra-colors-chakra-body-text);background:#202023;transition-property:background-color;transition-duration:var(--chakra-transition-duration-normal);line-height:var(--chakra-lineHeights-base);}*::-webkit-input-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*::-moz-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*:-ms-input-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*::placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*,*::before,::after{border-color:var(--chakra-colors-chakra-border-color);}</style><style data-emotion="css 1lp32oh">.css-1lp32oh{padding-bottom:var(--chakra-space-8);}</style><main class="css-1lp32oh"><style data-emotion="css e1s8q">.css-e1s8q{position:fixed;width:100%;background:#20202380;z-index:2;-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px);}</style><nav path="/blog/06_transformers_1" class="css-e1s8q"><style data-emotion="css 1wkbond">.css-1wkbond{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:var(--chakra-space-2);max-width:var(--chakra-sizes-container-md);}</style><div class="chakra-container css-1wkbond" wrap="wrap" align="center" justify="space-between"><style data-emotion="css qi9cid">.css-qi9cid{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-right:var(--chakra-space-5);}</style><div class="css-qi9cid"><style data-emotion="css 10zaf98">.css-10zaf98{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-2xl);line-height:1.33;letter-spacing:var(--chakra-letterSpacings-tighter);}@media screen and (min-width: 48em){.css-10zaf98{font-size:var(--chakra-fontSizes-3xl);line-height:1.2;}}</style><h1 class="chakra-heading css-10zaf98"><a href="/"><style data-emotion="css lr1k0q">.css-lr1k0q{font-weight:bold;font-size:18px;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:30px;line-height:20px;padding:10px;}.css-lr1k0q >svg{-webkit-transition:500ms ease;transition:500ms ease;}.css-lr1k0q:hover>svg{-webkit-transform:rotate(90deg);-moz-transform:rotate(90deg);-ms-transform:rotate(90deg);transform:rotate(90deg);}</style><span class="css-lr1k0q"><svg width="20" height="20" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><circle cx="10" cy="10" r="10" fill="#333A3F"></circle><path d="M 9 0 L 9 20 M 9 8 L 20 8" stroke="#FFFFFF" stroke-width="0.8"></path><circle cx="5.5" cy="3.9" r=".3" fill="#FFFFFF"></circle><circle cx="17" cy="5" r=".2" fill="#FFFFFF"></circle><circle cx="5" cy="17" r=".2" fill="#FFFFFF"></circle><circle cx="15" cy="15" r=".3" fill="#FFFFFF"></circle></svg><style data-emotion="css ug13ir">.css-ug13ir{margin-left:var(--chakra-space-3);color:var(--chakra-colors-whiteAlpha-900);font-family:M PLUS Rounded 1c;font-weight:var(--chakra-fontWeights-bold);}</style><p class="chakra-text css-ug13ir">Brent Tan</p></span></a></h1></div><style data-emotion="css 4r1ia7">.css-4r1ia7{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:0.5rem;width:var(--chakra-sizes-full);-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;margin-top:var(--chakra-space-4);}@media screen and (min-width: 48em){.css-4r1ia7{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;width:auto;margin-top:0px;}}</style><div class="chakra-stack css-4r1ia7"><style data-emotion="css fvhngf">.css-fvhngf{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;text-underline-offset:3px;padding:var(--chakra-space-2);color:var(--chakra-colors-whiteAlpha-900);}.css-fvhngf:hover,.css-fvhngf[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-fvhngf:focus-visible,.css-fvhngf[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-fvhngf" href="/writing">Posts</a><a class="chakra-link css-fvhngf" href="/papers">Projects</a><a class="chakra-link css-fvhngf" href="/research">Research</a><style data-emotion="css sy0160">.css-sy0160{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;text-underline-offset:3px;padding:var(--chakra-space-2);color:var(--chakra-colors-whiteAlpha-900);display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:var(--chakra-space-2);}.css-sy0160:hover,.css-sy0160[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-sy0160:focus-visible,.css-sy0160[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a target="_blank" class="chakra-link css-sy0160" style="gap:4px" href="https://github.com/zunyibrt"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M256 32C132.3 32 32 134.9 32 261.7c0 101.5 64.2 187.5 153.2 217.9a17.56 17.56 0 0 0 3.8.4c8.3 0 11.5-6.1 11.5-11.4 0-5.5-.2-19.9-.3-39.1a102.4 102.4 0 0 1-22.6 2.7c-43.1 0-52.9-33.5-52.9-33.5-10.2-26.5-24.9-33.6-24.9-33.6-19.5-13.7-.1-14.1 1.4-14.1h.1c22.5 2 34.3 23.8 34.3 23.8 11.2 19.6 26.2 25.1 39.6 25.1a63 63 0 0 0 25.6-6c2-14.8 7.8-24.9 14.2-30.7-49.7-5.8-102-25.5-102-113.5 0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8a18.64 18.64 0 0 1 5-.5c8.1 0 26.4 3.1 56.6 24.1a208.21 208.21 0 0 1 112.2 0c30.2-21 48.5-24.1 56.6-24.1a18.64 18.64 0 0 1 5 .5c12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6 0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5 0 30.7-.3 55.5-.3 63 0 5.4 3.1 11.5 11.4 11.5a19.35 19.35 0 0 0 4-.4C415.9 449.2 480 363.1 480 261.7 480 134.9 379.7 32 256 32z"></path></svg></a></div><style data-emotion="css 1rr4qq7">.css-1rr4qq7{-webkit-flex:1;-ms-flex:1;flex:1;}</style><div align="right" class="css-1rr4qq7"><div style="display:inline-block;opacity:1;will-change:transform,opacity;transform:none"><style data-emotion="css j7xjat">.css-j7xjat{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:relative;white-space:nowrap;vertical-align:middle;outline:2px solid transparent;outline-offset:2px;line-height:1.2;border-radius:var(--chakra-radii-md);font-weight:var(--chakra-fontWeights-semibold);transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);height:var(--chakra-sizes-10);min-width:var(--chakra-sizes-10);font-size:var(--chakra-fontSizes-md);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);background:var(--chakra-colors-orange-200);color:var(--chakra-colors-gray-800);padding:0px;}.css-j7xjat:focus-visible,.css-j7xjat[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}.css-j7xjat:disabled,.css-j7xjat[disabled],.css-j7xjat[aria-disabled=true],.css-j7xjat[data-disabled]{opacity:0.4;cursor:not-allowed;box-shadow:var(--chakra-shadows-none);}.css-j7xjat:hover,.css-j7xjat[data-hover]{background:var(--chakra-colors-orange-300);}.css-j7xjat:hover:disabled,.css-j7xjat[data-hover]:disabled,.css-j7xjat:hover[disabled],.css-j7xjat[data-hover][disabled],.css-j7xjat:hover[aria-disabled=true],.css-j7xjat[data-hover][aria-disabled=true],.css-j7xjat:hover[data-disabled],.css-j7xjat[data-hover][data-disabled]{background:var(--chakra-colors-orange-200);}.css-j7xjat:active,.css-j7xjat[data-active]{background:var(--chakra-colors-orange-400);}</style><button type="button" class="chakra-button css-j7xjat" aria-label="Toggle theme"><style data-emotion="css onkibi">.css-onkibi{width:1em;height:1em;display:inline-block;line-height:1em;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:currentColor;vertical-align:middle;}</style><svg viewBox="0 0 24 24" focusable="false" class="chakra-icon css-onkibi" aria-hidden="true"><g stroke-linejoin="round" stroke-linecap="round" stroke-width="2" fill="none" stroke="currentColor"><circle cx="12" cy="12" r="5"></circle><path d="M12 1v2"></path><path d="M12 21v2"></path><path d="M4.22 4.22l1.42 1.42"></path><path d="M18.36 18.36l1.42 1.42"></path><path d="M1 12h2"></path><path d="M21 12h2"></path><path d="M4.22 19.78l1.42-1.42"></path><path d="M18.36 5.64l1.42-1.42"></path></g></svg></button></div><style data-emotion="css 1q8cyil">.css-1q8cyil{margin-left:var(--chakra-space-2);display:inline-block;}@media screen and (min-width: 48em){.css-1q8cyil{display:none;}}</style><div class="css-1q8cyil"><style data-emotion="css 1wjcgnw">.css-1wjcgnw{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:relative;white-space:nowrap;vertical-align:middle;outline:2px solid transparent;outline-offset:2px;line-height:1.2;border-radius:var(--chakra-radii-md);font-weight:var(--chakra-fontWeights-semibold);transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);height:var(--chakra-sizes-10);min-width:var(--chakra-sizes-10);font-size:var(--chakra-fontSizes-md);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);border:1px solid;border-color:var(--chakra-colors-whiteAlpha-300);color:var(--chakra-colors-whiteAlpha-900);padding:0px;}.css-1wjcgnw:focus-visible,.css-1wjcgnw[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}.css-1wjcgnw:disabled,.css-1wjcgnw[disabled],.css-1wjcgnw[aria-disabled=true],.css-1wjcgnw[data-disabled]{opacity:0.4;cursor:not-allowed;box-shadow:var(--chakra-shadows-none);}.css-1wjcgnw:hover,.css-1wjcgnw[data-hover]{background:var(--chakra-colors-whiteAlpha-200);}.css-1wjcgnw:hover:disabled,.css-1wjcgnw[data-hover]:disabled,.css-1wjcgnw:hover[disabled],.css-1wjcgnw[data-hover][disabled],.css-1wjcgnw:hover[aria-disabled=true],.css-1wjcgnw[data-hover][aria-disabled=true],.css-1wjcgnw:hover[data-disabled],.css-1wjcgnw[data-hover][data-disabled]{background:initial;}.chakra-button__group[data-attached][data-orientation=horizontal]>.css-1wjcgnw:not(:last-of-type){-webkit-margin-end:-1px;margin-inline-end:-1px;}.chakra-button__group[data-attached][data-orientation=vertical]>.css-1wjcgnw:not(:last-of-type){margin-bottom:-1px;}.css-1wjcgnw:active,.css-1wjcgnw[data-active]{background:var(--chakra-colors-whiteAlpha-300);}</style><button type="button" class="chakra-button chakra-menu__menu-button css-1wjcgnw" aria-label="Options" id="menu-button-navbar-menu" aria-expanded="false" aria-haspopup="menu" aria-controls="menu-list-navbar-menu"><svg viewBox="0 0 24 24" focusable="false" class="chakra-icon css-onkibi" aria-hidden="true"><path fill="currentColor" d="M 3 5 A 1.0001 1.0001 0 1 0 3 7 L 21 7 A 1.0001 1.0001 0 1 0 21 5 L 3 5 z M 3 11 A 1.0001 1.0001 0 1 0 3 13 L 21 13 A 1.0001 1.0001 0 1 0 21 11 L 3 11 z M 3 17 A 1.0001 1.0001 0 1 0 3 19 L 21 19 A 1.0001 1.0001 0 1 0 21 17 L 3 17 z"></path></svg></button><style data-emotion="css r6z5ec">.css-r6z5ec{z-index:1;}</style><div style="visibility:hidden;position:absolute;min-width:max-content;inset:0 auto auto 0" class="css-r6z5ec"><style data-emotion="css 1kfu8nn">.css-1kfu8nn{outline:2px solid transparent;outline-offset:2px;--menu-bg:#fff;--menu-shadow:var(--chakra-shadows-sm);color:inherit;min-width:var(--chakra-sizes-3xs);padding-top:var(--chakra-space-2);padding-bottom:var(--chakra-space-2);z-index:1;border-radius:var(--chakra-radii-md);border-width:1px;background:var(--menu-bg);box-shadow:var(--menu-shadow);}.chakra-ui-dark .css-1kfu8nn:not([data-theme]),[data-theme=dark] .css-1kfu8nn:not([data-theme]),.css-1kfu8nn[data-theme=dark]{--menu-bg:var(--chakra-colors-gray-700);--menu-shadow:var(--chakra-shadows-dark-lg);}</style><div class="chakra-menu__menu-list css-1kfu8nn" tabindex="-1" role="menu" id="menu-list-navbar-menu" aria-orientation="vertical" style="transform-origin:var(--popper-transform-origin);opacity:0;visibility:hidden;transform:scale(0.8)"></div></div></div></div></div></nav><style data-emotion="css 11nbm5x">.css-11nbm5x{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);max-width:var(--chakra-sizes-container-md);padding-top:var(--chakra-space-14);}</style><div class="chakra-container css-11nbm5x"><div class="css-0"><style data-emotion="css 130gl9x">.css-130gl9x{text-align:center;margin-bottom:var(--chakra-space-1);}</style><div class="css-130gl9x"><style data-emotion="css 14076z4">.css-14076z4{font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-4xl);line-height:1.2;margin-bottom:var(--chakra-space-4);margin-top:var(--chakra-space-5);font-family:Arial;}@media screen and (min-width: 48em){.css-14076z4{font-size:var(--chakra-fontSizes-5xl);line-height:1;}}</style><h1 class="chakra-heading css-14076z4">Building GPT from Scratch - Part 1: Data Loading and the Bigram Model</h1><style data-emotion="css q9k0mw">.css-q9k0mw{color:var(--chakra-colors-gray-500);}</style><p class="chakra-text css-q9k0mw">Aug 4, 2025</p><style data-emotion="css 14v6z45">.css-14v6z45{opacity:0.6;border:0;border-style:solid;border-bottom-width:1px;width:100%;margin-top:var(--chakra-space-4);margin-bottom:var(--chakra-space-4);border-color:var(--chakra-colors-gray-600);}</style><hr aria-orientation="horizontal" class="chakra-divider css-14v6z45"/></div><style data-emotion="css 1u5ktcn">.css-1u5ktcn{max-width:800px;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;padding:var(--chakra-space-6);}</style><div class="css-1u5ktcn"><style data-emotion="css 6gleai">.css-6gleai{margin-bottom:var(--chakra-space-4);line-height:1.7;}</style><p class="chakra-text css-6gleai"><em>This is Part 1 of a 3-part series on building a Generative Pretrained Transformer from scratch, based on Andrej Karpathy&#x27;s excellent tutorial.</em></p>
<style data-emotion="css ebdw0u">.css-ebdw0u{font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-3xl);line-height:1.33;margin-bottom:var(--chakra-space-4);margin-top:var(--chakra-space-6);font-family:Arial;}@media screen and (min-width: 48em){.css-ebdw0u{font-size:var(--chakra-fontSizes-4xl);line-height:1.2;}}</style><h2 class="chakra-heading css-ebdw0u">Introduction</h2>
<p class="chakra-text css-6gleai">The goal of this series is to build a &#x27;Generative Pretrained Transformer&#x27; from scratch, following the attention architecture from the original 2017 <style data-emotion="css 1qdxmbw">.css-1qdxmbw{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:#ff63c3;text-underline-offset:3px;}.css-1qdxmbw:hover,.css-1qdxmbw[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-1qdxmbw:focus-visible,.css-1qdxmbw[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a target="_blank" rel="noopener" class="chakra-link css-1qdxmbw" href="https://arxiv.org/abs/1706.03762"><strong>Attention is all you need</strong></a> paper and OpenAI&#x27;s GPT-2. We&#x27;ll be building a character-level generative model - essentially a next character predictor that can generate Shakespeare-like text.</p>
<p class="chakra-text css-6gleai">This tutorial is based on Andrej Karpathy&#x27;s video <a target="_blank" rel="noopener" class="chakra-link css-1qdxmbw" href="https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;t=10s"><strong>Let&#x27;s build GPT: from scratch, in code, spelled out</strong></a>, which has gained 6 million views in 2 years! Transformers get their name from their original use case of machine translation.</p>
<h2 class="chakra-heading css-ebdw0u">Data Loading</h2>
<p class="chakra-text css-6gleai">The first step in building any language model is preparing our data. We&#x27;ll use the tiny Shakespeare dataset for this tutorial.</p>
<pre><style data-emotion="css 1t1ewm6">.css-1t1ewm6{font-family:var(--chakra-fonts-mono);font-size:var(--chakra-fontSizes-sm);-webkit-padding-start:0.2em;padding-inline-start:0.2em;-webkit-padding-end:0.2em;padding-inline-end:0.2em;box-shadow:var(--badge-shadow);--badge-bg:var(--chakra-colors-gray-100);--badge-color:var(--chakra-colors-gray-800);display:block;padding:var(--chakra-space-4);background:var(--chakra-colors-gray-900);color:var(--chakra-colors-white);border-radius:var(--chakra-radii-md);overflow:auto;margin-bottom:var(--chakra-space-4);}.chakra-ui-dark .css-1t1ewm6:not([data-theme]),[data-theme=dark] .css-1t1ewm6:not([data-theme]),.css-1t1ewm6[data-theme=dark]{--badge-bg:rgba(226, 232, 240, 0.16);--badge-color:var(--chakra-colors-gray-200);}</style><code class="chakra-code css-1t1ewm6"><span class="hljs-comment"># Download tiny shakespeare dataset</span>
!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/<span class="hljs-built_in">input</span>.txt

<span class="hljs-comment"># read and inspect the file</span>
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;input.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:
  text = f.read()
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Length of dataset in characters: &quot;</span>, <span class="hljs-built_in">len</span>(text))
<span class="hljs-built_in">print</span>(text[:<span class="hljs-number">1000</span>])
</code></pre>
<style data-emotion="css 2k8uwk">.css-2k8uwk{font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-2xl);line-height:1.33;margin-bottom:var(--chakra-space-3);margin-top:var(--chakra-space-5);font-family:Arial;}@media screen and (min-width: 48em){.css-2k8uwk{font-size:var(--chakra-fontSizes-3xl);line-height:1.2;}}</style><h3 class="chakra-heading css-2k8uwk">Tokenization</h3>
<p class="chakra-text css-6gleai">The first interesting concept is <strong>tokenization</strong>. We want to convert our input text into integers that our model can work with. The model will predict the next integer, which we can then decode back into a character.</p>
<p class="chakra-text css-6gleai">Our simple approach creates a dictionary of all available characters (including newline) and assigns each an integer. For more sophisticated treatments, you&#x27;d want to look at SentencePiece by Google or tiktoken by OpenAI. There&#x27;s always a tradeoff between dictionary size and encoding lengths.</p>
<pre><code class="chakra-code css-1t1ewm6">chars = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(text)))
vocab_size = <span class="hljs-built_in">len</span>(chars)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{vocab_size}</span> characters in dictionary : <span class="hljs-subst">{<span class="hljs-string">&#x27;&#x27;</span>.join(chars)}</span>&quot;</span>)
<span class="hljs-comment"># First one is a new line!</span>

<span class="hljs-comment"># Create a mapping from characters to integers (A Tokenizer)</span>
<span class="hljs-comment"># This is called tokenizing. Google uses SentencePiece. OpenAI and GPT uses tiktoken</span>
itos = { i:ch <span class="hljs-keyword">for</span> i, ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(chars)}
stoi = { ch:i <span class="hljs-keyword">for</span> i, ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(chars)}
encode = <span class="hljs-keyword">lambda</span> s: [stoi[c] <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> s] <span class="hljs-comment"># encoder: convert string to list of integers</span>
decode = <span class="hljs-keyword">lambda</span> l: <span class="hljs-string">&#x27;&#x27;</span>.join([itos[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> l]) <span class="hljs-comment"># decoder: convert list of integers to string</span>

<span class="hljs-comment"># Now encode the whole text dataset and store it as a pytorch Tensor</span>
<span class="hljs-keyword">import</span> torch
data = torch.tensor(encode(text), dtype=torch.long)
<span class="hljs-built_in">print</span>(data.shape, data.dtype)
<span class="hljs-built_in">print</span>(data[:<span class="hljs-number">100</span>])
<span class="hljs-built_in">print</span>(decode([<span class="hljs-built_in">int</span>(v) <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> (data[:<span class="hljs-number">100</span>].data)]))

<span class="hljs-comment"># Split the data into training and validation sets</span>
n = <span class="hljs-built_in">int</span>(<span class="hljs-number">0.9</span>*<span class="hljs-built_in">len</span>(data))
train_data = data[:n]
val_data = data[n:]
</code></pre>
<h3 class="chakra-heading css-2k8uwk">Block Size and Context Windows</h3>
<p class="chakra-text css-6gleai">The second important concept is <strong>block size</strong>, which sets the context window length. This is the size of the &#x27;memory&#x27; of the model. A larger block size would give more long-range associations, but is probably more expensive to train and would require more data.</p>
<p class="chakra-text css-6gleai">With block size defined, we can input batch_size rows of block_size integers for each data batch. Training in batches is faster since multiple independent blocks can be processed simultaneously.</p>
<pre><code class="chakra-code css-1t1ewm6">block_size = <span class="hljs-number">8</span> <span class="hljs-comment"># or alternatively context lengths</span>
train_data[:block_size+<span class="hljs-number">1</span>]
<span class="hljs-comment"># in a sample of 9 characters, we have 8 examples with different context lengths.</span>

torch.manual_seed(<span class="hljs-number">1337</span>)
batch_size = <span class="hljs-number">4</span>
block_size = <span class="hljs-number">8</span>

<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch</span>(<span class="hljs-params">split</span>):
  <span class="hljs-comment"># generate a small batch of data of inputs x and targets y</span>
  data = train_data <span class="hljs-keyword">if</span> split == <span class="hljs-string">&#x27;train&#x27;</span> <span class="hljs-keyword">else</span> val_data
  ix = torch.randint(<span class="hljs-built_in">len</span>(data) - block_size , (batch_size,)) <span class="hljs-comment"># should have a -1</span>
  x = torch.stack([data[i:i+block_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ix])
  y = torch.stack([data[i+<span class="hljs-number">1</span>:i+<span class="hljs-number">1</span>+block_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ix])
  <span class="hljs-keyword">return</span> x, y
</code></pre>
<h2 class="chakra-heading css-ebdw0u">The Bigram Model</h2>
<p class="chakra-text css-6gleai">Now let&#x27;s implement our first model - a simple bigram model. This model predicts the next character based only on the current character (context length of 1) and stores probabilities for the next character.</p>
<p class="chakra-text css-6gleai">We use a log-likelihood/cross-entropy loss. Initially, we expect the loss to be around ln(1/65) â‰ˆ 4.17, since we have 65 characters and initially each character should be equally likely.</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F
torch.manual_seed(<span class="hljs-number">1337</span>)
<span class="hljs-keyword">from</span> einops <span class="hljs-keyword">import</span> rearrange

<span class="hljs-keyword">class</span> <span class="hljs-title class_">BigramLanguageModel</span>(nn.Module):

  <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size</span>):
    <span class="hljs-built_in">super</span>().__init__() <span class="hljs-comment"># initialize the parent class Module</span>
    <span class="hljs-comment"># each token directly reads off the logits for the next token from a lookup table</span>
    <span class="hljs-variable language_">self</span>.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

  <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, idx, targets=<span class="hljs-literal">None</span></span>):
    <span class="hljs-comment"># idx and targets and both (B,T) tensors of integers</span>
    logits = <span class="hljs-variable language_">self</span>.token_embedding_table(idx) <span class="hljs-comment"># (B,T,C) batch time channel</span>

    <span class="hljs-keyword">if</span> targets <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
      loss = <span class="hljs-literal">None</span>
    <span class="hljs-keyword">else</span>:
      logits = rearrange(logits,<span class="hljs-string">&#x27;b t c -&gt; (b t) c&#x27;</span>)
      targets = rearrange(targets, <span class="hljs-string">&#x27;b t -&gt; (b t)&#x27;</span>)
      loss = F.cross_entropy(logits, targets)

    <span class="hljs-keyword">return</span> logits, loss

  <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">self, idx, max_new_tokens</span>):
    <span class="hljs-comment"># idx is (B, T) array of indicies in the current context</span>
    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):
      <span class="hljs-comment"># get the predictions</span>
      logits, loss = <span class="hljs-variable language_">self</span>(idx)
      <span class="hljs-comment"># focus only on last time step</span>
      logits = logits[:, -<span class="hljs-number">1</span> , :] <span class="hljs-comment"># becomes B x C</span>
      <span class="hljs-comment"># apply softmax to get probabilties</span>
      probs = F.softmax(logits, dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># becomes B x C</span>
      <span class="hljs-comment"># sample from distribution</span>
      idx_next = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>) <span class="hljs-comment"># B x 1</span>
      <span class="hljs-comment"># append sampled index to running sequence</span>
      idx = torch.cat((idx, idx_next), dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># B x T+1</span>
    <span class="hljs-keyword">return</span> idx

m = BigramLanguageModel(vocab_size)
logits, loss = m(xb, yb)
<span class="hljs-built_in">print</span>(logits.shape)
<span class="hljs-built_in">print</span>(loss)

idx = torch.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), dtype=torch.long)
<span class="hljs-built_in">print</span>(decode(m.generate(idx, max_new_tokens=<span class="hljs-number">100</span>)[<span class="hljs-number">0</span>].tolist()))
</code></pre>
<p class="chakra-text css-6gleai">Note that for PyTorch, when we subclass &#x27;nn.Module&#x27;, calling the model (including &#x27;self&#x27;) actually calls the &#x27;forward&#x27; method!</p>
<h2 class="chakra-heading css-ebdw0u">Training the Bigram Model</h2>
<p class="chakra-text css-6gleai">We can train our model by choosing an <strong>optimizer</strong> and a <strong>learning rate</strong>. A good learning rate is typically 1e-3, but for smaller networks you can get away with higher rates.</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-comment"># training</span>
batch_size = <span class="hljs-number">32</span>

optimizer = torch.optim.AdamW(m.parameters(), lr=<span class="hljs-number">1e-3</span>)

<span class="hljs-keyword">for</span> steps <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):
  <span class="hljs-comment"># sample a batch of data</span>
  xb, yb = get_batch(<span class="hljs-string">&#x27;train&#x27;</span>)

  logits, loss = m(xb, yb)
  optimizer.zero_grad(set_to_none=<span class="hljs-literal">True</span>)
  loss.backward()
  optimizer.step()

<span class="hljs-built_in">print</span>(loss.item())
</code></pre>
<h2 class="chakra-heading css-ebdw0u">Results</h2>
<p class="chakra-text css-6gleai">Unfortunately, the results using the bigram model aren&#x27;t great:</p>
<pre><style data-emotion="css 1ul63h6">.css-1ul63h6{display:inline-block;font-family:var(--chakra-fonts-mono);font-size:var(--chakra-fontSizes-sm);box-shadow:var(--badge-shadow);--badge-bg:var(--chakra-colors-gray-100);--badge-color:var(--chakra-colors-gray-800);-webkit-padding-start:var(--chakra-space-2);padding-inline-start:var(--chakra-space-2);-webkit-padding-end:var(--chakra-space-2);padding-inline-end:var(--chakra-space-2);padding-top:0.95px;padding-bottom:0.95px;background:var(--chakra-colors-gray-900);color:var(--chakra-colors-white);border-radius:var(--chakra-radii-md);}.chakra-ui-dark .css-1ul63h6:not([data-theme]),[data-theme=dark] .css-1ul63h6:not([data-theme]),.css-1ul63h6[data-theme=dark]{--badge-bg:rgba(226, 232, 240, 0.16);--badge-color:var(--chakra-colors-gray-200);}</style><code class="chakra-code css-1ul63h6">Iyoteng h hasbe pave pirance
Rie hicomyonthar&#x27;s
Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo&#x27;dsssit ey
KIN d pe wither vouprrouthercc.
hathe; d!
My hind tt hinig t ouchos tes; st yo
# Not quite Shakespeare
</code></pre>
<p class="chakra-text css-6gleai">The bigram model is too simple - it can only look at one character back to predict the next. We need something more sophisticated that can consider longer context windows and learn more complex patterns.</p>
<h2 class="chakra-heading css-ebdw0u">What&#x27;s Next?</h2>
<p class="chakra-text css-6gleai">In the next part of this series, we&#x27;ll introduce the key mathematical trick that makes transformers possible: the attention mechanism. We&#x27;ll start with a mathematical foundation for weighted aggregation and build up to single-head self-attention.</p>
<p class="chakra-text css-6gleai">The bigram model gives us a solid foundation to build upon, and more importantly, it establishes our training pipeline and data handling. Now we&#x27;re ready to dive into the real magic of transformers!</p></div></div><style data-emotion="css 17cqjj4">.css-17cqjj4{opacity:0.4;font-size:var(--chakra-fontSizes-sm);}</style><div align="center" class="css-17cqjj4">Build based on <a href="https://www.craftz.dog/" target="_blank">Takuya Matsuyama</a>.</div></div></main><span></span><span id="__chakra_env" hidden=""></span></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"06_transformers_1","content":"\n*This is Part 1 of a 3-part series on building a Generative Pretrained Transformer from scratch, based on Andrej Karpathy's excellent tutorial.*\n\n## Introduction\n\nThe goal of this series is to build a 'Generative Pretrained Transformer' from scratch, following the attention architecture from the original 2017 [**Attention is all you need**](https://arxiv.org/abs/1706.03762) paper and OpenAI's GPT-2. We'll be building a character-level generative model - essentially a next character predictor that can generate Shakespeare-like text.\n\nThis tutorial is based on Andrej Karpathy's video [**Let's build GPT: from scratch, in code, spelled out**](https://www.youtube.com/watch?v=kCc8FmEb1nY\u0026t=10s), which has gained 6 million views in 2 years! Transformers get their name from their original use case of machine translation.\n\n## Data Loading\n\nThe first step in building any language model is preparing our data. We'll use the tiny Shakespeare dataset for this tutorial.\n\n```python\n# Download tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# read and inspect the file\nwith open('input.txt', 'r') as f:\n  text = f.read()\nprint(\"Length of dataset in characters: \", len(text))\nprint(text[:1000])\n```\n\n### Tokenization\n\nThe first interesting concept is **tokenization**. We want to convert our input text into integers that our model can work with. The model will predict the next integer, which we can then decode back into a character.\n\nOur simple approach creates a dictionary of all available characters (including newline) and assigns each an integer. For more sophisticated treatments, you'd want to look at SentencePiece by Google or tiktoken by OpenAI. There's always a tradeoff between dictionary size and encoding lengths.\n\n```python\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(f\"{vocab_size} characters in dictionary : {''.join(chars)}\")\n# First one is a new line!\n\n# Create a mapping from characters to integers (A Tokenizer)\n# This is called tokenizing. Google uses SentencePiece. OpenAI and GPT uses tiktoken\nitos = { i:ch for i, ch in enumerate(chars)}\nstoi = { ch:i for i, ch in enumerate(chars)}\nencode = lambda s: [stoi[c] for c in s] # encoder: convert string to list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: convert list of integers to string\n\n# Now encode the whole text dataset and store it as a pytorch Tensor\nimport torch\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:100])\nprint(decode([int(v) for v in (data[:100].data)]))\n\n# Split the data into training and validation sets\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]\n```\n\n### Block Size and Context Windows\n\nThe second important concept is **block size**, which sets the context window length. This is the size of the 'memory' of the model. A larger block size would give more long-range associations, but is probably more expensive to train and would require more data.\n\nWith block size defined, we can input batch_size rows of block_size integers for each data batch. Training in batches is faster since multiple independent blocks can be processed simultaneously.\n\n```python\nblock_size = 8 # or alternatively context lengths\ntrain_data[:block_size+1]\n# in a sample of 9 characters, we have 8 examples with different context lengths.\n\ntorch.manual_seed(1337)\nbatch_size = 4\nblock_size = 8\n\ndef get_batch(split):\n  # generate a small batch of data of inputs x and targets y\n  data = train_data if split == 'train' else val_data\n  ix = torch.randint(len(data) - block_size , (batch_size,)) # should have a -1\n  x = torch.stack([data[i:i+block_size] for i in ix])\n  y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n  return x, y\n```\n\n## The Bigram Model\n\nNow let's implement our first model - a simple bigram model. This model predicts the next character based only on the current character (context length of 1) and stores probabilities for the next character.\n\nWe use a log-likelihood/cross-entropy loss. Initially, we expect the loss to be around ln(1/65) â‰ˆ 4.17, since we have 65 characters and initially each character should be equally likely.\n\n```python\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\nfrom einops import rearrange\n\nclass BigramLanguageModel(nn.Module):\n\n  def __init__(self, vocab_size):\n    super().__init__() # initialize the parent class Module\n    # each token directly reads off the logits for the next token from a lookup table\n    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n  def forward(self, idx, targets=None):\n    # idx and targets and both (B,T) tensors of integers\n    logits = self.token_embedding_table(idx) # (B,T,C) batch time channel\n\n    if targets is None:\n      loss = None\n    else:\n      logits = rearrange(logits,'b t c -\u003e (b t) c')\n      targets = rearrange(targets, 'b t -\u003e (b t)')\n      loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\n  def generate(self, idx, max_new_tokens):\n    # idx is (B, T) array of indicies in the current context\n    for _ in range(max_new_tokens):\n      # get the predictions\n      logits, loss = self(idx)\n      # focus only on last time step\n      logits = logits[:, -1 , :] # becomes B x C\n      # apply softmax to get probabilties\n      probs = F.softmax(logits, dim=1) # becomes B x C\n      # sample from distribution\n      idx_next = torch.multinomial(probs, num_samples=1) # B x 1\n      # append sampled index to running sequence\n      idx = torch.cat((idx, idx_next), dim=1) # B x T+1\n    return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nidx = torch.zeros((1,1), dtype=torch.long)\nprint(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n```\n\nNote that for PyTorch, when we subclass 'nn.Module', calling the model (including 'self') actually calls the 'forward' method!\n\n## Training the Bigram Model\n\nWe can train our model by choosing an **optimizer** and a **learning rate**. A good learning rate is typically 1e-3, but for smaller networks you can get away with higher rates.\n\n```python\n# training\nbatch_size = 32\n\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nfor steps in range(10000):\n  # sample a batch of data\n  xb, yb = get_batch('train')\n\n  logits, loss = m(xb, yb)\n  optimizer.zero_grad(set_to_none=True)\n  loss.backward()\n  optimizer.step()\n\nprint(loss.item())\n```\n\n## Results\n\nUnfortunately, the results using the bigram model aren't great:\n\n```\nIyoteng h hasbe pave pirance\nRie hicomyonthar's\nPlinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\nKIN d pe wither vouprrouthercc.\nhathe; d!\nMy hind tt hinig t ouchos tes; st yo\n# Not quite Shakespeare\n```\n\nThe bigram model is too simple - it can only look at one character back to predict the next. We need something more sophisticated that can consider longer context windows and learn more complex patterns.\n\n## What's Next?\n\nIn the next part of this series, we'll introduce the key mathematical trick that makes transformers possible: the attention mechanism. We'll start with a mathematical foundation for weighted aggregation and build up to single-head self-attention.\n\nThe bigram model gives us a solid foundation to build upon, and more importantly, it establishes our training pipeline and data handling. Now we're ready to dive into the real magic of transformers!","title":"Building GPT from Scratch - Part 1: Data Loading and the Bigram Model","date":"2025-08-04","excerpt":"","tags":["transformers","tutorial"]}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"06_transformers_1"},"buildId":"bUrBysIoWJKwE5ZkPd0HO","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>