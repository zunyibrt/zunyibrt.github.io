<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="description" content="Brent&#x27;s homepage"/><meta name="author" content="Brent Tan"/><title>Brent Tan - Homepage</title><meta name="next-head-count" content="5"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/e9556718e3f0f4cf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e9556718e3f0f4cf.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-c7e8ec73d08c57f3.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-676fc21436baeb6f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-cb76b27212f05dea.js" defer=""></script><script src="/_next/static/chunks/175675d1-254dc21e030ba2ac.js" defer=""></script><script src="/_next/static/chunks/645-40e6d947d4f7bb86.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-9dc0ab71c3bdc569.js" defer=""></script><script src="/_next/static/bUrBysIoWJKwE5ZkPd0HO/_buildManifest.js" defer=""></script><script src="/_next/static/bUrBysIoWJKwE5ZkPd0HO/_ssgManifest.js" defer=""></script><style id="__jsx-fed65fd97a038da">@import url("https://fonts.googleapis.com/css2?family=M+PLUS+Rounded+1c:wght@300;700&display=swap");@import url("https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@100..900&display=swap");</style></head><body><script id="chakra-script">!(function(){try{var a=function(c){var v="(prefers-color-scheme: dark)",h=window.matchMedia(v).matches?"dark":"light",r=c==="system"?h:c,o=document.documentElement,s=document.body,l="chakra-ui-light",d="chakra-ui-dark",i=r==="dark";return s.classList.add(i?d:l),s.classList.remove(i?l:d),o.style.colorScheme=r,o.dataset.theme=r,r},n=a,m="dark",e="chakra-ui-color-mode",t=localStorage.getItem(e);t?a(t):localStorage.setItem(e,a(m))}catch(a){}})();</script><div id="__next"><style data-emotion="css-global 93m1n8">:host,:root,[data-theme]{--chakra-ring-inset:var(--chakra-empty,/*!*/ /*!*/);--chakra-ring-offset-width:0px;--chakra-ring-offset-color:#fff;--chakra-ring-color:rgba(66, 153, 225, 0.6);--chakra-ring-offset-shadow:0 0 #0000;--chakra-ring-shadow:0 0 #0000;--chakra-space-x-reverse:0;--chakra-space-y-reverse:0;--chakra-colors-transparent:transparent;--chakra-colors-current:currentColor;--chakra-colors-black:#000000;--chakra-colors-white:#FFFFFF;--chakra-colors-whiteAlpha-50:rgba(255, 255, 255, 0.04);--chakra-colors-whiteAlpha-100:rgba(255, 255, 255, 0.06);--chakra-colors-whiteAlpha-200:rgba(255, 255, 255, 0.08);--chakra-colors-whiteAlpha-300:rgba(255, 255, 255, 0.16);--chakra-colors-whiteAlpha-400:rgba(255, 255, 255, 0.24);--chakra-colors-whiteAlpha-500:rgba(255, 255, 255, 0.36);--chakra-colors-whiteAlpha-600:rgba(255, 255, 255, 0.48);--chakra-colors-whiteAlpha-700:rgba(255, 255, 255, 0.64);--chakra-colors-whiteAlpha-800:rgba(255, 255, 255, 0.80);--chakra-colors-whiteAlpha-900:rgba(255, 255, 255, 0.92);--chakra-colors-blackAlpha-50:rgba(0, 0, 0, 0.04);--chakra-colors-blackAlpha-100:rgba(0, 0, 0, 0.06);--chakra-colors-blackAlpha-200:rgba(0, 0, 0, 0.08);--chakra-colors-blackAlpha-300:rgba(0, 0, 0, 0.16);--chakra-colors-blackAlpha-400:rgba(0, 0, 0, 0.24);--chakra-colors-blackAlpha-500:rgba(0, 0, 0, 0.36);--chakra-colors-blackAlpha-600:rgba(0, 0, 0, 0.48);--chakra-colors-blackAlpha-700:rgba(0, 0, 0, 0.64);--chakra-colors-blackAlpha-800:rgba(0, 0, 0, 0.80);--chakra-colors-blackAlpha-900:rgba(0, 0, 0, 0.92);--chakra-colors-gray-50:#F7FAFC;--chakra-colors-gray-100:#EDF2F7;--chakra-colors-gray-200:#E2E8F0;--chakra-colors-gray-300:#CBD5E0;--chakra-colors-gray-400:#A0AEC0;--chakra-colors-gray-500:#718096;--chakra-colors-gray-600:#4A5568;--chakra-colors-gray-700:#2D3748;--chakra-colors-gray-800:#1A202C;--chakra-colors-gray-900:#171923;--chakra-colors-red-50:#FFF5F5;--chakra-colors-red-100:#FED7D7;--chakra-colors-red-200:#FEB2B2;--chakra-colors-red-300:#FC8181;--chakra-colors-red-400:#F56565;--chakra-colors-red-500:#E53E3E;--chakra-colors-red-600:#C53030;--chakra-colors-red-700:#9B2C2C;--chakra-colors-red-800:#822727;--chakra-colors-red-900:#63171B;--chakra-colors-orange-50:#FFFAF0;--chakra-colors-orange-100:#FEEBC8;--chakra-colors-orange-200:#FBD38D;--chakra-colors-orange-300:#F6AD55;--chakra-colors-orange-400:#ED8936;--chakra-colors-orange-500:#DD6B20;--chakra-colors-orange-600:#C05621;--chakra-colors-orange-700:#9C4221;--chakra-colors-orange-800:#7B341E;--chakra-colors-orange-900:#652B19;--chakra-colors-yellow-50:#FFFFF0;--chakra-colors-yellow-100:#FEFCBF;--chakra-colors-yellow-200:#FAF089;--chakra-colors-yellow-300:#F6E05E;--chakra-colors-yellow-400:#ECC94B;--chakra-colors-yellow-500:#D69E2E;--chakra-colors-yellow-600:#B7791F;--chakra-colors-yellow-700:#975A16;--chakra-colors-yellow-800:#744210;--chakra-colors-yellow-900:#5F370E;--chakra-colors-green-50:#F0FFF4;--chakra-colors-green-100:#C6F6D5;--chakra-colors-green-200:#9AE6B4;--chakra-colors-green-300:#68D391;--chakra-colors-green-400:#48BB78;--chakra-colors-green-500:#38A169;--chakra-colors-green-600:#2F855A;--chakra-colors-green-700:#276749;--chakra-colors-green-800:#22543D;--chakra-colors-green-900:#1C4532;--chakra-colors-teal-50:#E6FFFA;--chakra-colors-teal-100:#B2F5EA;--chakra-colors-teal-200:#81E6D9;--chakra-colors-teal-300:#4FD1C5;--chakra-colors-teal-400:#38B2AC;--chakra-colors-teal-500:#319795;--chakra-colors-teal-600:#2C7A7B;--chakra-colors-teal-700:#285E61;--chakra-colors-teal-800:#234E52;--chakra-colors-teal-900:#1D4044;--chakra-colors-blue-50:#ebf8ff;--chakra-colors-blue-100:#bee3f8;--chakra-colors-blue-200:#90cdf4;--chakra-colors-blue-300:#63b3ed;--chakra-colors-blue-400:#4299e1;--chakra-colors-blue-500:#3182ce;--chakra-colors-blue-600:#2b6cb0;--chakra-colors-blue-700:#2c5282;--chakra-colors-blue-800:#2a4365;--chakra-colors-blue-900:#1A365D;--chakra-colors-cyan-50:#EDFDFD;--chakra-colors-cyan-100:#C4F1F9;--chakra-colors-cyan-200:#9DECF9;--chakra-colors-cyan-300:#76E4F7;--chakra-colors-cyan-400:#0BC5EA;--chakra-colors-cyan-500:#00B5D8;--chakra-colors-cyan-600:#00A3C4;--chakra-colors-cyan-700:#0987A0;--chakra-colors-cyan-800:#086F83;--chakra-colors-cyan-900:#065666;--chakra-colors-purple-50:#FAF5FF;--chakra-colors-purple-100:#E9D8FD;--chakra-colors-purple-200:#D6BCFA;--chakra-colors-purple-300:#B794F4;--chakra-colors-purple-400:#9F7AEA;--chakra-colors-purple-500:#805AD5;--chakra-colors-purple-600:#6B46C1;--chakra-colors-purple-700:#553C9A;--chakra-colors-purple-800:#44337A;--chakra-colors-purple-900:#322659;--chakra-colors-pink-50:#FFF5F7;--chakra-colors-pink-100:#FED7E2;--chakra-colors-pink-200:#FBB6CE;--chakra-colors-pink-300:#F687B3;--chakra-colors-pink-400:#ED64A6;--chakra-colors-pink-500:#D53F8C;--chakra-colors-pink-600:#B83280;--chakra-colors-pink-700:#97266D;--chakra-colors-pink-800:#702459;--chakra-colors-pink-900:#521B41;--chakra-colors-linkedin-50:#E8F4F9;--chakra-colors-linkedin-100:#CFEDFB;--chakra-colors-linkedin-200:#9BDAF3;--chakra-colors-linkedin-300:#68C7EC;--chakra-colors-linkedin-400:#34B3E4;--chakra-colors-linkedin-500:#00A0DC;--chakra-colors-linkedin-600:#008CC9;--chakra-colors-linkedin-700:#0077B5;--chakra-colors-linkedin-800:#005E93;--chakra-colors-linkedin-900:#004471;--chakra-colors-facebook-50:#E8F4F9;--chakra-colors-facebook-100:#D9DEE9;--chakra-colors-facebook-200:#B7C2DA;--chakra-colors-facebook-300:#6482C0;--chakra-colors-facebook-400:#4267B2;--chakra-colors-facebook-500:#385898;--chakra-colors-facebook-600:#314E89;--chakra-colors-facebook-700:#29487D;--chakra-colors-facebook-800:#223B67;--chakra-colors-facebook-900:#1E355B;--chakra-colors-messenger-50:#D0E6FF;--chakra-colors-messenger-100:#B9DAFF;--chakra-colors-messenger-200:#A2CDFF;--chakra-colors-messenger-300:#7AB8FF;--chakra-colors-messenger-400:#2E90FF;--chakra-colors-messenger-500:#0078FF;--chakra-colors-messenger-600:#0063D1;--chakra-colors-messenger-700:#0052AC;--chakra-colors-messenger-800:#003C7E;--chakra-colors-messenger-900:#002C5C;--chakra-colors-whatsapp-50:#dffeec;--chakra-colors-whatsapp-100:#b9f5d0;--chakra-colors-whatsapp-200:#90edb3;--chakra-colors-whatsapp-300:#65e495;--chakra-colors-whatsapp-400:#3cdd78;--chakra-colors-whatsapp-500:#22c35e;--chakra-colors-whatsapp-600:#179848;--chakra-colors-whatsapp-700:#0c6c33;--chakra-colors-whatsapp-800:#01421c;--chakra-colors-whatsapp-900:#001803;--chakra-colors-twitter-50:#E5F4FD;--chakra-colors-twitter-100:#C8E9FB;--chakra-colors-twitter-200:#A8DCFA;--chakra-colors-twitter-300:#83CDF7;--chakra-colors-twitter-400:#57BBF5;--chakra-colors-twitter-500:#1DA1F2;--chakra-colors-twitter-600:#1A94DA;--chakra-colors-twitter-700:#1681BF;--chakra-colors-twitter-800:#136B9E;--chakra-colors-twitter-900:#0D4D71;--chakra-colors-telegram-50:#E3F2F9;--chakra-colors-telegram-100:#C5E4F3;--chakra-colors-telegram-200:#A2D4EC;--chakra-colors-telegram-300:#7AC1E4;--chakra-colors-telegram-400:#47A9DA;--chakra-colors-telegram-500:#0088CC;--chakra-colors-telegram-600:#007AB8;--chakra-colors-telegram-700:#006BA1;--chakra-colors-telegram-800:#005885;--chakra-colors-telegram-900:#003F5E;--chakra-colors-grassTeal:#88ccca;--chakra-borders-none:0;--chakra-borders-1px:1px solid;--chakra-borders-2px:2px solid;--chakra-borders-4px:4px solid;--chakra-borders-8px:8px solid;--chakra-fonts-heading:'M PLUS Rounded 1c';--chakra-fonts-body:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--chakra-fonts-mono:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--chakra-fontSizes-3xs:0.45rem;--chakra-fontSizes-2xs:0.625rem;--chakra-fontSizes-xs:0.75rem;--chakra-fontSizes-sm:0.875rem;--chakra-fontSizes-md:1rem;--chakra-fontSizes-lg:1.125rem;--chakra-fontSizes-xl:1.25rem;--chakra-fontSizes-2xl:1.5rem;--chakra-fontSizes-3xl:1.875rem;--chakra-fontSizes-4xl:2.25rem;--chakra-fontSizes-5xl:3rem;--chakra-fontSizes-6xl:3.75rem;--chakra-fontSizes-7xl:4.5rem;--chakra-fontSizes-8xl:6rem;--chakra-fontSizes-9xl:8rem;--chakra-fontWeights-hairline:100;--chakra-fontWeights-thin:200;--chakra-fontWeights-light:300;--chakra-fontWeights-normal:400;--chakra-fontWeights-medium:500;--chakra-fontWeights-semibold:600;--chakra-fontWeights-bold:700;--chakra-fontWeights-extrabold:800;--chakra-fontWeights-black:900;--chakra-letterSpacings-tighter:-0.05em;--chakra-letterSpacings-tight:-0.025em;--chakra-letterSpacings-normal:0;--chakra-letterSpacings-wide:0.025em;--chakra-letterSpacings-wider:0.05em;--chakra-letterSpacings-widest:0.1em;--chakra-lineHeights-3:.75rem;--chakra-lineHeights-4:1rem;--chakra-lineHeights-5:1.25rem;--chakra-lineHeights-6:1.5rem;--chakra-lineHeights-7:1.75rem;--chakra-lineHeights-8:2rem;--chakra-lineHeights-9:2.25rem;--chakra-lineHeights-10:2.5rem;--chakra-lineHeights-normal:normal;--chakra-lineHeights-none:1;--chakra-lineHeights-shorter:1.25;--chakra-lineHeights-short:1.375;--chakra-lineHeights-base:1.5;--chakra-lineHeights-tall:1.625;--chakra-lineHeights-taller:2;--chakra-radii-none:0;--chakra-radii-sm:0.125rem;--chakra-radii-base:0.25rem;--chakra-radii-md:0.375rem;--chakra-radii-lg:0.5rem;--chakra-radii-xl:0.75rem;--chakra-radii-2xl:1rem;--chakra-radii-3xl:1.5rem;--chakra-radii-full:9999px;--chakra-space-1:0.25rem;--chakra-space-2:0.5rem;--chakra-space-3:0.75rem;--chakra-space-4:1rem;--chakra-space-5:1.25rem;--chakra-space-6:1.5rem;--chakra-space-7:1.75rem;--chakra-space-8:2rem;--chakra-space-9:2.25rem;--chakra-space-10:2.5rem;--chakra-space-12:3rem;--chakra-space-14:3.5rem;--chakra-space-16:4rem;--chakra-space-20:5rem;--chakra-space-24:6rem;--chakra-space-28:7rem;--chakra-space-32:8rem;--chakra-space-36:9rem;--chakra-space-40:10rem;--chakra-space-44:11rem;--chakra-space-48:12rem;--chakra-space-52:13rem;--chakra-space-56:14rem;--chakra-space-60:15rem;--chakra-space-64:16rem;--chakra-space-72:18rem;--chakra-space-80:20rem;--chakra-space-96:24rem;--chakra-space-px:1px;--chakra-space-0-5:0.125rem;--chakra-space-1-5:0.375rem;--chakra-space-2-5:0.625rem;--chakra-space-3-5:0.875rem;--chakra-shadows-xs:0 0 0 1px rgba(0, 0, 0, 0.05);--chakra-shadows-sm:0 1px 2px 0 rgba(0, 0, 0, 0.05);--chakra-shadows-base:0 1px 3px 0 rgba(0, 0, 0, 0.1),0 1px 2px 0 rgba(0, 0, 0, 0.06);--chakra-shadows-md:0 4px 6px -1px rgba(0, 0, 0, 0.1),0 2px 4px -1px rgba(0, 0, 0, 0.06);--chakra-shadows-lg:0 10px 15px -3px rgba(0, 0, 0, 0.1),0 4px 6px -2px rgba(0, 0, 0, 0.05);--chakra-shadows-xl:0 20px 25px -5px rgba(0, 0, 0, 0.1),0 10px 10px -5px rgba(0, 0, 0, 0.04);--chakra-shadows-2xl:0 25px 50px -12px rgba(0, 0, 0, 0.25);--chakra-shadows-outline:0 0 0 3px rgba(66, 153, 225, 0.6);--chakra-shadows-inner:inset 0 2px 4px 0 rgba(0,0,0,0.06);--chakra-shadows-none:none;--chakra-shadows-dark-lg:rgba(0, 0, 0, 0.1) 0px 0px 0px 1px,rgba(0, 0, 0, 0.2) 0px 5px 10px,rgba(0, 0, 0, 0.4) 0px 15px 40px;--chakra-sizes-1:0.25rem;--chakra-sizes-2:0.5rem;--chakra-sizes-3:0.75rem;--chakra-sizes-4:1rem;--chakra-sizes-5:1.25rem;--chakra-sizes-6:1.5rem;--chakra-sizes-7:1.75rem;--chakra-sizes-8:2rem;--chakra-sizes-9:2.25rem;--chakra-sizes-10:2.5rem;--chakra-sizes-12:3rem;--chakra-sizes-14:3.5rem;--chakra-sizes-16:4rem;--chakra-sizes-20:5rem;--chakra-sizes-24:6rem;--chakra-sizes-28:7rem;--chakra-sizes-32:8rem;--chakra-sizes-36:9rem;--chakra-sizes-40:10rem;--chakra-sizes-44:11rem;--chakra-sizes-48:12rem;--chakra-sizes-52:13rem;--chakra-sizes-56:14rem;--chakra-sizes-60:15rem;--chakra-sizes-64:16rem;--chakra-sizes-72:18rem;--chakra-sizes-80:20rem;--chakra-sizes-96:24rem;--chakra-sizes-px:1px;--chakra-sizes-0-5:0.125rem;--chakra-sizes-1-5:0.375rem;--chakra-sizes-2-5:0.625rem;--chakra-sizes-3-5:0.875rem;--chakra-sizes-max:max-content;--chakra-sizes-min:min-content;--chakra-sizes-full:100%;--chakra-sizes-3xs:14rem;--chakra-sizes-2xs:16rem;--chakra-sizes-xs:20rem;--chakra-sizes-sm:24rem;--chakra-sizes-md:28rem;--chakra-sizes-lg:32rem;--chakra-sizes-xl:36rem;--chakra-sizes-2xl:42rem;--chakra-sizes-3xl:48rem;--chakra-sizes-4xl:56rem;--chakra-sizes-5xl:64rem;--chakra-sizes-6xl:72rem;--chakra-sizes-7xl:80rem;--chakra-sizes-8xl:90rem;--chakra-sizes-prose:60ch;--chakra-sizes-container-sm:640px;--chakra-sizes-container-md:768px;--chakra-sizes-container-lg:1024px;--chakra-sizes-container-xl:1280px;--chakra-zIndices-hide:-1;--chakra-zIndices-auto:auto;--chakra-zIndices-base:0;--chakra-zIndices-docked:10;--chakra-zIndices-dropdown:1000;--chakra-zIndices-sticky:1100;--chakra-zIndices-banner:1200;--chakra-zIndices-overlay:1300;--chakra-zIndices-modal:1400;--chakra-zIndices-popover:1500;--chakra-zIndices-skipLink:1600;--chakra-zIndices-toast:1700;--chakra-zIndices-tooltip:1800;--chakra-transition-property-common:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform;--chakra-transition-property-colors:background-color,border-color,color,fill,stroke;--chakra-transition-property-dimensions:width,height;--chakra-transition-property-position:left,right,top,bottom;--chakra-transition-property-background:background-color,background-image,background-position;--chakra-transition-easing-ease-in:cubic-bezier(0.4, 0, 1, 1);--chakra-transition-easing-ease-out:cubic-bezier(0, 0, 0.2, 1);--chakra-transition-easing-ease-in-out:cubic-bezier(0.4, 0, 0.2, 1);--chakra-transition-duration-ultra-fast:50ms;--chakra-transition-duration-faster:100ms;--chakra-transition-duration-fast:150ms;--chakra-transition-duration-normal:200ms;--chakra-transition-duration-slow:300ms;--chakra-transition-duration-slower:400ms;--chakra-transition-duration-ultra-slow:500ms;--chakra-blur-none:0;--chakra-blur-sm:4px;--chakra-blur-base:8px;--chakra-blur-md:12px;--chakra-blur-lg:16px;--chakra-blur-xl:24px;--chakra-blur-2xl:40px;--chakra-blur-3xl:64px;--chakra-breakpoints-base:0em;--chakra-breakpoints-sm:30em;--chakra-breakpoints-md:48em;--chakra-breakpoints-lg:62em;--chakra-breakpoints-xl:80em;--chakra-breakpoints-2xl:96em;}.chakra-ui-light :host:not([data-theme]),.chakra-ui-light :root:not([data-theme]),.chakra-ui-light [data-theme]:not([data-theme]),[data-theme=light] :host:not([data-theme]),[data-theme=light] :root:not([data-theme]),[data-theme=light] [data-theme]:not([data-theme]),:host[data-theme=light],:root[data-theme=light],[data-theme][data-theme=light]{--chakra-colors-chakra-body-text:var(--chakra-colors-gray-800);--chakra-colors-chakra-body-bg:var(--chakra-colors-white);--chakra-colors-chakra-border-color:var(--chakra-colors-gray-200);--chakra-colors-chakra-inverse-text:var(--chakra-colors-white);--chakra-colors-chakra-subtle-bg:var(--chakra-colors-gray-100);--chakra-colors-chakra-subtle-text:var(--chakra-colors-gray-600);--chakra-colors-chakra-placeholder-color:var(--chakra-colors-gray-500);}.chakra-ui-dark :host:not([data-theme]),.chakra-ui-dark :root:not([data-theme]),.chakra-ui-dark [data-theme]:not([data-theme]),[data-theme=dark] :host:not([data-theme]),[data-theme=dark] :root:not([data-theme]),[data-theme=dark] [data-theme]:not([data-theme]),:host[data-theme=dark],:root[data-theme=dark],[data-theme][data-theme=dark]{--chakra-colors-chakra-body-text:var(--chakra-colors-whiteAlpha-900);--chakra-colors-chakra-body-bg:var(--chakra-colors-gray-800);--chakra-colors-chakra-border-color:var(--chakra-colors-whiteAlpha-300);--chakra-colors-chakra-inverse-text:var(--chakra-colors-gray-800);--chakra-colors-chakra-subtle-bg:var(--chakra-colors-gray-700);--chakra-colors-chakra-subtle-text:var(--chakra-colors-gray-400);--chakra-colors-chakra-placeholder-color:var(--chakra-colors-whiteAlpha-400);}</style><style data-emotion="css-global fubdgu">html{line-height:1.5;-webkit-text-size-adjust:100%;font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;-moz-osx-font-smoothing:grayscale;touch-action:manipulation;}body{position:relative;min-height:100%;margin:0;font-feature-settings:"kern";}:where(*, *::before, *::after){border-width:0;border-style:solid;box-sizing:border-box;word-wrap:break-word;}main{display:block;}hr{border-top-width:1px;box-sizing:content-box;height:0;overflow:visible;}:where(pre, code, kbd,samp){font-family:SFMono-Regular,Menlo,Monaco,Consolas,monospace;font-size:1em;}a{background-color:transparent;color:inherit;-webkit-text-decoration:inherit;text-decoration:inherit;}abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;}:where(b, strong){font-weight:bold;}small{font-size:80%;}:where(sub,sup){font-size:75%;line-height:0;position:relative;vertical-align:baseline;}sub{bottom:-0.25em;}sup{top:-0.5em;}img{border-style:none;}:where(button, input, optgroup, select, textarea){font-family:inherit;font-size:100%;line-height:1.15;margin:0;}:where(button, input){overflow:visible;}:where(button, select){text-transform:none;}:where(
          button::-moz-focus-inner,
          [type="button"]::-moz-focus-inner,
          [type="reset"]::-moz-focus-inner,
          [type="submit"]::-moz-focus-inner
        ){border-style:none;padding:0;}fieldset{padding:0.35em 0.75em 0.625em;}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;}progress{vertical-align:baseline;}textarea{overflow:auto;}:where([type="checkbox"], [type="radio"]){box-sizing:border-box;padding:0;}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{-webkit-appearance:none!important;}input[type="number"]{-moz-appearance:textfield;}input[type="search"]{-webkit-appearance:textfield;outline-offset:-2px;}input[type="search"]::-webkit-search-decoration{-webkit-appearance:none!important;}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;}details{display:block;}summary{display:-webkit-box;display:-webkit-list-item;display:-ms-list-itembox;display:list-item;}template{display:none;}[hidden]{display:none!important;}:where(
          blockquote,
          dl,
          dd,
          h1,
          h2,
          h3,
          h4,
          h5,
          h6,
          hr,
          figure,
          p,
          pre
        ){margin:0;}button{background:transparent;padding:0;}fieldset{margin:0;padding:0;}:where(ol, ul){margin:0;padding:0;}textarea{resize:vertical;}:where(button, [role="button"]){cursor:pointer;}button::-moz-focus-inner{border:0!important;}table{border-collapse:collapse;}:where(h1, h2, h3, h4, h5, h6){font-size:inherit;font-weight:inherit;}:where(button, input, optgroup, select, textarea){padding:0;line-height:inherit;color:inherit;}:where(img, svg, video, canvas, audio, iframe, embed, object){display:block;}:where(img, video){max-width:100%;height:auto;}[data-js-focus-visible] :focus:not([data-focus-visible-added]):not(
          [data-focus-visible-disabled]
        ){outline:none;box-shadow:none;}select::-ms-expand{display:none;}:root,:host{--chakra-vh:100vh;}@supports (height: -webkit-fill-available){:root,:host{--chakra-vh:-webkit-fill-available;}}@supports (height: -moz-fill-available){:root,:host{--chakra-vh:-moz-fill-available;}}@supports (height: 100dvh){:root,:host{--chakra-vh:100dvh;}}</style><style data-emotion="css-global lbfpqv">body{font-family:var(--chakra-fonts-body);color:var(--chakra-colors-chakra-body-text);background:#202023;transition-property:background-color;transition-duration:var(--chakra-transition-duration-normal);line-height:var(--chakra-lineHeights-base);}*::-webkit-input-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*::-moz-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*:-ms-input-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*::placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*,*::before,::after{border-color:var(--chakra-colors-chakra-border-color);}</style><style data-emotion="css 1lp32oh">.css-1lp32oh{padding-bottom:var(--chakra-space-8);}</style><main class="css-1lp32oh"><style data-emotion="css e1s8q">.css-e1s8q{position:fixed;width:100%;background:#20202380;z-index:2;-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px);}</style><nav path="/blog/07_transformers_2" class="css-e1s8q"><style data-emotion="css 1wkbond">.css-1wkbond{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:var(--chakra-space-2);max-width:var(--chakra-sizes-container-md);}</style><div class="chakra-container css-1wkbond" wrap="wrap" align="center" justify="space-between"><style data-emotion="css qi9cid">.css-qi9cid{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-right:var(--chakra-space-5);}</style><div class="css-qi9cid"><style data-emotion="css 10zaf98">.css-10zaf98{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-2xl);line-height:1.33;letter-spacing:var(--chakra-letterSpacings-tighter);}@media screen and (min-width: 48em){.css-10zaf98{font-size:var(--chakra-fontSizes-3xl);line-height:1.2;}}</style><h1 class="chakra-heading css-10zaf98"><a href="/"><style data-emotion="css lr1k0q">.css-lr1k0q{font-weight:bold;font-size:18px;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:30px;line-height:20px;padding:10px;}.css-lr1k0q >svg{-webkit-transition:500ms ease;transition:500ms ease;}.css-lr1k0q:hover>svg{-webkit-transform:rotate(90deg);-moz-transform:rotate(90deg);-ms-transform:rotate(90deg);transform:rotate(90deg);}</style><span class="css-lr1k0q"><svg width="20" height="20" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><circle cx="10" cy="10" r="10" fill="#333A3F"></circle><path d="M 9 0 L 9 20 M 9 8 L 20 8" stroke="#FFFFFF" stroke-width="0.8"></path><circle cx="5.5" cy="3.9" r=".3" fill="#FFFFFF"></circle><circle cx="17" cy="5" r=".2" fill="#FFFFFF"></circle><circle cx="5" cy="17" r=".2" fill="#FFFFFF"></circle><circle cx="15" cy="15" r=".3" fill="#FFFFFF"></circle></svg><style data-emotion="css ug13ir">.css-ug13ir{margin-left:var(--chakra-space-3);color:var(--chakra-colors-whiteAlpha-900);font-family:M PLUS Rounded 1c;font-weight:var(--chakra-fontWeights-bold);}</style><p class="chakra-text css-ug13ir">Brent Tan</p></span></a></h1></div><style data-emotion="css 4r1ia7">.css-4r1ia7{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:0.5rem;width:var(--chakra-sizes-full);-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;margin-top:var(--chakra-space-4);}@media screen and (min-width: 48em){.css-4r1ia7{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;width:auto;margin-top:0px;}}</style><div class="chakra-stack css-4r1ia7"><style data-emotion="css fvhngf">.css-fvhngf{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;text-underline-offset:3px;padding:var(--chakra-space-2);color:var(--chakra-colors-whiteAlpha-900);}.css-fvhngf:hover,.css-fvhngf[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-fvhngf:focus-visible,.css-fvhngf[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-fvhngf" href="/writing">Posts</a><a class="chakra-link css-fvhngf" href="/papers">Projects</a><a class="chakra-link css-fvhngf" href="/research">Research</a><style data-emotion="css sy0160">.css-sy0160{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;text-underline-offset:3px;padding:var(--chakra-space-2);color:var(--chakra-colors-whiteAlpha-900);display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:var(--chakra-space-2);}.css-sy0160:hover,.css-sy0160[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-sy0160:focus-visible,.css-sy0160[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a target="_blank" class="chakra-link css-sy0160" style="gap:4px" href="https://github.com/zunyibrt"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M256 32C132.3 32 32 134.9 32 261.7c0 101.5 64.2 187.5 153.2 217.9a17.56 17.56 0 0 0 3.8.4c8.3 0 11.5-6.1 11.5-11.4 0-5.5-.2-19.9-.3-39.1a102.4 102.4 0 0 1-22.6 2.7c-43.1 0-52.9-33.5-52.9-33.5-10.2-26.5-24.9-33.6-24.9-33.6-19.5-13.7-.1-14.1 1.4-14.1h.1c22.5 2 34.3 23.8 34.3 23.8 11.2 19.6 26.2 25.1 39.6 25.1a63 63 0 0 0 25.6-6c2-14.8 7.8-24.9 14.2-30.7-49.7-5.8-102-25.5-102-113.5 0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8a18.64 18.64 0 0 1 5-.5c8.1 0 26.4 3.1 56.6 24.1a208.21 208.21 0 0 1 112.2 0c30.2-21 48.5-24.1 56.6-24.1a18.64 18.64 0 0 1 5 .5c12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6 0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5 0 30.7-.3 55.5-.3 63 0 5.4 3.1 11.5 11.4 11.5a19.35 19.35 0 0 0 4-.4C415.9 449.2 480 363.1 480 261.7 480 134.9 379.7 32 256 32z"></path></svg></a></div><style data-emotion="css 1rr4qq7">.css-1rr4qq7{-webkit-flex:1;-ms-flex:1;flex:1;}</style><div align="right" class="css-1rr4qq7"><div style="display:inline-block;opacity:1;will-change:transform,opacity;transform:none"><style data-emotion="css j7xjat">.css-j7xjat{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:relative;white-space:nowrap;vertical-align:middle;outline:2px solid transparent;outline-offset:2px;line-height:1.2;border-radius:var(--chakra-radii-md);font-weight:var(--chakra-fontWeights-semibold);transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);height:var(--chakra-sizes-10);min-width:var(--chakra-sizes-10);font-size:var(--chakra-fontSizes-md);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);background:var(--chakra-colors-orange-200);color:var(--chakra-colors-gray-800);padding:0px;}.css-j7xjat:focus-visible,.css-j7xjat[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}.css-j7xjat:disabled,.css-j7xjat[disabled],.css-j7xjat[aria-disabled=true],.css-j7xjat[data-disabled]{opacity:0.4;cursor:not-allowed;box-shadow:var(--chakra-shadows-none);}.css-j7xjat:hover,.css-j7xjat[data-hover]{background:var(--chakra-colors-orange-300);}.css-j7xjat:hover:disabled,.css-j7xjat[data-hover]:disabled,.css-j7xjat:hover[disabled],.css-j7xjat[data-hover][disabled],.css-j7xjat:hover[aria-disabled=true],.css-j7xjat[data-hover][aria-disabled=true],.css-j7xjat:hover[data-disabled],.css-j7xjat[data-hover][data-disabled]{background:var(--chakra-colors-orange-200);}.css-j7xjat:active,.css-j7xjat[data-active]{background:var(--chakra-colors-orange-400);}</style><button type="button" class="chakra-button css-j7xjat" aria-label="Toggle theme"><style data-emotion="css onkibi">.css-onkibi{width:1em;height:1em;display:inline-block;line-height:1em;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:currentColor;vertical-align:middle;}</style><svg viewBox="0 0 24 24" focusable="false" class="chakra-icon css-onkibi" aria-hidden="true"><g stroke-linejoin="round" stroke-linecap="round" stroke-width="2" fill="none" stroke="currentColor"><circle cx="12" cy="12" r="5"></circle><path d="M12 1v2"></path><path d="M12 21v2"></path><path d="M4.22 4.22l1.42 1.42"></path><path d="M18.36 18.36l1.42 1.42"></path><path d="M1 12h2"></path><path d="M21 12h2"></path><path d="M4.22 19.78l1.42-1.42"></path><path d="M18.36 5.64l1.42-1.42"></path></g></svg></button></div><style data-emotion="css 1q8cyil">.css-1q8cyil{margin-left:var(--chakra-space-2);display:inline-block;}@media screen and (min-width: 48em){.css-1q8cyil{display:none;}}</style><div class="css-1q8cyil"><style data-emotion="css 1wjcgnw">.css-1wjcgnw{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:relative;white-space:nowrap;vertical-align:middle;outline:2px solid transparent;outline-offset:2px;line-height:1.2;border-radius:var(--chakra-radii-md);font-weight:var(--chakra-fontWeights-semibold);transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);height:var(--chakra-sizes-10);min-width:var(--chakra-sizes-10);font-size:var(--chakra-fontSizes-md);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);border:1px solid;border-color:var(--chakra-colors-whiteAlpha-300);color:var(--chakra-colors-whiteAlpha-900);padding:0px;}.css-1wjcgnw:focus-visible,.css-1wjcgnw[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}.css-1wjcgnw:disabled,.css-1wjcgnw[disabled],.css-1wjcgnw[aria-disabled=true],.css-1wjcgnw[data-disabled]{opacity:0.4;cursor:not-allowed;box-shadow:var(--chakra-shadows-none);}.css-1wjcgnw:hover,.css-1wjcgnw[data-hover]{background:var(--chakra-colors-whiteAlpha-200);}.css-1wjcgnw:hover:disabled,.css-1wjcgnw[data-hover]:disabled,.css-1wjcgnw:hover[disabled],.css-1wjcgnw[data-hover][disabled],.css-1wjcgnw:hover[aria-disabled=true],.css-1wjcgnw[data-hover][aria-disabled=true],.css-1wjcgnw:hover[data-disabled],.css-1wjcgnw[data-hover][data-disabled]{background:initial;}.chakra-button__group[data-attached][data-orientation=horizontal]>.css-1wjcgnw:not(:last-of-type){-webkit-margin-end:-1px;margin-inline-end:-1px;}.chakra-button__group[data-attached][data-orientation=vertical]>.css-1wjcgnw:not(:last-of-type){margin-bottom:-1px;}.css-1wjcgnw:active,.css-1wjcgnw[data-active]{background:var(--chakra-colors-whiteAlpha-300);}</style><button type="button" class="chakra-button chakra-menu__menu-button css-1wjcgnw" aria-label="Options" id="menu-button-navbar-menu" aria-expanded="false" aria-haspopup="menu" aria-controls="menu-list-navbar-menu"><svg viewBox="0 0 24 24" focusable="false" class="chakra-icon css-onkibi" aria-hidden="true"><path fill="currentColor" d="M 3 5 A 1.0001 1.0001 0 1 0 3 7 L 21 7 A 1.0001 1.0001 0 1 0 21 5 L 3 5 z M 3 11 A 1.0001 1.0001 0 1 0 3 13 L 21 13 A 1.0001 1.0001 0 1 0 21 11 L 3 11 z M 3 17 A 1.0001 1.0001 0 1 0 3 19 L 21 19 A 1.0001 1.0001 0 1 0 21 17 L 3 17 z"></path></svg></button><style data-emotion="css r6z5ec">.css-r6z5ec{z-index:1;}</style><div style="visibility:hidden;position:absolute;min-width:max-content;inset:0 auto auto 0" class="css-r6z5ec"><style data-emotion="css 1kfu8nn">.css-1kfu8nn{outline:2px solid transparent;outline-offset:2px;--menu-bg:#fff;--menu-shadow:var(--chakra-shadows-sm);color:inherit;min-width:var(--chakra-sizes-3xs);padding-top:var(--chakra-space-2);padding-bottom:var(--chakra-space-2);z-index:1;border-radius:var(--chakra-radii-md);border-width:1px;background:var(--menu-bg);box-shadow:var(--menu-shadow);}.chakra-ui-dark .css-1kfu8nn:not([data-theme]),[data-theme=dark] .css-1kfu8nn:not([data-theme]),.css-1kfu8nn[data-theme=dark]{--menu-bg:var(--chakra-colors-gray-700);--menu-shadow:var(--chakra-shadows-dark-lg);}</style><div class="chakra-menu__menu-list css-1kfu8nn" tabindex="-1" role="menu" id="menu-list-navbar-menu" aria-orientation="vertical" style="transform-origin:var(--popper-transform-origin);opacity:0;visibility:hidden;transform:scale(0.8)"></div></div></div></div></div></nav><style data-emotion="css 11nbm5x">.css-11nbm5x{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);max-width:var(--chakra-sizes-container-md);padding-top:var(--chakra-space-14);}</style><div class="chakra-container css-11nbm5x"><div class="css-0"><style data-emotion="css 130gl9x">.css-130gl9x{text-align:center;margin-bottom:var(--chakra-space-1);}</style><div class="css-130gl9x"><style data-emotion="css 14076z4">.css-14076z4{font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-4xl);line-height:1.2;margin-bottom:var(--chakra-space-4);margin-top:var(--chakra-space-5);font-family:Arial;}@media screen and (min-width: 48em){.css-14076z4{font-size:var(--chakra-fontSizes-5xl);line-height:1;}}</style><h1 class="chakra-heading css-14076z4">Building GPT from Scratch - Part 2: The Attention Mechanism</h1><style data-emotion="css q9k0mw">.css-q9k0mw{color:var(--chakra-colors-gray-500);}</style><p class="chakra-text css-q9k0mw">Aug 5, 2025</p><style data-emotion="css 14v6z45">.css-14v6z45{opacity:0.6;border:0;border-style:solid;border-bottom-width:1px;width:100%;margin-top:var(--chakra-space-4);margin-bottom:var(--chakra-space-4);border-color:var(--chakra-colors-gray-600);}</style><hr aria-orientation="horizontal" class="chakra-divider css-14v6z45"/></div><style data-emotion="css 1u5ktcn">.css-1u5ktcn{max-width:800px;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;padding:var(--chakra-space-6);}</style><div class="css-1u5ktcn"><style data-emotion="css 6gleai">.css-6gleai{margin-bottom:var(--chakra-space-4);line-height:1.7;}</style><p class="chakra-text css-6gleai"><em>This is Part 2 of a 3-part series on building a Generative Pretrained Transformer from scratch. In Part 1, we built a simple bigram model. Now we&#x27;ll introduce the core innovation that makes transformers work: Attention.</em></p>
<style data-emotion="css ebdw0u">.css-ebdw0u{font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-3xl);line-height:1.33;margin-bottom:var(--chakra-space-4);margin-top:var(--chakra-space-6);font-family:Arial;}@media screen and (min-width: 48em){.css-ebdw0u{font-size:var(--chakra-fontSizes-4xl);line-height:1.2;}}</style><h2 class="chakra-heading css-ebdw0u">The Mathematical Foundation: Weighted Aggregation</h2>
<p class="chakra-text css-6gleai">Before diving into attention, let&#x27;s understand a neat mathematical trick that forms its foundation. By multiplying a row-normalized T×T lower triangular matrix with a T×C matrix, we can get a T×C matrix which calculates a running average of the preceding rows.</p>
<p class="chakra-text css-6gleai">When we combine this with softmax (setting -inf instead of 0), we get a more general weighted aggregation of past elements:</p>
<pre><style data-emotion="css 1t1ewm6">.css-1t1ewm6{font-family:var(--chakra-fonts-mono);font-size:var(--chakra-fontSizes-sm);-webkit-padding-start:0.2em;padding-inline-start:0.2em;-webkit-padding-end:0.2em;padding-inline-end:0.2em;box-shadow:var(--badge-shadow);--badge-bg:var(--chakra-colors-gray-100);--badge-color:var(--chakra-colors-gray-800);display:block;padding:var(--chakra-space-4);background:var(--chakra-colors-gray-900);color:var(--chakra-colors-white);border-radius:var(--chakra-radii-md);overflow:auto;margin-bottom:var(--chakra-space-4);}.chakra-ui-dark .css-1t1ewm6:not([data-theme]),[data-theme=dark] .css-1t1ewm6:not([data-theme]),.css-1t1ewm6[data-theme=dark]{--badge-bg:rgba(226, 232, 240, 0.16);--badge-color:var(--chakra-colors-gray-200);}</style><code class="chakra-code css-1t1ewm6"><span class="hljs-comment"># Weighted aggregation with softmax</span>
torch.manual_seed(<span class="hljs-number">1337</span>)
B,T,C = <span class="hljs-number">4</span>,<span class="hljs-number">8</span>,<span class="hljs-number">2</span>
x = torch.randn(B,T,C)

tril = torch.tril(torch.ones(T,T)) <span class="hljs-comment"># lower triangular</span>
wei = torch.zeros((T,T))
wei = wei.masked_fill(tril==<span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>))
wei = F.softmax(wei, dim=-<span class="hljs-number">1</span>)
res = wei @ x
res.shape
</code></pre>
<p class="chakra-text css-6gleai">This is the mathematical foundation of attention - we&#x27;re learning to compute weighted averages of past tokens, where the weights are determined by the content of the tokens themselves.</p>
<h2 class="chakra-heading css-ebdw0u">Single Head Self-Attention</h2>
<p class="chakra-text css-6gleai">Now for the meat of transformers: the attention mechanism. We create an attention space of dimension &#x27;head_size&#x27;. We project each token into this space using three matrices: <strong>Key (K)</strong>, <strong>Query (Q)</strong>, and <strong>Value (V)</strong>.</p>
<p class="chakra-text css-6gleai">Here&#x27;s the intuition:</p>
<style data-emotion="css s8s4xl">.css-s8s4xl{list-style-type:initial;-webkit-margin-start:1em;margin-inline-start:1em;margin-bottom:var(--chakra-space-4);}.css-s8s4xl>*:not(style)~*:not(style){margin-top:var(--chakra-space-2);}</style><ul role="list" class="css-s8s4xl"><li class="css-0"><strong>Query (Q)</strong>: What the token is looking for</li><li class="css-0"><strong>Key (K)</strong>: What the token contains/offers</li><li class="css-0"><strong>Value (V)</strong>: What the token communicates if paid attention to</li></ul>
<p class="chakra-text css-6gleai">When K and Q are similar (measured using dot product), they have high affinity and receive higher attention weights. We don&#x27;t aggregate tokens directly - we aggregate V(X), which represents what each token communicates when attended to.</p>
<p class="chakra-text css-6gleai">The normalization by 1/√(head_size) is crucial. When Q and K have unit variance, this ensures the attention weights also have unit variance. Without this normalization, softmax would saturate towards one-hot vectors, meaning tokens would only aggregate information from a single previous token.</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Head</span>(nn.Module):
    <span class="hljs-string">&quot;&quot;&quot; one head of self-attention &quot;&quot;&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, head_size</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.key = nn.Linear(n_embd, head_size, bias=<span class="hljs-literal">False</span>)
        <span class="hljs-variable language_">self</span>.query = nn.Linear(n_embd, head_size, bias=<span class="hljs-literal">False</span>)
        <span class="hljs-variable language_">self</span>.value = nn.Linear(n_embd, head_size, bias=<span class="hljs-literal">False</span>)
        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&#x27;tril&#x27;</span>, torch.tril(torch.ones(block_size, block_size)))

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-comment"># input of size (batch, time-step, channels)</span>
        <span class="hljs-comment"># output of size (batch, time-step, head size)</span>
        B,T,C = x.shape
        k = <span class="hljs-variable language_">self</span>.key(x)   <span class="hljs-comment"># (B,T,hs)</span>
        q = <span class="hljs-variable language_">self</span>.query(x) <span class="hljs-comment"># (B,T,hs)</span>
        <span class="hljs-comment"># compute attention scores (&quot;affinities&quot;)</span>
        wei = q @ rearrange(k, <span class="hljs-string">&#x27;b t h -&gt; b h t&#x27;</span>) * C**-<span class="hljs-number">0.5</span>
        wei = wei.masked_fill(<span class="hljs-variable language_">self</span>.tril[:T, :T] == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;-inf&#x27;</span>)) <span class="hljs-comment"># (B, T, T)</span>
        wei = F.softmax(wei, dim=-<span class="hljs-number">1</span>) <span class="hljs-comment"># (B, T, T)</span>
        <span class="hljs-comment"># perform the weighted aggregation of the values</span>
        v = <span class="hljs-variable language_">self</span>.value(x) <span class="hljs-comment"># (B,T,hs)</span>
        out = wei @ v <span class="hljs-comment"># (B, T, T) @ (B, T, hs) -&gt; (B, T, hs)</span>
        <span class="hljs-keyword">return</span> out
</code></pre>
<p class="chakra-text css-6gleai">Note that we register &#x27;tril&#x27; as a buffer since it&#x27;s not a learnable parameter of the model.</p>
<h2 class="chakra-heading css-ebdw0u">Important Notes About Attention</h2>
<p class="chakra-text css-6gleai">Here are some crucial points about how attention works:</p>
<ul role="list" class="css-s8s4xl"><li class="css-0">
<p class="chakra-text css-6gleai"><strong>Note 1</strong>: Attention is a communication mechanism. You have a directed graph and you want every node to take weighted sums of every node pointing to it.</p>
</li><li class="css-0">
<p class="chakra-text css-6gleai"><strong>Note 2</strong>: Positional encoding must be embedded in the nodes, since the structure of the attention mechanism doesn&#x27;t inherently understand position.</p>
</li><li class="css-0">
<p class="chakra-text css-6gleai"><strong>Note 3</strong>: Batches are independent - there&#x27;s no cross-communication between different sequences in a batch.</p>
</li><li class="css-0">
<p class="chakra-text css-6gleai"><strong>Note 4</strong>: Here we ignore future tokens using triangular masking. If you were doing sentiment analysis, you might want to allow this communication. Encoder blocks would allow bidirectional attention, decoder blocks would not. The difference is just the triangular masking.</p>
</li><li class="css-0">
<p class="chakra-text css-6gleai"><strong>Note 5</strong>: This is called &quot;self-attention&quot; because K, Q, V all come from the same input X. If the keys and values came from somewhere else, you&#x27;d have &quot;cross-attention&quot;, which is used when there&#x27;s a separate source of information to pull from.</p>
</li></ul>
<h2 class="chakra-heading css-ebdw0u">Building the Complete Single-Head Model</h2>
<p class="chakra-text css-6gleai">Let&#x27;s put everything together into a complete model with single-head attention. We&#x27;ll make several improvements to our training setup:</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-comment"># hyperparameters</span>
batch_size = <span class="hljs-number">32</span>
block_size = <span class="hljs-number">8</span> 
max_iters = <span class="hljs-number">5000</span>
eval_interval = <span class="hljs-number">500</span>
learning_rate = <span class="hljs-number">1e-3</span>
device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>
eval_iters = <span class="hljs-number">200</span>
n_embd = <span class="hljs-number">32</span>
torch.manual_seed(<span class="hljs-number">1337</span>)

<span class="hljs-comment"># data loading</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch</span>(<span class="hljs-params">split</span>):
    <span class="hljs-comment"># generate a small batch of data of inputs x and targets y</span>
    data = train_data <span class="hljs-keyword">if</span> split == <span class="hljs-string">&#x27;train&#x27;</span> <span class="hljs-keyword">else</span> val_data
    ix = torch.randint(<span class="hljs-built_in">len</span>(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ix])
    y = torch.stack([data[i+<span class="hljs-number">1</span>:i+block_size+<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ix])
    x, y = x.to(device), y.to(device)
    <span class="hljs-keyword">return</span> x, y

<span class="hljs-meta">@torch.no_grad()</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">estimate_loss</span>():
    out = {}
    model.<span class="hljs-built_in">eval</span>()
    <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;val&#x27;</span>]:
        losses = torch.zeros(eval_iters)
        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    <span class="hljs-keyword">return</span> out
</code></pre>
<p class="chakra-text css-6gleai">Key improvements in this version:</p>
<style data-emotion="css 11bj9n">.css-11bj9n{list-style-type:decimal;-webkit-margin-start:1em;margin-inline-start:1em;margin-bottom:var(--chakra-space-4);}.css-11bj9n>*:not(style)~*:not(style){margin-top:var(--chakra-space-2);}</style><ol role="list" class="css-11bj9n"><li class="css-0"><strong>GPU Support</strong>: We move data and model to GPU using &#x27;device = &#x27;cuda&#x27;&#x27;</li><li class="css-0"><strong>Better Loss Estimation</strong>: &#x27;estimate_loss()&#x27; averages losses over multiple batches for more accurate estimates</li><li class="css-0"><strong>No Gradient Context</strong>: &#x27;@torch.no_grad()&#x27; tells PyTorch this function won&#x27;t be backpropagated through</li><li class="css-0"><strong>Embedding Dimension</strong>: Our logits are now embedded in an embedding dimension, so we need an &quot;unembedding&quot; linear layer</li><li class="css-0"><strong>Positional Embeddings</strong>: We add position information so tokens know where they are in the sequence</li></ol>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPTLanguageModel</span>(nn.Module):

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-comment"># each token directly reads off the logits for the next token from a lookup table</span>
        <span class="hljs-variable language_">self</span>.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        <span class="hljs-variable language_">self</span>.position_embedding_table = nn.Embedding(block_size, n_embd)
        <span class="hljs-variable language_">self</span>.sa_head = Head(n_embd)
        <span class="hljs-variable language_">self</span>.lm_head = nn.Linear(n_embd, vocab_size)

        <span class="hljs-comment"># better init, not covered in the original GPT video, but important</span>
        <span class="hljs-variable language_">self</span>.apply(<span class="hljs-variable language_">self</span>._init_weights)
    
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_weights</span>(<span class="hljs-params">self, module</span>):
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)
            <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                torch.nn.init.zeros_(module.bias)
        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.02</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, idx, targets=<span class="hljs-literal">None</span></span>):
        B, T = idx.shape

        <span class="hljs-comment"># idx and targets are both (B,T) tensor of integers</span>
        tok_emb = <span class="hljs-variable language_">self</span>.token_embedding_table(idx) <span class="hljs-comment"># (B,T,C)</span>
        pos_emb = <span class="hljs-variable language_">self</span>.position_embedding_table(torch.arange(T, device=device)) <span class="hljs-comment"># (T,C)</span>
        x = tok_emb + pos_emb <span class="hljs-comment"># (B,T,C)</span>
        x = <span class="hljs-variable language_">self</span>.sa_head(x)
        logits = <span class="hljs-variable language_">self</span>.lm_head(x)

        <span class="hljs-keyword">if</span> targets <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            loss = <span class="hljs-literal">None</span>
        <span class="hljs-keyword">else</span>:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        <span class="hljs-keyword">return</span> logits, loss

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">self, idx, max_new_tokens</span>):
        <span class="hljs-comment"># idx is (B, T) array of indices in the current context</span>
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):
            <span class="hljs-comment"># crop idx to the last block_size tokens</span>
            idx_cond = idx[:, -block_size:]
            <span class="hljs-comment"># get the predictions</span>
            logits, loss = <span class="hljs-variable language_">self</span>(idx_cond)
            <span class="hljs-comment"># focus only on the last time step</span>
            logits = logits[:, -<span class="hljs-number">1</span>, :] <span class="hljs-comment"># becomes (B, C)</span>
            <span class="hljs-comment"># apply softmax to get probabilities</span>
            probs = F.softmax(logits, dim=-<span class="hljs-number">1</span>) <span class="hljs-comment"># (B, C)</span>
            <span class="hljs-comment"># sample from the distribution</span>
            idx_next = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>) <span class="hljs-comment"># (B, 1)</span>
            <span class="hljs-comment"># append sampled index to the running sequence</span>
            idx = torch.cat((idx, idx_next), dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># (B, T+1)</span>
        <span class="hljs-keyword">return</span> idx
</code></pre>
<h2 class="chakra-heading css-ebdw0u">Results with Single-Head Attention</h2>
<p class="chakra-text css-6gleai">After training, we get improved results that show more structure, though still not quite Shakespeare:</p>
<pre><style data-emotion="css 1ul63h6">.css-1ul63h6{display:inline-block;font-family:var(--chakra-fonts-mono);font-size:var(--chakra-fontSizes-sm);box-shadow:var(--badge-shadow);--badge-bg:var(--chakra-colors-gray-100);--badge-color:var(--chakra-colors-gray-800);-webkit-padding-start:var(--chakra-space-2);padding-inline-start:var(--chakra-space-2);-webkit-padding-end:var(--chakra-space-2);padding-inline-end:var(--chakra-space-2);padding-top:0.95px;padding-bottom:0.95px;background:var(--chakra-colors-gray-900);color:var(--chakra-colors-white);border-radius:var(--chakra-radii-md);}.chakra-ui-dark .css-1ul63h6:not([data-theme]),[data-theme=dark] .css-1ul63h6:not([data-theme]),.css-1ul63h6[data-theme=dark]{--badge-bg:rgba(226, 232, 240, 0.16);--badge-color:var(--chakra-colors-gray-200);}</style><code class="chakra-code css-1ul63h6">Arrg hor; ho, ho aak.

LUCKUCARE Fqhelree ndel te:
Thece waplt ko.
FOCI
MHABRIEKY tur waverer orid betievis dyof b.

Bo,
ABIUCE:
N-, evesune athid nt cobasr!, Go hern, alsemin rsin varit ther I;
ANher en:ouingaroua lis py Bh mithe ast prird band bad youun theis pioat hed man ile ere sty hanonoue avillars d ty hon I:
So Ilar chy w&#x27;guma my tono yat d,
I more hathive, ha utithe baag tee
Iile athadog at y hoke hay hate whinsintoourtarenol usdon co whe sou; mer wif yoh hlele coud illesen?o I choteban
</code></pre>
<p class="chakra-text css-6gleai">The attention mechanism has given our model the ability to look back at previous characters and make more informed predictions. We can see more English-like structure emerging.</p>
<h2 class="chakra-heading css-ebdw0u">Multi-Head Attention</h2>
<p class="chakra-text css-6gleai">Instead of using a single attention head, we can use multiple smaller heads in parallel. This allows the model to attend to different types of relationships simultaneously - some heads might focus on syntax, others on semantics, etc.</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):
    <span class="hljs-string">&quot;&quot;&quot; multiple heads of self-attention in parallel &quot;&quot;&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_heads, head_size</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.heads = nn.ModuleList([Head(head_size) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_heads)])

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-keyword">return</span> torch.cat([h(x) <span class="hljs-keyword">for</span> h <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.heads], dim=-<span class="hljs-number">1</span>)
</code></pre>
<p class="chakra-text css-6gleai">In our model, we replace the single head with:</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-variable language_">self</span>.sa_head = MultiHeadAttention(<span class="hljs-number">4</span>, n_embd//<span class="hljs-number">4</span>)
</code></pre>
<p class="chakra-text css-6gleai">This gives us slightly better loss and more diverse attention patterns.</p>
<h2 class="chakra-heading css-ebdw0u">What&#x27;s Next?</h2>
<p class="chakra-text css-6gleai">We now have a working attention mechanism! However, our model is still missing several key components that make modern transformers work well:</p>
<ul role="list" class="css-s8s4xl"><li class="css-0"><strong>Feed-forward networks</strong> for computation after attention</li><li class="css-0"><strong>Transformer blocks</strong> that repeat the attention + computation pattern</li><li class="css-0"><strong>Residual connections</strong> and <strong>layer normalization</strong> for training stability</li><li class="css-0"><strong>Dropout</strong> for regularization</li></ul>
<p class="chakra-text css-6gleai">In Part 3, we&#x27;ll add these final components and see our model start generating much more coherent text. The attention mechanism we&#x27;ve built here is the foundation - everything else builds on top of it to create the full transformer architecture.</p></div></div><style data-emotion="css 17cqjj4">.css-17cqjj4{opacity:0.4;font-size:var(--chakra-fontSizes-sm);}</style><div align="center" class="css-17cqjj4">Build based on <a href="https://www.craftz.dog/" target="_blank">Takuya Matsuyama</a>.</div></div></main><span></span><span id="__chakra_env" hidden=""></span></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"07_transformers_2","content":"*This is Part 2 of a 3-part series on building a Generative Pretrained Transformer from scratch. In Part 1, we built a simple bigram model. Now we'll introduce the core innovation that makes transformers work: Attention.*\n\n## The Mathematical Foundation: Weighted Aggregation\n\nBefore diving into attention, let's understand a neat mathematical trick that forms its foundation. By multiplying a row-normalized T×T lower triangular matrix with a T×C matrix, we can get a T×C matrix which calculates a running average of the preceding rows.\n\nWhen we combine this with softmax (setting -inf instead of 0), we get a more general weighted aggregation of past elements:\n\n```python\n# Weighted aggregation with softmax\ntorch.manual_seed(1337)\nB,T,C = 4,8,2\nx = torch.randn(B,T,C)\n\ntril = torch.tril(torch.ones(T,T)) # lower triangular\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril==0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nres = wei @ x\nres.shape\n```\n\nThis is the mathematical foundation of attention - we're learning to compute weighted averages of past tokens, where the weights are determined by the content of the tokens themselves.\n\n## Single Head Self-Attention\n\nNow for the meat of transformers: the attention mechanism. We create an attention space of dimension 'head_size'. We project each token into this space using three matrices: **Key (K)**, **Query (Q)**, and **Value (V)**.\n\nHere's the intuition:\n- **Query (Q)**: What the token is looking for\n- **Key (K)**: What the token contains/offers\n- **Value (V)**: What the token communicates if paid attention to\n\nWhen K and Q are similar (measured using dot product), they have high affinity and receive higher attention weights. We don't aggregate tokens directly - we aggregate V(X), which represents what each token communicates when attended to.\n\nThe normalization by 1/√(head_size) is crucial. When Q and K have unit variance, this ensures the attention weights also have unit variance. Without this normalization, softmax would saturate towards one-hot vectors, meaning tokens would only aggregate information from a single previous token.\n\n```python\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ rearrange(k, 'b t h -\u003e b h t') * C**-0.5\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -\u003e (B, T, hs)\n        return out\n```\n\nNote that we register 'tril' as a buffer since it's not a learnable parameter of the model.\n\n## Important Notes About Attention\n\nHere are some crucial points about how attention works:\n\n- **Note 1**: Attention is a communication mechanism. You have a directed graph and you want every node to take weighted sums of every node pointing to it.\n\n- **Note 2**: Positional encoding must be embedded in the nodes, since the structure of the attention mechanism doesn't inherently understand position.\n\n- **Note 3**: Batches are independent - there's no cross-communication between different sequences in a batch.\n\n- **Note 4**: Here we ignore future tokens using triangular masking. If you were doing sentiment analysis, you might want to allow this communication. Encoder blocks would allow bidirectional attention, decoder blocks would not. The difference is just the triangular masking.\n\n- **Note 5**: This is called \"self-attention\" because K, Q, V all come from the same input X. If the keys and values came from somewhere else, you'd have \"cross-attention\", which is used when there's a separate source of information to pull from.\n\n## Building the Complete Single-Head Model\n\nLet's put everything together into a complete model with single-head attention. We'll make several improvements to our training setup:\n\n```python\n# hyperparameters\nbatch_size = 32\nblock_size = 8 \nmax_iters = 5000\neval_interval = 500\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 32\ntorch.manual_seed(1337)\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n```\n\nKey improvements in this version:\n\n1. **GPU Support**: We move data and model to GPU using 'device = 'cuda''\n2. **Better Loss Estimation**: 'estimate_loss()' averages losses over multiple batches for more accurate estimates\n3. **No Gradient Context**: '@torch.no_grad()' tells PyTorch this function won't be backpropagated through\n4. **Embedding Dimension**: Our logits are now embedded in an embedding dimension, so we need an \"unembedding\" linear layer\n5. **Positional Embeddings**: We add position information so tokens know where they are in the sequence\n\n```python\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.sa_head = Head(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.sa_head(x)\n        logits = self.lm_head(x)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```\n\n## Results with Single-Head Attention\n\nAfter training, we get improved results that show more structure, though still not quite Shakespeare:\n\n```\nArrg hor; ho, ho aak.\n\nLUCKUCARE Fqhelree ndel te:\nThece waplt ko.\nFOCI\nMHABRIEKY tur waverer orid betievis dyof b.\n\nBo,\nABIUCE:\nN-, evesune athid nt cobasr!, Go hern, alsemin rsin varit ther I;\nANher en:ouingaroua lis py Bh mithe ast prird band bad youun theis pioat hed man ile ere sty hanonoue avillars d ty hon I:\nSo Ilar chy w'guma my tono yat d,\nI more hathive, ha utithe baag tee\nIile athadog at y hoke hay hate whinsintoourtarenol usdon co whe sou; mer wif yoh hlele coud illesen?o I choteban\n```\n\nThe attention mechanism has given our model the ability to look back at previous characters and make more informed predictions. We can see more English-like structure emerging.\n\n## Multi-Head Attention\n\nInstead of using a single attention head, we can use multiple smaller heads in parallel. This allows the model to attend to different types of relationships simultaneously - some heads might focus on syntax, others on semantics, etc.\n\n```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n\n    def forward(self, x):\n        return torch.cat([h(x) for h in self.heads], dim=-1)\n```\n\nIn our model, we replace the single head with:\n```python\nself.sa_head = MultiHeadAttention(4, n_embd//4)\n```\n\nThis gives us slightly better loss and more diverse attention patterns.\n\n## What's Next?\n\nWe now have a working attention mechanism! However, our model is still missing several key components that make modern transformers work well:\n\n- **Feed-forward networks** for computation after attention\n- **Transformer blocks** that repeat the attention + computation pattern\n- **Residual connections** and **layer normalization** for training stability\n- **Dropout** for regularization\n\nIn Part 3, we'll add these final components and see our model start generating much more coherent text. The attention mechanism we've built here is the foundation - everything else builds on top of it to create the full transformer architecture.","title":"Building GPT from Scratch - Part 2: The Attention Mechanism","date":"2025-08-05","excerpt":"","tags":["transformers","tutorial"]}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"07_transformers_2"},"buildId":"bUrBysIoWJKwE5ZkPd0HO","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>