<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="description" content="Brent&#x27;s homepage"/><meta name="author" content="Brent Tan"/><title>Brent Tan - Homepage</title><meta name="next-head-count" content="5"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/e9556718e3f0f4cf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e9556718e3f0f4cf.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-c7e8ec73d08c57f3.js" defer=""></script><script src="/_next/static/chunks/framework-64ad27b21261a9ce.js" defer=""></script><script src="/_next/static/chunks/main-676fc21436baeb6f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-cb76b27212f05dea.js" defer=""></script><script src="/_next/static/chunks/175675d1-254dc21e030ba2ac.js" defer=""></script><script src="/_next/static/chunks/645-40e6d947d4f7bb86.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-0279922c060c670b.js" defer=""></script><script src="/_next/static/2ZPzPUuPIYYAGhw1kMV-o/_buildManifest.js" defer=""></script><script src="/_next/static/2ZPzPUuPIYYAGhw1kMV-o/_ssgManifest.js" defer=""></script><style id="__jsx-fed65fd97a038da">@import url("https://fonts.googleapis.com/css2?family=M+PLUS+Rounded+1c:wght@300;700&display=swap");@import url("https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@100..900&display=swap");</style></head><body><script id="chakra-script">!(function(){try{var a=function(c){var v="(prefers-color-scheme: dark)",h=window.matchMedia(v).matches?"dark":"light",r=c==="system"?h:c,o=document.documentElement,s=document.body,l="chakra-ui-light",d="chakra-ui-dark",i=r==="dark";return s.classList.add(i?d:l),s.classList.remove(i?l:d),o.style.colorScheme=r,o.dataset.theme=r,r},n=a,m="dark",e="chakra-ui-color-mode",t=localStorage.getItem(e);t?a(t):localStorage.setItem(e,a(m))}catch(a){}})();</script><div id="__next"><style data-emotion="css-global 93m1n8">:host,:root,[data-theme]{--chakra-ring-inset:var(--chakra-empty,/*!*/ /*!*/);--chakra-ring-offset-width:0px;--chakra-ring-offset-color:#fff;--chakra-ring-color:rgba(66, 153, 225, 0.6);--chakra-ring-offset-shadow:0 0 #0000;--chakra-ring-shadow:0 0 #0000;--chakra-space-x-reverse:0;--chakra-space-y-reverse:0;--chakra-colors-transparent:transparent;--chakra-colors-current:currentColor;--chakra-colors-black:#000000;--chakra-colors-white:#FFFFFF;--chakra-colors-whiteAlpha-50:rgba(255, 255, 255, 0.04);--chakra-colors-whiteAlpha-100:rgba(255, 255, 255, 0.06);--chakra-colors-whiteAlpha-200:rgba(255, 255, 255, 0.08);--chakra-colors-whiteAlpha-300:rgba(255, 255, 255, 0.16);--chakra-colors-whiteAlpha-400:rgba(255, 255, 255, 0.24);--chakra-colors-whiteAlpha-500:rgba(255, 255, 255, 0.36);--chakra-colors-whiteAlpha-600:rgba(255, 255, 255, 0.48);--chakra-colors-whiteAlpha-700:rgba(255, 255, 255, 0.64);--chakra-colors-whiteAlpha-800:rgba(255, 255, 255, 0.80);--chakra-colors-whiteAlpha-900:rgba(255, 255, 255, 0.92);--chakra-colors-blackAlpha-50:rgba(0, 0, 0, 0.04);--chakra-colors-blackAlpha-100:rgba(0, 0, 0, 0.06);--chakra-colors-blackAlpha-200:rgba(0, 0, 0, 0.08);--chakra-colors-blackAlpha-300:rgba(0, 0, 0, 0.16);--chakra-colors-blackAlpha-400:rgba(0, 0, 0, 0.24);--chakra-colors-blackAlpha-500:rgba(0, 0, 0, 0.36);--chakra-colors-blackAlpha-600:rgba(0, 0, 0, 0.48);--chakra-colors-blackAlpha-700:rgba(0, 0, 0, 0.64);--chakra-colors-blackAlpha-800:rgba(0, 0, 0, 0.80);--chakra-colors-blackAlpha-900:rgba(0, 0, 0, 0.92);--chakra-colors-gray-50:#F7FAFC;--chakra-colors-gray-100:#EDF2F7;--chakra-colors-gray-200:#E2E8F0;--chakra-colors-gray-300:#CBD5E0;--chakra-colors-gray-400:#A0AEC0;--chakra-colors-gray-500:#718096;--chakra-colors-gray-600:#4A5568;--chakra-colors-gray-700:#2D3748;--chakra-colors-gray-800:#1A202C;--chakra-colors-gray-900:#171923;--chakra-colors-red-50:#FFF5F5;--chakra-colors-red-100:#FED7D7;--chakra-colors-red-200:#FEB2B2;--chakra-colors-red-300:#FC8181;--chakra-colors-red-400:#F56565;--chakra-colors-red-500:#E53E3E;--chakra-colors-red-600:#C53030;--chakra-colors-red-700:#9B2C2C;--chakra-colors-red-800:#822727;--chakra-colors-red-900:#63171B;--chakra-colors-orange-50:#FFFAF0;--chakra-colors-orange-100:#FEEBC8;--chakra-colors-orange-200:#FBD38D;--chakra-colors-orange-300:#F6AD55;--chakra-colors-orange-400:#ED8936;--chakra-colors-orange-500:#DD6B20;--chakra-colors-orange-600:#C05621;--chakra-colors-orange-700:#9C4221;--chakra-colors-orange-800:#7B341E;--chakra-colors-orange-900:#652B19;--chakra-colors-yellow-50:#FFFFF0;--chakra-colors-yellow-100:#FEFCBF;--chakra-colors-yellow-200:#FAF089;--chakra-colors-yellow-300:#F6E05E;--chakra-colors-yellow-400:#ECC94B;--chakra-colors-yellow-500:#D69E2E;--chakra-colors-yellow-600:#B7791F;--chakra-colors-yellow-700:#975A16;--chakra-colors-yellow-800:#744210;--chakra-colors-yellow-900:#5F370E;--chakra-colors-green-50:#F0FFF4;--chakra-colors-green-100:#C6F6D5;--chakra-colors-green-200:#9AE6B4;--chakra-colors-green-300:#68D391;--chakra-colors-green-400:#48BB78;--chakra-colors-green-500:#38A169;--chakra-colors-green-600:#2F855A;--chakra-colors-green-700:#276749;--chakra-colors-green-800:#22543D;--chakra-colors-green-900:#1C4532;--chakra-colors-teal-50:#E6FFFA;--chakra-colors-teal-100:#B2F5EA;--chakra-colors-teal-200:#81E6D9;--chakra-colors-teal-300:#4FD1C5;--chakra-colors-teal-400:#38B2AC;--chakra-colors-teal-500:#319795;--chakra-colors-teal-600:#2C7A7B;--chakra-colors-teal-700:#285E61;--chakra-colors-teal-800:#234E52;--chakra-colors-teal-900:#1D4044;--chakra-colors-blue-50:#ebf8ff;--chakra-colors-blue-100:#bee3f8;--chakra-colors-blue-200:#90cdf4;--chakra-colors-blue-300:#63b3ed;--chakra-colors-blue-400:#4299e1;--chakra-colors-blue-500:#3182ce;--chakra-colors-blue-600:#2b6cb0;--chakra-colors-blue-700:#2c5282;--chakra-colors-blue-800:#2a4365;--chakra-colors-blue-900:#1A365D;--chakra-colors-cyan-50:#EDFDFD;--chakra-colors-cyan-100:#C4F1F9;--chakra-colors-cyan-200:#9DECF9;--chakra-colors-cyan-300:#76E4F7;--chakra-colors-cyan-400:#0BC5EA;--chakra-colors-cyan-500:#00B5D8;--chakra-colors-cyan-600:#00A3C4;--chakra-colors-cyan-700:#0987A0;--chakra-colors-cyan-800:#086F83;--chakra-colors-cyan-900:#065666;--chakra-colors-purple-50:#FAF5FF;--chakra-colors-purple-100:#E9D8FD;--chakra-colors-purple-200:#D6BCFA;--chakra-colors-purple-300:#B794F4;--chakra-colors-purple-400:#9F7AEA;--chakra-colors-purple-500:#805AD5;--chakra-colors-purple-600:#6B46C1;--chakra-colors-purple-700:#553C9A;--chakra-colors-purple-800:#44337A;--chakra-colors-purple-900:#322659;--chakra-colors-pink-50:#FFF5F7;--chakra-colors-pink-100:#FED7E2;--chakra-colors-pink-200:#FBB6CE;--chakra-colors-pink-300:#F687B3;--chakra-colors-pink-400:#ED64A6;--chakra-colors-pink-500:#D53F8C;--chakra-colors-pink-600:#B83280;--chakra-colors-pink-700:#97266D;--chakra-colors-pink-800:#702459;--chakra-colors-pink-900:#521B41;--chakra-colors-linkedin-50:#E8F4F9;--chakra-colors-linkedin-100:#CFEDFB;--chakra-colors-linkedin-200:#9BDAF3;--chakra-colors-linkedin-300:#68C7EC;--chakra-colors-linkedin-400:#34B3E4;--chakra-colors-linkedin-500:#00A0DC;--chakra-colors-linkedin-600:#008CC9;--chakra-colors-linkedin-700:#0077B5;--chakra-colors-linkedin-800:#005E93;--chakra-colors-linkedin-900:#004471;--chakra-colors-facebook-50:#E8F4F9;--chakra-colors-facebook-100:#D9DEE9;--chakra-colors-facebook-200:#B7C2DA;--chakra-colors-facebook-300:#6482C0;--chakra-colors-facebook-400:#4267B2;--chakra-colors-facebook-500:#385898;--chakra-colors-facebook-600:#314E89;--chakra-colors-facebook-700:#29487D;--chakra-colors-facebook-800:#223B67;--chakra-colors-facebook-900:#1E355B;--chakra-colors-messenger-50:#D0E6FF;--chakra-colors-messenger-100:#B9DAFF;--chakra-colors-messenger-200:#A2CDFF;--chakra-colors-messenger-300:#7AB8FF;--chakra-colors-messenger-400:#2E90FF;--chakra-colors-messenger-500:#0078FF;--chakra-colors-messenger-600:#0063D1;--chakra-colors-messenger-700:#0052AC;--chakra-colors-messenger-800:#003C7E;--chakra-colors-messenger-900:#002C5C;--chakra-colors-whatsapp-50:#dffeec;--chakra-colors-whatsapp-100:#b9f5d0;--chakra-colors-whatsapp-200:#90edb3;--chakra-colors-whatsapp-300:#65e495;--chakra-colors-whatsapp-400:#3cdd78;--chakra-colors-whatsapp-500:#22c35e;--chakra-colors-whatsapp-600:#179848;--chakra-colors-whatsapp-700:#0c6c33;--chakra-colors-whatsapp-800:#01421c;--chakra-colors-whatsapp-900:#001803;--chakra-colors-twitter-50:#E5F4FD;--chakra-colors-twitter-100:#C8E9FB;--chakra-colors-twitter-200:#A8DCFA;--chakra-colors-twitter-300:#83CDF7;--chakra-colors-twitter-400:#57BBF5;--chakra-colors-twitter-500:#1DA1F2;--chakra-colors-twitter-600:#1A94DA;--chakra-colors-twitter-700:#1681BF;--chakra-colors-twitter-800:#136B9E;--chakra-colors-twitter-900:#0D4D71;--chakra-colors-telegram-50:#E3F2F9;--chakra-colors-telegram-100:#C5E4F3;--chakra-colors-telegram-200:#A2D4EC;--chakra-colors-telegram-300:#7AC1E4;--chakra-colors-telegram-400:#47A9DA;--chakra-colors-telegram-500:#0088CC;--chakra-colors-telegram-600:#007AB8;--chakra-colors-telegram-700:#006BA1;--chakra-colors-telegram-800:#005885;--chakra-colors-telegram-900:#003F5E;--chakra-colors-grassTeal:#88ccca;--chakra-borders-none:0;--chakra-borders-1px:1px solid;--chakra-borders-2px:2px solid;--chakra-borders-4px:4px solid;--chakra-borders-8px:8px solid;--chakra-fonts-heading:'M PLUS Rounded 1c';--chakra-fonts-body:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--chakra-fonts-mono:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--chakra-fontSizes-3xs:0.45rem;--chakra-fontSizes-2xs:0.625rem;--chakra-fontSizes-xs:0.75rem;--chakra-fontSizes-sm:0.875rem;--chakra-fontSizes-md:1rem;--chakra-fontSizes-lg:1.125rem;--chakra-fontSizes-xl:1.25rem;--chakra-fontSizes-2xl:1.5rem;--chakra-fontSizes-3xl:1.875rem;--chakra-fontSizes-4xl:2.25rem;--chakra-fontSizes-5xl:3rem;--chakra-fontSizes-6xl:3.75rem;--chakra-fontSizes-7xl:4.5rem;--chakra-fontSizes-8xl:6rem;--chakra-fontSizes-9xl:8rem;--chakra-fontWeights-hairline:100;--chakra-fontWeights-thin:200;--chakra-fontWeights-light:300;--chakra-fontWeights-normal:400;--chakra-fontWeights-medium:500;--chakra-fontWeights-semibold:600;--chakra-fontWeights-bold:700;--chakra-fontWeights-extrabold:800;--chakra-fontWeights-black:900;--chakra-letterSpacings-tighter:-0.05em;--chakra-letterSpacings-tight:-0.025em;--chakra-letterSpacings-normal:0;--chakra-letterSpacings-wide:0.025em;--chakra-letterSpacings-wider:0.05em;--chakra-letterSpacings-widest:0.1em;--chakra-lineHeights-3:.75rem;--chakra-lineHeights-4:1rem;--chakra-lineHeights-5:1.25rem;--chakra-lineHeights-6:1.5rem;--chakra-lineHeights-7:1.75rem;--chakra-lineHeights-8:2rem;--chakra-lineHeights-9:2.25rem;--chakra-lineHeights-10:2.5rem;--chakra-lineHeights-normal:normal;--chakra-lineHeights-none:1;--chakra-lineHeights-shorter:1.25;--chakra-lineHeights-short:1.375;--chakra-lineHeights-base:1.5;--chakra-lineHeights-tall:1.625;--chakra-lineHeights-taller:2;--chakra-radii-none:0;--chakra-radii-sm:0.125rem;--chakra-radii-base:0.25rem;--chakra-radii-md:0.375rem;--chakra-radii-lg:0.5rem;--chakra-radii-xl:0.75rem;--chakra-radii-2xl:1rem;--chakra-radii-3xl:1.5rem;--chakra-radii-full:9999px;--chakra-space-1:0.25rem;--chakra-space-2:0.5rem;--chakra-space-3:0.75rem;--chakra-space-4:1rem;--chakra-space-5:1.25rem;--chakra-space-6:1.5rem;--chakra-space-7:1.75rem;--chakra-space-8:2rem;--chakra-space-9:2.25rem;--chakra-space-10:2.5rem;--chakra-space-12:3rem;--chakra-space-14:3.5rem;--chakra-space-16:4rem;--chakra-space-20:5rem;--chakra-space-24:6rem;--chakra-space-28:7rem;--chakra-space-32:8rem;--chakra-space-36:9rem;--chakra-space-40:10rem;--chakra-space-44:11rem;--chakra-space-48:12rem;--chakra-space-52:13rem;--chakra-space-56:14rem;--chakra-space-60:15rem;--chakra-space-64:16rem;--chakra-space-72:18rem;--chakra-space-80:20rem;--chakra-space-96:24rem;--chakra-space-px:1px;--chakra-space-0-5:0.125rem;--chakra-space-1-5:0.375rem;--chakra-space-2-5:0.625rem;--chakra-space-3-5:0.875rem;--chakra-shadows-xs:0 0 0 1px rgba(0, 0, 0, 0.05);--chakra-shadows-sm:0 1px 2px 0 rgba(0, 0, 0, 0.05);--chakra-shadows-base:0 1px 3px 0 rgba(0, 0, 0, 0.1),0 1px 2px 0 rgba(0, 0, 0, 0.06);--chakra-shadows-md:0 4px 6px -1px rgba(0, 0, 0, 0.1),0 2px 4px -1px rgba(0, 0, 0, 0.06);--chakra-shadows-lg:0 10px 15px -3px rgba(0, 0, 0, 0.1),0 4px 6px -2px rgba(0, 0, 0, 0.05);--chakra-shadows-xl:0 20px 25px -5px rgba(0, 0, 0, 0.1),0 10px 10px -5px rgba(0, 0, 0, 0.04);--chakra-shadows-2xl:0 25px 50px -12px rgba(0, 0, 0, 0.25);--chakra-shadows-outline:0 0 0 3px rgba(66, 153, 225, 0.6);--chakra-shadows-inner:inset 0 2px 4px 0 rgba(0,0,0,0.06);--chakra-shadows-none:none;--chakra-shadows-dark-lg:rgba(0, 0, 0, 0.1) 0px 0px 0px 1px,rgba(0, 0, 0, 0.2) 0px 5px 10px,rgba(0, 0, 0, 0.4) 0px 15px 40px;--chakra-sizes-1:0.25rem;--chakra-sizes-2:0.5rem;--chakra-sizes-3:0.75rem;--chakra-sizes-4:1rem;--chakra-sizes-5:1.25rem;--chakra-sizes-6:1.5rem;--chakra-sizes-7:1.75rem;--chakra-sizes-8:2rem;--chakra-sizes-9:2.25rem;--chakra-sizes-10:2.5rem;--chakra-sizes-12:3rem;--chakra-sizes-14:3.5rem;--chakra-sizes-16:4rem;--chakra-sizes-20:5rem;--chakra-sizes-24:6rem;--chakra-sizes-28:7rem;--chakra-sizes-32:8rem;--chakra-sizes-36:9rem;--chakra-sizes-40:10rem;--chakra-sizes-44:11rem;--chakra-sizes-48:12rem;--chakra-sizes-52:13rem;--chakra-sizes-56:14rem;--chakra-sizes-60:15rem;--chakra-sizes-64:16rem;--chakra-sizes-72:18rem;--chakra-sizes-80:20rem;--chakra-sizes-96:24rem;--chakra-sizes-px:1px;--chakra-sizes-0-5:0.125rem;--chakra-sizes-1-5:0.375rem;--chakra-sizes-2-5:0.625rem;--chakra-sizes-3-5:0.875rem;--chakra-sizes-max:max-content;--chakra-sizes-min:min-content;--chakra-sizes-full:100%;--chakra-sizes-3xs:14rem;--chakra-sizes-2xs:16rem;--chakra-sizes-xs:20rem;--chakra-sizes-sm:24rem;--chakra-sizes-md:28rem;--chakra-sizes-lg:32rem;--chakra-sizes-xl:36rem;--chakra-sizes-2xl:42rem;--chakra-sizes-3xl:48rem;--chakra-sizes-4xl:56rem;--chakra-sizes-5xl:64rem;--chakra-sizes-6xl:72rem;--chakra-sizes-7xl:80rem;--chakra-sizes-8xl:90rem;--chakra-sizes-prose:60ch;--chakra-sizes-container-sm:640px;--chakra-sizes-container-md:768px;--chakra-sizes-container-lg:1024px;--chakra-sizes-container-xl:1280px;--chakra-zIndices-hide:-1;--chakra-zIndices-auto:auto;--chakra-zIndices-base:0;--chakra-zIndices-docked:10;--chakra-zIndices-dropdown:1000;--chakra-zIndices-sticky:1100;--chakra-zIndices-banner:1200;--chakra-zIndices-overlay:1300;--chakra-zIndices-modal:1400;--chakra-zIndices-popover:1500;--chakra-zIndices-skipLink:1600;--chakra-zIndices-toast:1700;--chakra-zIndices-tooltip:1800;--chakra-transition-property-common:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform;--chakra-transition-property-colors:background-color,border-color,color,fill,stroke;--chakra-transition-property-dimensions:width,height;--chakra-transition-property-position:left,right,top,bottom;--chakra-transition-property-background:background-color,background-image,background-position;--chakra-transition-easing-ease-in:cubic-bezier(0.4, 0, 1, 1);--chakra-transition-easing-ease-out:cubic-bezier(0, 0, 0.2, 1);--chakra-transition-easing-ease-in-out:cubic-bezier(0.4, 0, 0.2, 1);--chakra-transition-duration-ultra-fast:50ms;--chakra-transition-duration-faster:100ms;--chakra-transition-duration-fast:150ms;--chakra-transition-duration-normal:200ms;--chakra-transition-duration-slow:300ms;--chakra-transition-duration-slower:400ms;--chakra-transition-duration-ultra-slow:500ms;--chakra-blur-none:0;--chakra-blur-sm:4px;--chakra-blur-base:8px;--chakra-blur-md:12px;--chakra-blur-lg:16px;--chakra-blur-xl:24px;--chakra-blur-2xl:40px;--chakra-blur-3xl:64px;--chakra-breakpoints-base:0em;--chakra-breakpoints-sm:30em;--chakra-breakpoints-md:48em;--chakra-breakpoints-lg:62em;--chakra-breakpoints-xl:80em;--chakra-breakpoints-2xl:96em;}.chakra-ui-light :host:not([data-theme]),.chakra-ui-light :root:not([data-theme]),.chakra-ui-light [data-theme]:not([data-theme]),[data-theme=light] :host:not([data-theme]),[data-theme=light] :root:not([data-theme]),[data-theme=light] [data-theme]:not([data-theme]),:host[data-theme=light],:root[data-theme=light],[data-theme][data-theme=light]{--chakra-colors-chakra-body-text:var(--chakra-colors-gray-800);--chakra-colors-chakra-body-bg:var(--chakra-colors-white);--chakra-colors-chakra-border-color:var(--chakra-colors-gray-200);--chakra-colors-chakra-inverse-text:var(--chakra-colors-white);--chakra-colors-chakra-subtle-bg:var(--chakra-colors-gray-100);--chakra-colors-chakra-subtle-text:var(--chakra-colors-gray-600);--chakra-colors-chakra-placeholder-color:var(--chakra-colors-gray-500);}.chakra-ui-dark :host:not([data-theme]),.chakra-ui-dark :root:not([data-theme]),.chakra-ui-dark [data-theme]:not([data-theme]),[data-theme=dark] :host:not([data-theme]),[data-theme=dark] :root:not([data-theme]),[data-theme=dark] [data-theme]:not([data-theme]),:host[data-theme=dark],:root[data-theme=dark],[data-theme][data-theme=dark]{--chakra-colors-chakra-body-text:var(--chakra-colors-whiteAlpha-900);--chakra-colors-chakra-body-bg:var(--chakra-colors-gray-800);--chakra-colors-chakra-border-color:var(--chakra-colors-whiteAlpha-300);--chakra-colors-chakra-inverse-text:var(--chakra-colors-gray-800);--chakra-colors-chakra-subtle-bg:var(--chakra-colors-gray-700);--chakra-colors-chakra-subtle-text:var(--chakra-colors-gray-400);--chakra-colors-chakra-placeholder-color:var(--chakra-colors-whiteAlpha-400);}</style><style data-emotion="css-global fubdgu">html{line-height:1.5;-webkit-text-size-adjust:100%;font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;-moz-osx-font-smoothing:grayscale;touch-action:manipulation;}body{position:relative;min-height:100%;margin:0;font-feature-settings:"kern";}:where(*, *::before, *::after){border-width:0;border-style:solid;box-sizing:border-box;word-wrap:break-word;}main{display:block;}hr{border-top-width:1px;box-sizing:content-box;height:0;overflow:visible;}:where(pre, code, kbd,samp){font-family:SFMono-Regular,Menlo,Monaco,Consolas,monospace;font-size:1em;}a{background-color:transparent;color:inherit;-webkit-text-decoration:inherit;text-decoration:inherit;}abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;}:where(b, strong){font-weight:bold;}small{font-size:80%;}:where(sub,sup){font-size:75%;line-height:0;position:relative;vertical-align:baseline;}sub{bottom:-0.25em;}sup{top:-0.5em;}img{border-style:none;}:where(button, input, optgroup, select, textarea){font-family:inherit;font-size:100%;line-height:1.15;margin:0;}:where(button, input){overflow:visible;}:where(button, select){text-transform:none;}:where(
          button::-moz-focus-inner,
          [type="button"]::-moz-focus-inner,
          [type="reset"]::-moz-focus-inner,
          [type="submit"]::-moz-focus-inner
        ){border-style:none;padding:0;}fieldset{padding:0.35em 0.75em 0.625em;}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;}progress{vertical-align:baseline;}textarea{overflow:auto;}:where([type="checkbox"], [type="radio"]){box-sizing:border-box;padding:0;}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{-webkit-appearance:none!important;}input[type="number"]{-moz-appearance:textfield;}input[type="search"]{-webkit-appearance:textfield;outline-offset:-2px;}input[type="search"]::-webkit-search-decoration{-webkit-appearance:none!important;}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;}details{display:block;}summary{display:-webkit-box;display:-webkit-list-item;display:-ms-list-itembox;display:list-item;}template{display:none;}[hidden]{display:none!important;}:where(
          blockquote,
          dl,
          dd,
          h1,
          h2,
          h3,
          h4,
          h5,
          h6,
          hr,
          figure,
          p,
          pre
        ){margin:0;}button{background:transparent;padding:0;}fieldset{margin:0;padding:0;}:where(ol, ul){margin:0;padding:0;}textarea{resize:vertical;}:where(button, [role="button"]){cursor:pointer;}button::-moz-focus-inner{border:0!important;}table{border-collapse:collapse;}:where(h1, h2, h3, h4, h5, h6){font-size:inherit;font-weight:inherit;}:where(button, input, optgroup, select, textarea){padding:0;line-height:inherit;color:inherit;}:where(img, svg, video, canvas, audio, iframe, embed, object){display:block;}:where(img, video){max-width:100%;height:auto;}[data-js-focus-visible] :focus:not([data-focus-visible-added]):not(
          [data-focus-visible-disabled]
        ){outline:none;box-shadow:none;}select::-ms-expand{display:none;}:root,:host{--chakra-vh:100vh;}@supports (height: -webkit-fill-available){:root,:host{--chakra-vh:-webkit-fill-available;}}@supports (height: -moz-fill-available){:root,:host{--chakra-vh:-moz-fill-available;}}@supports (height: 100dvh){:root,:host{--chakra-vh:100dvh;}}</style><style data-emotion="css-global lbfpqv">body{font-family:var(--chakra-fonts-body);color:var(--chakra-colors-chakra-body-text);background:#202023;transition-property:background-color;transition-duration:var(--chakra-transition-duration-normal);line-height:var(--chakra-lineHeights-base);}*::-webkit-input-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*::-moz-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*:-ms-input-placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*::placeholder{color:var(--chakra-colors-chakra-placeholder-color);}*,*::before,::after{border-color:var(--chakra-colors-chakra-border-color);}</style><style data-emotion="css 1lp32oh">.css-1lp32oh{padding-bottom:var(--chakra-space-8);}</style><main class="css-1lp32oh"><style data-emotion="css e1s8q">.css-e1s8q{position:fixed;width:100%;background:#20202380;z-index:2;-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px);}</style><nav path="/blog/08_transfomers_3" class="css-e1s8q"><style data-emotion="css 1wkbond">.css-1wkbond{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding:var(--chakra-space-2);max-width:var(--chakra-sizes-container-md);}</style><div class="chakra-container css-1wkbond" wrap="wrap" align="center" justify="space-between"><style data-emotion="css qi9cid">.css-qi9cid{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-right:var(--chakra-space-5);}</style><div class="css-qi9cid"><style data-emotion="css 10zaf98">.css-10zaf98{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-2xl);line-height:1.33;letter-spacing:var(--chakra-letterSpacings-tighter);}@media screen and (min-width: 48em){.css-10zaf98{font-size:var(--chakra-fontSizes-3xl);line-height:1.2;}}</style><h1 class="chakra-heading css-10zaf98"><a href="/"><style data-emotion="css lr1k0q">.css-lr1k0q{font-weight:bold;font-size:18px;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:30px;line-height:20px;padding:10px;}.css-lr1k0q >svg{-webkit-transition:500ms ease;transition:500ms ease;}.css-lr1k0q:hover>svg{-webkit-transform:rotate(90deg);-moz-transform:rotate(90deg);-ms-transform:rotate(90deg);transform:rotate(90deg);}</style><span class="css-lr1k0q"><svg width="20" height="20" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><circle cx="10" cy="10" r="10" fill="#333A3F"></circle><path d="M 9 0 L 9 20 M 9 8 L 20 8" stroke="#FFFFFF" stroke-width="0.8"></path><circle cx="5.5" cy="3.9" r=".3" fill="#FFFFFF"></circle><circle cx="17" cy="5" r=".2" fill="#FFFFFF"></circle><circle cx="5" cy="17" r=".2" fill="#FFFFFF"></circle><circle cx="15" cy="15" r=".3" fill="#FFFFFF"></circle></svg><style data-emotion="css ug13ir">.css-ug13ir{margin-left:var(--chakra-space-3);color:var(--chakra-colors-whiteAlpha-900);font-family:M PLUS Rounded 1c;font-weight:var(--chakra-fontWeights-bold);}</style><p class="chakra-text css-ug13ir">Brent Tan</p></span></a></h1></div><style data-emotion="css 4r1ia7">.css-4r1ia7{display:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;gap:0.5rem;width:var(--chakra-sizes-full);-webkit-box-flex:1;-webkit-flex-grow:1;-ms-flex-positive:1;flex-grow:1;margin-top:var(--chakra-space-4);}@media screen and (min-width: 48em){.css-4r1ia7{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;width:auto;margin-top:0px;}}</style><div class="chakra-stack css-4r1ia7"><style data-emotion="css fvhngf">.css-fvhngf{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;text-underline-offset:3px;padding:var(--chakra-space-2);color:var(--chakra-colors-whiteAlpha-900);}.css-fvhngf:hover,.css-fvhngf[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-fvhngf:focus-visible,.css-fvhngf[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-fvhngf" href="/writing">Posts</a><a class="chakra-link css-fvhngf" href="/papers">Projects</a><a class="chakra-link css-fvhngf" href="/research">Research</a><style data-emotion="css sy0160">.css-sy0160{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;text-underline-offset:3px;padding:var(--chakra-space-2);color:var(--chakra-colors-whiteAlpha-900);display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:var(--chakra-space-2);}.css-sy0160:hover,.css-sy0160[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-sy0160:focus-visible,.css-sy0160[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a target="_blank" class="chakra-link css-sy0160" style="gap:4px" href="https://github.com/zunyibrt"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M256 32C132.3 32 32 134.9 32 261.7c0 101.5 64.2 187.5 153.2 217.9a17.56 17.56 0 0 0 3.8.4c8.3 0 11.5-6.1 11.5-11.4 0-5.5-.2-19.9-.3-39.1a102.4 102.4 0 0 1-22.6 2.7c-43.1 0-52.9-33.5-52.9-33.5-10.2-26.5-24.9-33.6-24.9-33.6-19.5-13.7-.1-14.1 1.4-14.1h.1c22.5 2 34.3 23.8 34.3 23.8 11.2 19.6 26.2 25.1 39.6 25.1a63 63 0 0 0 25.6-6c2-14.8 7.8-24.9 14.2-30.7-49.7-5.8-102-25.5-102-113.5 0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8a18.64 18.64 0 0 1 5-.5c8.1 0 26.4 3.1 56.6 24.1a208.21 208.21 0 0 1 112.2 0c30.2-21 48.5-24.1 56.6-24.1a18.64 18.64 0 0 1 5 .5c12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6 0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5 0 30.7-.3 55.5-.3 63 0 5.4 3.1 11.5 11.4 11.5a19.35 19.35 0 0 0 4-.4C415.9 449.2 480 363.1 480 261.7 480 134.9 379.7 32 256 32z"></path></svg></a></div><style data-emotion="css 1rr4qq7">.css-1rr4qq7{-webkit-flex:1;-ms-flex:1;flex:1;}</style><div align="right" class="css-1rr4qq7"><div style="display:inline-block;opacity:1;will-change:transform,opacity;transform:none"><style data-emotion="css j7xjat">.css-j7xjat{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:relative;white-space:nowrap;vertical-align:middle;outline:2px solid transparent;outline-offset:2px;line-height:1.2;border-radius:var(--chakra-radii-md);font-weight:var(--chakra-fontWeights-semibold);transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);height:var(--chakra-sizes-10);min-width:var(--chakra-sizes-10);font-size:var(--chakra-fontSizes-md);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);background:var(--chakra-colors-orange-200);color:var(--chakra-colors-gray-800);padding:0px;}.css-j7xjat:focus-visible,.css-j7xjat[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}.css-j7xjat:disabled,.css-j7xjat[disabled],.css-j7xjat[aria-disabled=true],.css-j7xjat[data-disabled]{opacity:0.4;cursor:not-allowed;box-shadow:var(--chakra-shadows-none);}.css-j7xjat:hover,.css-j7xjat[data-hover]{background:var(--chakra-colors-orange-300);}.css-j7xjat:hover:disabled,.css-j7xjat[data-hover]:disabled,.css-j7xjat:hover[disabled],.css-j7xjat[data-hover][disabled],.css-j7xjat:hover[aria-disabled=true],.css-j7xjat[data-hover][aria-disabled=true],.css-j7xjat:hover[data-disabled],.css-j7xjat[data-hover][data-disabled]{background:var(--chakra-colors-orange-200);}.css-j7xjat:active,.css-j7xjat[data-active]{background:var(--chakra-colors-orange-400);}</style><button type="button" class="chakra-button css-j7xjat" aria-label="Toggle theme"><style data-emotion="css onkibi">.css-onkibi{width:1em;height:1em;display:inline-block;line-height:1em;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:currentColor;vertical-align:middle;}</style><svg viewBox="0 0 24 24" focusable="false" class="chakra-icon css-onkibi" aria-hidden="true"><g stroke-linejoin="round" stroke-linecap="round" stroke-width="2" fill="none" stroke="currentColor"><circle cx="12" cy="12" r="5"></circle><path d="M12 1v2"></path><path d="M12 21v2"></path><path d="M4.22 4.22l1.42 1.42"></path><path d="M18.36 18.36l1.42 1.42"></path><path d="M1 12h2"></path><path d="M21 12h2"></path><path d="M4.22 19.78l1.42-1.42"></path><path d="M18.36 5.64l1.42-1.42"></path></g></svg></button></div><style data-emotion="css 1q8cyil">.css-1q8cyil{margin-left:var(--chakra-space-2);display:inline-block;}@media screen and (min-width: 48em){.css-1q8cyil{display:none;}}</style><div class="css-1q8cyil"><style data-emotion="css 1wjcgnw">.css-1wjcgnw{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-appearance:none;-moz-appearance:none;-ms-appearance:none;appearance:none;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:relative;white-space:nowrap;vertical-align:middle;outline:2px solid transparent;outline-offset:2px;line-height:1.2;border-radius:var(--chakra-radii-md);font-weight:var(--chakra-fontWeights-semibold);transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-normal);height:var(--chakra-sizes-10);min-width:var(--chakra-sizes-10);font-size:var(--chakra-fontSizes-md);-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);border:1px solid;border-color:var(--chakra-colors-whiteAlpha-300);color:var(--chakra-colors-whiteAlpha-900);padding:0px;}.css-1wjcgnw:focus-visible,.css-1wjcgnw[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}.css-1wjcgnw:disabled,.css-1wjcgnw[disabled],.css-1wjcgnw[aria-disabled=true],.css-1wjcgnw[data-disabled]{opacity:0.4;cursor:not-allowed;box-shadow:var(--chakra-shadows-none);}.css-1wjcgnw:hover,.css-1wjcgnw[data-hover]{background:var(--chakra-colors-whiteAlpha-200);}.css-1wjcgnw:hover:disabled,.css-1wjcgnw[data-hover]:disabled,.css-1wjcgnw:hover[disabled],.css-1wjcgnw[data-hover][disabled],.css-1wjcgnw:hover[aria-disabled=true],.css-1wjcgnw[data-hover][aria-disabled=true],.css-1wjcgnw:hover[data-disabled],.css-1wjcgnw[data-hover][data-disabled]{background:initial;}.chakra-button__group[data-attached][data-orientation=horizontal]>.css-1wjcgnw:not(:last-of-type){-webkit-margin-end:-1px;margin-inline-end:-1px;}.chakra-button__group[data-attached][data-orientation=vertical]>.css-1wjcgnw:not(:last-of-type){margin-bottom:-1px;}.css-1wjcgnw:active,.css-1wjcgnw[data-active]{background:var(--chakra-colors-whiteAlpha-300);}</style><button type="button" class="chakra-button chakra-menu__menu-button css-1wjcgnw" aria-label="Options" id="menu-button-navbar-menu" aria-expanded="false" aria-haspopup="menu" aria-controls="menu-list-navbar-menu"><svg viewBox="0 0 24 24" focusable="false" class="chakra-icon css-onkibi" aria-hidden="true"><path fill="currentColor" d="M 3 5 A 1.0001 1.0001 0 1 0 3 7 L 21 7 A 1.0001 1.0001 0 1 0 21 5 L 3 5 z M 3 11 A 1.0001 1.0001 0 1 0 3 13 L 21 13 A 1.0001 1.0001 0 1 0 21 11 L 3 11 z M 3 17 A 1.0001 1.0001 0 1 0 3 19 L 21 19 A 1.0001 1.0001 0 1 0 21 17 L 3 17 z"></path></svg></button><style data-emotion="css r6z5ec">.css-r6z5ec{z-index:1;}</style><div style="visibility:hidden;position:absolute;min-width:max-content;inset:0 auto auto 0" class="css-r6z5ec"><style data-emotion="css 1kfu8nn">.css-1kfu8nn{outline:2px solid transparent;outline-offset:2px;--menu-bg:#fff;--menu-shadow:var(--chakra-shadows-sm);color:inherit;min-width:var(--chakra-sizes-3xs);padding-top:var(--chakra-space-2);padding-bottom:var(--chakra-space-2);z-index:1;border-radius:var(--chakra-radii-md);border-width:1px;background:var(--menu-bg);box-shadow:var(--menu-shadow);}.chakra-ui-dark .css-1kfu8nn:not([data-theme]),[data-theme=dark] .css-1kfu8nn:not([data-theme]),.css-1kfu8nn[data-theme=dark]{--menu-bg:var(--chakra-colors-gray-700);--menu-shadow:var(--chakra-shadows-dark-lg);}</style><div class="chakra-menu__menu-list css-1kfu8nn" tabindex="-1" role="menu" id="menu-list-navbar-menu" aria-orientation="vertical" style="transform-origin:var(--popper-transform-origin);opacity:0;visibility:hidden;transform:scale(0.8)"></div></div></div></div></div></nav><style data-emotion="css 11nbm5x">.css-11nbm5x{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;-webkit-padding-start:var(--chakra-space-4);padding-inline-start:var(--chakra-space-4);-webkit-padding-end:var(--chakra-space-4);padding-inline-end:var(--chakra-space-4);max-width:var(--chakra-sizes-container-md);padding-top:var(--chakra-space-14);}</style><div class="chakra-container css-11nbm5x"><div class="css-0"><style data-emotion="css 130gl9x">.css-130gl9x{text-align:center;margin-bottom:var(--chakra-space-1);}</style><div class="css-130gl9x"><style data-emotion="css 14076z4">.css-14076z4{font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-4xl);line-height:1.2;margin-bottom:var(--chakra-space-4);margin-top:var(--chakra-space-5);font-family:Arial;}@media screen and (min-width: 48em){.css-14076z4{font-size:var(--chakra-fontSizes-5xl);line-height:1;}}</style><h1 class="chakra-heading css-14076z4">Building GPT from Scratch - Part 3: Building the Complete Transformer</h1><style data-emotion="css q9k0mw">.css-q9k0mw{color:var(--chakra-colors-gray-500);}</style><p class="chakra-text css-q9k0mw">Aug 6, 2025</p><style data-emotion="css 14v6z45">.css-14v6z45{opacity:0.6;border:0;border-style:solid;border-bottom-width:1px;width:100%;margin-top:var(--chakra-space-4);margin-bottom:var(--chakra-space-4);border-color:var(--chakra-colors-gray-600);}</style><hr aria-orientation="horizontal" class="chakra-divider css-14v6z45"/></div><style data-emotion="css 1u5ktcn">.css-1u5ktcn{max-width:800px;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;padding:var(--chakra-space-6);}</style><div class="css-1u5ktcn"><style data-emotion="css 6gleai">.css-6gleai{margin-bottom:var(--chakra-space-4);line-height:1.7;}</style><p class="chakra-text css-6gleai"><em>This is Part 3 of a 3-part series on building a Generative Pretrained Transformer from scratch. In Parts 1 and 2, we built the data pipeline and attention mechanism. Now we&#x27;ll complete our transformer with the remaining key components.</em></p>
<style data-emotion="css ebdw0u">.css-ebdw0u{font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-3xl);line-height:1.33;margin-bottom:var(--chakra-space-4);margin-top:var(--chakra-space-6);font-family:Arial;}@media screen and (min-width: 48em){.css-ebdw0u{font-size:var(--chakra-fontSizes-4xl);line-height:1.2;}}</style><h2 class="chakra-heading css-ebdw0u">Feed-Forward Networks</h2>
<p class="chakra-text css-6gleai">After our attention mechanism learns to communicate between tokens, we need to give the model computation time to process what it has learned. This is where feed-forward networks come in - they&#x27;re simply MLPs with ReLU activation that allow the model to &quot;think&quot; about the information it just gathered.</p>
<pre><style data-emotion="css 1t1ewm6">.css-1t1ewm6{font-family:var(--chakra-fonts-mono);font-size:var(--chakra-fontSizes-sm);-webkit-padding-start:0.2em;padding-inline-start:0.2em;-webkit-padding-end:0.2em;padding-inline-end:0.2em;box-shadow:var(--badge-shadow);--badge-bg:var(--chakra-colors-gray-100);--badge-color:var(--chakra-colors-gray-800);display:block;padding:var(--chakra-space-4);background:var(--chakra-colors-gray-900);color:var(--chakra-colors-white);border-radius:var(--chakra-radii-md);overflow:auto;margin-bottom:var(--chakra-space-4);}.chakra-ui-dark .css-1t1ewm6:not([data-theme]),[data-theme=dark] .css-1t1ewm6:not([data-theme]),.css-1t1ewm6[data-theme=dark]{--badge-bg:rgba(226, 232, 240, 0.16);--badge-color:var(--chakra-colors-gray-200);}</style><code class="chakra-code css-1t1ewm6"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedFoward</span>(nn.Module):
    <span class="hljs-string">&quot;&quot;&quot; a simple linear layer followed by a non-linearity &quot;&quot;&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_embd</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.net = nn.Sequential(
            nn.Linear(n_embd, n_embd),
            nn.ReLU(),
        )

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.net(x)
</code></pre>
<p class="chakra-text css-6gleai">We add this to our model after the attention:</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-comment"># In our model:</span>
<span class="hljs-variable language_">self</span>.sa_head = MultiHeadAttention(<span class="hljs-number">4</span>, n_embd//<span class="hljs-number">4</span>)
<span class="hljs-variable language_">self</span>.ffwd = FeedFoward(n_embd) <span class="hljs-comment"># MLP with RELU</span>

<span class="hljs-comment"># In forward pass:</span>
x = <span class="hljs-variable language_">self</span>.sa_head(x)
x = <span class="hljs-variable language_">self</span>.ffwd(x)
logits = <span class="hljs-variable language_">self</span>.lm_head(x)
</code></pre>
<h2 class="chakra-heading css-ebdw0u">Transformer Blocks: Communication + Computation</h2>
<p class="chakra-text css-6gleai">The pattern of attention (communication) followed by feed-forward (computation) is so fundamental that we package it into reusable <strong>Transformer Blocks</strong>. These blocks can be stacked to create deeper networks that alternate between communication and computation phases.</p>
<pre><code class="chakra-code css-1t1ewm6">n_head = <span class="hljs-number">4</span>
n_layer = <span class="hljs-number">2</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">Block</span>(nn.Module):
    <span class="hljs-string">&quot;&quot;&quot; Transformer block: communication followed by computation &quot;&quot;&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_embd, n_head</span>):
        <span class="hljs-comment"># n_embd: embedding dimension, n_head: the number of heads we&#x27;d like</span>
        <span class="hljs-built_in">super</span>().__init__()
        head_size = n_embd // n_head
        <span class="hljs-variable language_">self</span>.sa = MultiHeadAttention(n_head, head_size)
        <span class="hljs-variable language_">self</span>.ffwd = FeedFoward(n_embd)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = <span class="hljs-variable language_">self</span>.sa(x)
        x = <span class="hljs-variable language_">self</span>.ffwd(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
<p class="chakra-text css-6gleai">We can then stack multiple blocks:</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-comment"># In our model:</span>
<span class="hljs-variable language_">self</span>.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_layer)])

<span class="hljs-comment"># In forward pass:</span>
x = tok_emb + pos_emb <span class="hljs-comment"># (B,T,C)</span>
x = <span class="hljs-variable language_">self</span>.blocks(x)
logits = <span class="hljs-variable language_">self</span>.lm_head(x)
</code></pre>
<p class="chakra-text css-6gleai">At this point, our model starts generating recognizable English words!</p>
<pre><code class="chakra-code css-1t1ewm6">RILELE
MAssell use you.
Nt I RYan lake off.

Ficink&#x27;d
hote carewtledeche quie to whanl Gatt Mejesery ely:
The if, bet leveses it theave be ry skit you file.

Kay wred tome dake stance, suks,
Adech JORo!

ALOUSBET:
Wis brake grourst and ald creapsss,
Andite
noat,
Amothery are doreast is
</code></pre>
<h2 class="chakra-heading css-ebdw0u">Training Optimizations</h2>
<p class="chakra-text css-6gleai">As our network gets deeper with multiple transformer blocks, we need optimizations to improve training stability and performance.</p>
<style data-emotion="css 2k8uwk">.css-2k8uwk{font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-2xl);line-height:1.33;margin-bottom:var(--chakra-space-3);margin-top:var(--chakra-space-5);font-family:Arial;}@media screen and (min-width: 48em){.css-2k8uwk{font-size:var(--chakra-fontSizes-3xl);line-height:1.2;}}</style><h3 class="chakra-heading css-2k8uwk">Residual Connections</h3>
<p class="chakra-text css-6gleai">The first optimization is <strong>residual/skip connections</strong>, introduced in the <style data-emotion="css 1qdxmbw">.css-1qdxmbw{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:#ff63c3;text-underline-offset:3px;}.css-1qdxmbw:hover,.css-1qdxmbw[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-1qdxmbw:focus-visible,.css-1qdxmbw[data-focus-visible]{box-shadow:var(--chakra-shadows-outline);}</style><a target="_blank" rel="noopener" class="chakra-link css-1qdxmbw" href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a> paper. These connections allow gradients to flow directly through the network, avoiding training bottlenecks in deep networks.</p>
<p class="chakra-text css-6gleai">Instead of &#x27;x = self.sa(x)&#x27;, we do &#x27;x = x + self.sa(x)&#x27;. The computation now returns a residual that gets added to the original input:</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Block</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = x + <span class="hljs-variable language_">self</span>.sa(x)  <span class="hljs-comment"># residual connection</span>
        x = x + <span class="hljs-variable language_">self</span>.ffwd(x)  <span class="hljs-comment"># residual connection</span>
        <span class="hljs-keyword">return</span> x
</code></pre>
<h3 class="chakra-heading css-2k8uwk">Projection Layers</h3>
<p class="chakra-text css-6gleai">We also add projection layers to map back to the embedding space after our multi-head attention and expand our feed-forward networks as in the original paper:</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):
    <span class="hljs-string">&quot;&quot;&quot; multiple heads of self-attention in parallel &quot;&quot;&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_heads, head_size</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.heads = nn.ModuleList([Head(head_size) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_heads)])
        <span class="hljs-variable language_">self</span>.proj = nn.Linear(head_size * num_heads, n_embd)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        out = torch.cat([h(x) <span class="hljs-keyword">for</span> h <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.heads], dim=-<span class="hljs-number">1</span>)
        out = <span class="hljs-variable language_">self</span>.proj(out)
        <span class="hljs-keyword">return</span> out

<span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedFoward</span>(nn.Module):
    <span class="hljs-string">&quot;&quot;&quot; a simple linear layer followed by a non-linearity &quot;&quot;&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_embd</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.net = nn.Sequential(
            nn.Linear(n_embd, <span class="hljs-number">4</span> * n_embd),  <span class="hljs-comment"># expand by 4x as in the paper</span>
            nn.ReLU(),
            nn.Linear(<span class="hljs-number">4</span> * n_embd, n_embd),  <span class="hljs-comment"># project back down</span>
        )

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.net(x)
</code></pre>
<h3 class="chakra-heading css-2k8uwk">Layer Normalization</h3>
<p class="chakra-text css-6gleai">Layer normalization helps stabilize training by normalizing inputs to have zero mean and unit variance. We apply it before both the self-attention and feed-forward computations in each block, and once more after all blocks:</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Block</span>(nn.Module):
    <span class="hljs-string">&quot;&quot;&quot; Transformer block: communication followed by computation &quot;&quot;&quot;</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_embd, n_head</span>):
        <span class="hljs-built_in">super</span>().__init__()
        head_size = n_embd // n_head
        <span class="hljs-variable language_">self</span>.sa = MultiHeadAttention(n_head, head_size)
        <span class="hljs-variable language_">self</span>.ffwd = FeedFoward(n_embd)
        <span class="hljs-variable language_">self</span>.ln1 = nn.LayerNorm(n_embd)
        <span class="hljs-variable language_">self</span>.ln2 = nn.LayerNorm(n_embd)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        x = x + <span class="hljs-variable language_">self</span>.sa(<span class="hljs-variable language_">self</span>.ln1(x))
        x = x + <span class="hljs-variable language_">self</span>.ffwd(<span class="hljs-variable language_">self</span>.ln2(x))
        <span class="hljs-keyword">return</span> x
</code></pre>
<h3 class="chakra-heading css-2k8uwk">Dropout</h3>
<p class="chakra-text css-6gleai">Finally, we add <strong>dropout</strong> for regularization, which randomly sets some activations to zero during training to prevent overfitting:</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedFoward</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_embd</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.net = nn.Sequential(
            nn.Linear(n_embd, <span class="hljs-number">4</span> * n_embd),
            nn.ReLU(),
            nn.Linear(<span class="hljs-number">4</span> * n_embd, n_embd),
            nn.Dropout(dropout),  <span class="hljs-comment"># add dropout</span>
        )

<span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_heads, head_size</span>):
        <span class="hljs-built_in">super</span>().__init__()
        <span class="hljs-variable language_">self</span>.heads = nn.ModuleList([Head(head_size) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_heads)])
        <span class="hljs-variable language_">self</span>.proj = nn.Linear(head_size * num_heads, n_embd)
        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        out = torch.cat([h(x) <span class="hljs-keyword">for</span> h <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.heads], dim=-<span class="hljs-number">1</span>)
        out = <span class="hljs-variable language_">self</span>.dropout(<span class="hljs-variable language_">self</span>.proj(out))
        <span class="hljs-keyword">return</span> out
</code></pre>
<h2 class="chakra-heading css-ebdw0u">Final Model and Hyperparameters</h2>
<p class="chakra-text css-6gleai">We now have a complete decoder-only transformer ready to scale and train! Here are the hyperparameters for our final model:</p>
<pre><code class="chakra-code css-1t1ewm6"><span class="hljs-comment"># hyperparameters</span>
batch_size = <span class="hljs-number">32</span>
block_size = <span class="hljs-number">32</span>
max_iters = <span class="hljs-number">5000</span>
eval_interval = <span class="hljs-number">500</span>
learning_rate = <span class="hljs-number">3e-4</span>
eval_iters = <span class="hljs-number">200</span>
n_embd = <span class="hljs-number">64</span>
n_head = <span class="hljs-number">6</span>
n_layer = <span class="hljs-number">6</span>
dropout = <span class="hljs-number">0.2</span>
</code></pre>
<h2 class="chakra-heading css-ebdw0u">Final Results</h2>
<p class="chakra-text css-6gleai">After training our complete transformer, we get much more structured output:</p>
<pre><code class="chakra-code css-1t1ewm6">DOF GlLN:
They good, then then, bladgy bone not thindnes
I way Jeain, fainly!

ISABELLA:
I way, thou fourd to havary flown dews&#x27;m-sine.

NowZALLET:
Which here old thy&#x27;s warring hiod
On dearys hory be wive to more; greseli!

But nighd Wart, prance:
Barch,
And prayem not welld she you, coldinger:
O I the oldst God somed:
Sirry is let never To be whith, I new&#x27;n be thy limpiny
Word: where deblitiss for give upon the conqueennifult,
And so pobeterl. by they she thy truge,
If you his let a brotess.
</code></pre>
<p class="chakra-text css-6gleai">While still not perfect, the improvement is dramatic! The model now:</p>
<style data-emotion="css s8s4xl">.css-s8s4xl{list-style-type:initial;-webkit-margin-start:1em;margin-inline-start:1em;margin-bottom:var(--chakra-space-4);}.css-s8s4xl>*:not(style)~*:not(style){margin-top:var(--chakra-space-2);}</style><ul role="list" class="css-s8s4xl"><li class="css-0">Uses proper character names (ISABELLA, etc.)</li><li class="css-0">Maintains somewhat consistent dramatic structure</li><li class="css-0">Shows more coherent word formation</li><li class="css-0">Demonstrates longer-range dependencies</li></ul>
<h2 class="chakra-heading css-ebdw0u">What We&#x27;ve Accomplished</h2>
<p class="chakra-text css-6gleai">In this three-part series, we&#x27;ve built a complete transformer from scratch:</p>
<style data-emotion="css 11bj9n">.css-11bj9n{list-style-type:decimal;-webkit-margin-start:1em;margin-inline-start:1em;margin-bottom:var(--chakra-space-4);}.css-11bj9n>*:not(style)~*:not(style){margin-top:var(--chakra-space-2);}</style><ol role="list" class="css-11bj9n"><li class="css-0"><strong>Part 1</strong>: Data loading, tokenization, and a simple bigram baseline</li><li class="css-0"><strong>Part 2</strong>: The attention mechanism - the core innovation of transformers</li><li class="css-0"><strong>Part 3</strong>: Complete transformer architecture with all the essential components</li></ol>
<h3 class="chakra-heading css-2k8uwk">Key Components We&#x27;ve Implemented:</h3>
<ul role="list" class="css-s8s4xl"><li class="css-0"><strong>Self-attention mechanisms</strong> for learning relationships between tokens</li><li class="css-0"><strong>Multi-head attention</strong> for capturing different types of relationships</li><li class="css-0"><strong>Feed-forward networks</strong> for computation after communication</li><li class="css-0"><strong>Transformer blocks</strong> that stack attention and computation</li><li class="css-0"><strong>Residual connections</strong> for stable training of deep networks</li><li class="css-0"><strong>Layer normalization</strong> for training stability</li><li class="css-0"><strong>Positional embeddings</strong> so the model understands sequence order</li><li class="css-0"><strong>Dropout</strong> for regularization and preventing overfitting</li></ul>
<h2 class="chakra-heading css-ebdw0u">The Path Forward</h2>
<p class="chakra-text css-6gleai">Our small transformer demonstrates the core principles, but to achieve GPT-level performance, you&#x27;d need to scale up significantly:</p>
<ul role="list" class="css-s8s4xl"><li class="css-0"><strong>More parameters</strong>: Modern language models have billions of parameters</li><li class="css-0"><strong>More data</strong>: Training on much larger text corpora</li><li class="css-0"><strong>More compute</strong>: Training for weeks or months on powerful hardware</li><li class="css-0"><strong>Better tokenization</strong>: Using subword tokenizers like BPE or SentencePiece</li><li class="css-0"><strong>Longer context windows</strong>: Supporting much longer sequences</li></ul>
<h2 class="chakra-heading css-ebdw0u">Key Takeaways</h2>
<ol role="list" class="css-11bj9n"><li class="css-0">
<p class="chakra-text css-6gleai"><strong>Attention is the core innovation</strong>: The self-attention mechanism allows tokens to communicate and share information based on content rather than just position.</p>
</li><li class="css-0">
<p class="chakra-text css-6gleai"><strong>Transformers are surprisingly simple</strong>: The architecture is just attention + feed-forward blocks stacked together with some normalization and residual connections.</p>
</li><li class="css-0">
<p class="chakra-text css-6gleai"><strong>Scale matters</strong>: The same architecture that gives us semi-coherent Shakespeare can generate human-level text when scaled up with more parameters, data, and compute.</p>
</li><li class="css-0">
<p class="chakra-text css-6gleai"><strong>Training stability is crucial</strong>: Residual connections, layer normalization, and proper weight initialization are essential for training deep networks.</p>
</li></ol>
<h2 class="chakra-heading css-ebdw0u">Conclusion</h2>
<p class="chakra-text css-6gleai">This tutorial has shown how to build a transformer model from scratch using PyTorch. We&#x27;ve implemented all the key components of the decoder transformer architecture and seen how they work together to create a language model capable of generating structured text.</p>
<p class="chakra-text css-6gleai">The progression from random gibberish (bigram model) to semi-coherent Shakespeare-like text (full transformer) demonstrates the power of the attention mechanism and proper architectural choices. While our small model doesn&#x27;t generate fully coherent text yet, scaling up the parameters, training data, and computation would lead to increasingly capable language models.</p>
<p class="chakra-text css-6gleai">The complete nanoGPT implementation, with additional optimizations and the ability to train larger models, can be found in <a target="_blank" rel="noopener" class="chakra-link css-1qdxmbw" href="https://github.com/karpathy/nanoGPT">Andrej Karpathy&#x27;s repository</a>. This serves as an excellent foundation for understanding and experimenting with transformer architectures.</p></div></div><style data-emotion="css 17cqjj4">.css-17cqjj4{opacity:0.4;font-size:var(--chakra-fontSizes-sm);}</style><div align="center" class="css-17cqjj4">Build based on <a href="https://www.craftz.dog/" target="_blank">Takuya Matsuyama</a>.</div></div></main><span></span><span id="__chakra_env" hidden=""></span></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"08_transfomers_3","content":"*This is Part 3 of a 3-part series on building a Generative Pretrained Transformer from scratch. In Parts 1 and 2, we built the data pipeline and attention mechanism. Now we'll complete our transformer with the remaining key components.*\n\n## Feed-Forward Networks\n\nAfter our attention mechanism learns to communicate between tokens, we need to give the model computation time to process what it has learned. This is where feed-forward networks come in - they're simply MLPs with ReLU activation that allow the model to \"think\" about the information it just gathered.\n\n```python\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, n_embd),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n\nWe add this to our model after the attention:\n\n```python\n# In our model:\nself.sa_head = MultiHeadAttention(4, n_embd//4)\nself.ffwd = FeedFoward(n_embd) # MLP with RELU\n\n# In forward pass:\nx = self.sa_head(x)\nx = self.ffwd(x)\nlogits = self.lm_head(x)\n```\n\n## Transformer Blocks: Communication + Computation\n\nThe pattern of attention (communication) followed by feed-forward (computation) is so fundamental that we package it into reusable **Transformer Blocks**. These blocks can be stacked to create deeper networks that alternate between communication and computation phases.\n\n```python\nn_head = 4\nn_layer = 2\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n\n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n```\n\nWe can then stack multiple blocks:\n\n```python\n# In our model:\nself.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n\n# In forward pass:\nx = tok_emb + pos_emb # (B,T,C)\nx = self.blocks(x)\nlogits = self.lm_head(x)\n```\n\nAt this point, our model starts generating recognizable English words!\n\n```\nRILELE\nMAssell use you.\nNt I RYan lake off.\n\nFicink'd\nhote carewtledeche quie to whanl Gatt Mejesery ely:\nThe if, bet leveses it theave be ry skit you file.\n\nKay wred tome dake stance, suks,\nAdech JORo!\n\nALOUSBET:\nWis brake grourst and ald creapsss,\nAndite\nnoat,\nAmothery are doreast is\n```\n\n## Training Optimizations\n\nAs our network gets deeper with multiple transformer blocks, we need optimizations to improve training stability and performance.\n\n### Residual Connections\n\nThe first optimization is **residual/skip connections**, introduced in the [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) paper. These connections allow gradients to flow directly through the network, avoiding training bottlenecks in deep networks.\n\nInstead of 'x = self.sa(x)', we do 'x = x + self.sa(x)'. The computation now returns a residual that gets added to the original input:\n\n```python\nclass Block(nn.Module):\n    def forward(self, x):\n        x = x + self.sa(x)  # residual connection\n        x = x + self.ffwd(x)  # residual connection\n        return x\n```\n\n### Projection Layers\n\nWe also add projection layers to map back to the embedding space after our multi-head attention and expand our feed-forward networks as in the original paper:\n\n```python\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.proj(out)\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),  # expand by 4x as in the paper\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),  # project back down\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n\n### Layer Normalization\n\nLayer normalization helps stabilize training by normalizing inputs to have zero mean and unit variance. We apply it before both the self-attention and feed-forward computations in each block, and once more after all blocks:\n\n```python\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n```\n\n### Dropout\n\nFinally, we add **dropout** for regularization, which randomly sets some activations to zero during training to prevent overfitting:\n\n```python\nclass FeedFoward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),  # add dropout\n        )\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n```\n\n## Final Model and Hyperparameters\n\nWe now have a complete decoder-only transformer ready to scale and train! Here are the hyperparameters for our final model:\n\n```python\n# hyperparameters\nbatch_size = 32\nblock_size = 32\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\neval_iters = 200\nn_embd = 64\nn_head = 6\nn_layer = 6\ndropout = 0.2\n```\n\n## Final Results\n\nAfter training our complete transformer, we get much more structured output:\n\n```\nDOF GlLN:\nThey good, then then, bladgy bone not thindnes\nI way Jeain, fainly!\n\nISABELLA:\nI way, thou fourd to havary flown dews'm-sine.\n\nNowZALLET:\nWhich here old thy's warring hiod\nOn dearys hory be wive to more; greseli!\n\nBut nighd Wart, prance:\nBarch,\nAnd prayem not welld she you, coldinger:\nO I the oldst God somed:\nSirry is let never To be whith, I new'n be thy limpiny\nWord: where deblitiss for give upon the conqueennifult,\nAnd so pobeterl. by they she thy truge,\nIf you his let a brotess.\n```\n\nWhile still not perfect, the improvement is dramatic! The model now:\n- Uses proper character names (ISABELLA, etc.)\n- Maintains somewhat consistent dramatic structure\n- Shows more coherent word formation\n- Demonstrates longer-range dependencies\n\n## What We've Accomplished\n\nIn this three-part series, we've built a complete transformer from scratch:\n\n1. **Part 1**: Data loading, tokenization, and a simple bigram baseline\n2. **Part 2**: The attention mechanism - the core innovation of transformers\n3. **Part 3**: Complete transformer architecture with all the essential components\n\n### Key Components We've Implemented:\n\n- **Self-attention mechanisms** for learning relationships between tokens\n- **Multi-head attention** for capturing different types of relationships\n- **Feed-forward networks** for computation after communication\n- **Transformer blocks** that stack attention and computation\n- **Residual connections** for stable training of deep networks\n- **Layer normalization** for training stability\n- **Positional embeddings** so the model understands sequence order\n- **Dropout** for regularization and preventing overfitting\n\n## The Path Forward\n\nOur small transformer demonstrates the core principles, but to achieve GPT-level performance, you'd need to scale up significantly:\n\n- **More parameters**: Modern language models have billions of parameters\n- **More data**: Training on much larger text corpora\n- **More compute**: Training for weeks or months on powerful hardware\n- **Better tokenization**: Using subword tokenizers like BPE or SentencePiece\n- **Longer context windows**: Supporting much longer sequences\n\n## Key Takeaways\n\n1. **Attention is the core innovation**: The self-attention mechanism allows tokens to communicate and share information based on content rather than just position.\n\n2. **Transformers are surprisingly simple**: The architecture is just attention + feed-forward blocks stacked together with some normalization and residual connections.\n\n3. **Scale matters**: The same architecture that gives us semi-coherent Shakespeare can generate human-level text when scaled up with more parameters, data, and compute.\n\n4. **Training stability is crucial**: Residual connections, layer normalization, and proper weight initialization are essential for training deep networks.\n\n## Conclusion\n\nThis tutorial has shown how to build a transformer model from scratch using PyTorch. We've implemented all the key components of the decoder transformer architecture and seen how they work together to create a language model capable of generating structured text.\n\nThe progression from random gibberish (bigram model) to semi-coherent Shakespeare-like text (full transformer) demonstrates the power of the attention mechanism and proper architectural choices. While our small model doesn't generate fully coherent text yet, scaling up the parameters, training data, and computation would lead to increasingly capable language models.\n\nThe complete nanoGPT implementation, with additional optimizations and the ability to train larger models, can be found in [Andrej Karpathy's repository](https://github.com/karpathy/nanoGPT). This serves as an excellent foundation for understanding and experimenting with transformer architectures.","title":"Building GPT from Scratch - Part 3: Building the Complete Transformer","date":"2025-08-06","excerpt":"","tags":["transformers","tutorial"]}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"08_transfomers_3"},"buildId":"2ZPzPUuPIYYAGhw1kMV-o","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>